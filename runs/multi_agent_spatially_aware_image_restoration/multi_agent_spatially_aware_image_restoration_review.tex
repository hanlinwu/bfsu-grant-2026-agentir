% !TEX program = xelatex
\documentclass[11pt,a4paper]{article}

% Page layout
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}

% Language and fonts
\usepackage{ctex}
\usepackage{lmodern}
\usepackage{amsmath,amssymb}

% Bibliography
\usepackage[numbers,sort&compress]{natbib}
\renewcommand*{\bibfont}{\footnotesize}
\bibliographystyle{gbt7714-nsfc}

% Links and formatting
\usepackage{xcolor}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=cyan]{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{longtable}

% Section formatting
\usepackage{titlesec}
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{1em}{}

\providecommand{\tightlist}{\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\title{Multi-Agent Collaborative Spatially-Aware Image Restoration:\\A Comprehensive Review}
\author{}
\date{\today}

\begin{document}
\maketitle
\setcounter{secnumdepth}{3}

\section*{Abstract}

Image restoration, the task of recovering high-quality images from degraded observations corrupted by noise, blur, haze, rain, and compression artifacts, has witnessed transformative advances driven by deep learning. Early convolutional approaches gave way to Transformer-based architectures that capture long-range dependencies, while all-in-one models have attempted to handle multiple degradation types within a single framework. More recently, diffusion-based generative methods have demonstrated remarkable perceptual quality in severely degraded scenarios. Despite this progress, most existing approaches treat the entire image uniformly and employ static inference pipelines, fundamentally limiting their ability to address real-world images where degradation varies spatially in both type and severity. A nascent but rapidly growing paradigm addresses this limitation by leveraging large language models and vision-language models as intelligent agents that dynamically perceive degradation, plan restoration strategies, and orchestrate specialized tools. This review systematically examines the evolution from single-task restoration through unified models to agent-based systems, with particular emphasis on spatially-aware processing and multi-agent collaboration. We identify key challenges including the trade-off between restoration quality and computational efficiency, the gap between synthetic and real-world degradation, and the pressing need for lightweight deployment strategies such as trajectory distillation. By synthesizing findings across 123 representative studies, we delineate emerging research directions and highlight the potential of spatially-aware multi-agent frameworks to establish a new paradigm for intelligent, adaptive image restoration.

\section{Introduction}

Image restoration encompasses a family of inverse problems aimed at recovering a clean image $\mathbf{x}$ from a degraded observation $\mathbf{y} = \mathcal{D}(\mathbf{x}) + \mathbf{n}$, where $\mathcal{D}(\cdot)$ denotes the degradation operator and $\mathbf{n}$ represents additive noise. This problem is fundamental to computer vision, as the quality of input images directly affects the performance of downstream tasks such as object detection, semantic segmentation, and autonomous navigation. In practical scenarios---including surveillance under adverse weather, satellite remote sensing, medical imaging, and consumer photography---images suffer from diverse and often co-occurring degradations: Gaussian and Poisson noise from sensor limitations, motion and defocus blur from camera or scene dynamics, haze and rain from atmospheric interference, and compression artifacts from lossy encoding.

The past decade has seen dramatic improvements in restoration quality, propelled by the transition from hand-crafted priors to learned representations. Convolutional neural networks (CNNs) first demonstrated that end-to-end learning could surpass classical methods by significant margins. Subsequently, Transformer architectures revolutionized the field by enabling global context modeling through self-attention mechanisms. Liang et al.\cite{Liang2021Image} introduced SwinIR, which leverages Swin Transformer blocks with shifted window attention to achieve state-of-the-art results across denoising, super-resolution, and JPEG artifact removal. Zamir et al.\cite{Zamir2022Efficient} proposed Restormer, employing transposed attention to efficiently process high-resolution images while capturing long-range pixel interactions. These methods established Transformer-based designs as the dominant backbone for image restoration.

However, a critical limitation persists: most methods are designed and trained for a single degradation type, requiring separate models for denoising, deblurring, dehazing, and deraining. This paradigm is impractical for real-world deployment where degradation types are unknown a priori and often coexist within a single image. To address this, all-in-one (AiO) restoration methods have emerged as a significant research direction. Li et al.\cite{Li2022AirNet} proposed AirNet, using contrastive learning to build degradation-aware representations within a unified framework. Potlapalli et al.\cite{Potlapalli2023Prompting} introduced PromptIR, which employs learnable prompt vectors to implicitly encode degradation-specific information. Conde et al.\cite{Conde2024InstructIR} developed InstructIR, the first method to leverage natural language instructions for guiding the restoration process. While these unified models represent substantial progress, they remain fundamentally constrained by the set of degradation types seen during training and process the entire image through a single static pipeline.

A parallel development has been the adoption of diffusion probabilistic models for image restoration. Methods such as DiffBIR\cite{Lin2024Toward} and Diff-Retinex\cite{Yi2023Rethinking} leverage the powerful generative priors learned by diffusion models to produce perceptually realistic restorations, particularly for severely degraded inputs where deterministic methods tend to produce oversmoothed results. Li et al.\cite{Li2023Diffusion} provided a comprehensive survey of diffusion-based restoration approaches, highlighting their ability to model complex image distributions. Nevertheless, diffusion models typically require many iterative denoising steps, resulting in high computational costs that limit real-time applications.

Most critically, all the aforementioned approaches---whether single-task, all-in-one, or diffusion-based---share a fundamental assumption: the entire image should be processed uniformly with the same restoration strategy. In reality, degradation is spatially heterogeneous. A single photograph may exhibit motion blur in the foreground, haze in the background, noise in shadow regions, and rain streaks across the scene. Applying a single restoration pipeline globally inevitably leads to suboptimal results: regions that are lightly degraded may be over-processed, while severely degraded areas may be insufficiently restored.

This observation has motivated the emergence of agent-based image restoration, a paradigm that represents a fundamental departure from traditional approaches. Zhu et al.\cite{Zhu2025AgenticIR} proposed AgenticIR, a pioneering framework that employs vision-language models (VLMs) as intelligent agents to perceive image degradation, plan restoration strategies, and orchestrate specialized tools through a five-stage pipeline of perception, scheduling, execution, reflection, and rescheduling. Chen et al.\cite{Chen2024Autonomous} developed RestoreAgent, which fine-tunes a multimodal large language model to directly predict restoration workflows. Jiang et al.\cite{Jiang2025Image} introduced MAIR (Multi-Agent Image Restoration), employing a multi-agent architecture with degradation priors that improves inference efficiency by 44\%. Zhou et al.\cite{Zhou2025QAgent} proposed Q-Agent, which uses quality-driven greedy strategies to achieve linear-complexity restoration planning. These methods open the door to spatially-aware, adaptive restoration that can dynamically adjust strategies based on the specific degradation characteristics of different image regions.

This review provides a systematic examination of the evolution from classical single-task restoration to the emerging paradigm of multi-agent spatially-aware image restoration. We organize the literature into six thematic areas: (1) Transformer-based single-task restoration architectures, (2) all-in-one unified restoration models, (3) diffusion-based generative restoration, (4) agent-based restoration systems, (5) foundation models for degradation analysis and quality assessment, and (6) model compression and lightweight deployment. For each area, we analyze key methodological innovations, compare representative approaches, identify limitations, and discuss open challenges. We conclude by outlining future research directions, with particular emphasis on how spatially-aware multi-agent collaboration can address the remaining gaps between current methods and the demands of real-world deployment.

\section{Transformer-Based Single-Task Image Restoration}

The introduction of Transformer architectures into image restoration marked a paradigm shift from purely local convolutional operations to global context modeling through self-attention. This section examines the key architectural innovations that have made Transformers the dominant backbone for restoration tasks, analyzes the efficiency--quality trade-offs inherent in different attention designs, and identifies the limitations that motivate the transition toward unified and agent-based approaches.

\subsection{Window-Based Attention Architectures}

The computational cost of standard self-attention scales quadratically with the number of pixels, making it prohibitively expensive for high-resolution image restoration. Window-based attention mechanisms address this challenge by restricting attention computation to local windows while maintaining the ability to capture long-range dependencies through window shifting or overlapping strategies. SwinIR\cite{Liang2021Image} adapts the Swin Transformer architecture to image restoration by stacking residual Swin Transformer blocks (RSTBs), each containing multiple Swin Transformer layers with shifted window multi-head self-attention. This design achieves linear computational complexity with respect to image size while maintaining competitive or superior performance to CNN-based methods across denoising (achieving 0.14--0.31 dB PSNR improvement over prior art on benchmark datasets), classical and lightweight super-resolution, and JPEG compression artifact reduction. The shifted window mechanism enables cross-window connections that effectively propagate information across the entire feature map without the quadratic cost of global attention.

Wu et al.\cite{Wu2025Rethinking} revisited the window-based attention paradigm in DSwinIR, demonstrating that the original shifted window strategy in SwinIR introduces boundary artifacts at window borders and proposed dynamic window attention with learnable offsets that adapt window positions to image content. This content-adaptive windowing strategy yields consistent improvements over SwinIR across multiple restoration tasks, suggesting that the fixed grid pattern of standard shifted windows is suboptimal for spatially varying image content. Conde et al.\cite{Conde2023Transformer} extended the Swin Transformer to its second version in Swin2SR, incorporating SwinV2's improvements including cosine attention and log-spaced continuous position bias, demonstrating enhanced stability for compressed image super-resolution and achieving competitive performance with reduced training instability. The original Swin2SR work\cite{Conde2022Transformer} showed that these architectural refinements are particularly beneficial for handling the diverse artifact patterns introduced by different JPEG quality factors, as the improved attention mechanism provides more stable gradient flow during training on heavily compressed images. Jing et al.\cite{Jing2025A} further explored the lightweight super-resolution direction by proposing a parallel connection of convolution and Swin Transformer blocks, where the convolutional branch captures fine-grained local textures while the Transformer branch models global structural dependencies. The parallel design avoids the sequential bottleneck of cascade architectures and reduces parameter count through shared intermediate representations, achieving competitive super-resolution quality with significantly fewer parameters---an important consideration for deploying window-based Transformer restoration on resource-constrained devices.

\subsection{Channel and Transposed Attention Designs}

An alternative approach to reducing self-attention complexity operates along the channel dimension rather than the spatial dimension. Restormer\cite{Zamir2022Efficient} introduces multi-Dconv head transposed attention (MDTA), which computes attention across channels rather than spatial locations, thereby achieving linear complexity with respect to spatial resolution while preserving the ability to model global context. This design is complemented by a gated-Dconv feed-forward network (GDFN) that applies depth-wise convolutions to introduce local context. Restormer achieves state-of-the-art results on image denoising (0.13--0.52 dB improvement on the SIDD benchmark\cite{Zamir2022Efficient}), single-image motion deblurring, defocus deblurring, and deraining, establishing transposed attention as a highly effective alternative to spatial attention for restoration.

Wang et al.\cite{Wang2022A} proposed Uformer, a hierarchical U-shaped Transformer that combines window-based self-attention in the encoder--decoder structure with a learnable multi-scale restoration modulator. The modulator adjusts features at each level based on degradation characteristics, enabling the network to adapt its behavior to different spatial scales. Uformer demonstrates strong performance on denoising and deblurring while maintaining computational efficiency through its window-based attention in each level of the U-Net hierarchy.

\subsection{Hybrid CNN-Transformer Architectures}

Recognizing that CNNs excel at capturing local texture patterns while Transformers are superior at modeling global structure, several works have proposed hybrid architectures that combine both paradigms. Chen et al.\cite{Chen2022Hybrid} developed Dual-former, which employs parallel CNN and Transformer branches with feature exchange mechanisms, allowing the network to simultaneously capture local details and global context. The dual-branch design achieves consistent improvements over single-backbone models, particularly on tasks where both fine texture preservation and global structural coherence are important.

Chen et al.\cite{Chen2023Hybrid} proposed a hybrid CNN-Transformer feature fusion network for single image deraining that uses cross-attention to merge CNN-extracted local features with Transformer-captured global dependencies. Shi et al.\cite{Shi2023CNN} introduced Sandformer, which integrates CNN and Transformer under a gated fusion mechanism specifically designed for sand dust image restoration, demonstrating the versatility of hybrid designs for specialized degradation types. The convergence toward hybrid architectures reflects a broader recognition that no single attention mechanism is optimal for all spatial scales and degradation patterns encountered in restoration tasks.

\subsection{Efficient Attention and State-Space Models}

Beyond window-based and channel-based strategies, researchers have explored alternative efficient attention mechanisms. Kong et al.\cite{Kong2023Efficient} proposed FFTformer, which replaces spatial self-attention with frequency-domain operations, computing attention in the Fourier domain to achieve global receptive fields with reduced computational cost. This approach is particularly effective for image deblurring, where frequency-domain representations naturally capture the spectral characteristics of blur kernels. Building on this frequency-domain perspective, Jiang et al.\cite{Jiang2024When} investigated the synergy between fast Fourier transforms (FFT) and Transformer architectures for image restoration, demonstrating that interleaving FFT-based global mixing layers with local window attention layers yields a complementary representation: the FFT layers capture periodic and global frequency patterns efficiently, while the window attention layers preserve spatially localized detail. This dual-domain processing achieves superior restoration quality compared to either approach in isolation, particularly for degradation types that manifest in specific frequency bands such as JPEG compression artifacts and periodic noise.

Mao et al.\cite{Mao2023Intriguing} presented intriguing findings regarding frequency selection for image deblurring, revealing that not all frequency components contribute equally to restoration performance. Their analysis shows that selectively attending to mid-frequency bands---which carry the majority of structural information degraded by blur---while suppressing contributions from very low and very high frequencies leads to both improved deblurring quality and reduced computational overhead. This frequency-aware selection strategy can be integrated into existing Transformer backbones as a lightweight plug-in module, providing a principled approach to frequency-domain attention that avoids the computational burden of processing the full spectrum.

Shi et al.\cite{Shi2025Visual} introduced VmambaIR, which applies the Visual Mamba state-space model to image restoration. Unlike Transformer attention, which has quadratic complexity, the Mamba architecture achieves linear complexity through selective state-space scanning while maintaining the ability to model long-range dependencies. VmambaIR demonstrates competitive performance with Transformer-based methods at significantly lower computational cost, suggesting that state-space models represent a promising direction for efficient restoration. Similarly, Lee et al.\cite{Lee2024Decompose} proposed Decomformer, which decomposes self-attention into complementary low-rank and sparse components to reduce redundancy while preserving expressive power for efficient image restoration.

Ghasemabadi et al.\cite{Ghasemabadi2024Efficiency} introduced CascadedGaze, a method designed to efficiently extract global context for image restoration without incurring the quadratic cost of full self-attention. CascadedGaze employs a cascaded structure where each stage progressively expands its receptive field through gaze-shift operations, aggregating local attention outputs from one stage as contextual tokens for the next. This cascading strategy achieves near-global context modeling with computational cost that scales linearly with spatial resolution, making it particularly suitable for high-resolution restoration tasks where full global attention is impractical. Zhang et al.\cite{Zhang2024Joint} proposed a joint multi-dimensional dynamic attention mechanism that simultaneously models dependencies across spatial, channel, and scale dimensions. Unlike approaches that treat these dimensions independently, the joint formulation captures cross-dimensional correlations---for instance, the relationship between specific spatial regions and the channel features most relevant to restoring those regions---through a shared dynamic attention kernel. This multi-dimensional approach yields consistent improvements across denoising, deblurring, and super-resolution tasks, suggesting that cross-dimensional attention modeling remains an underexplored avenue for restoration performance gains.

Additional innovations include the Omni-Kernel Network (OKNet)\cite{Cui2024Network}, which designs omni-dimensional convolution kernels that can adaptively aggregate features across spatial, channel, and kernel dimensions, and GoLDFormer\cite{Chen2024A}, which introduces global--local deformable window attention that combines deformable attention with window-based processing for flexible receptive field adaptation. Gao et al.\cite{Gao2023Mixed} proposed a mixed hierarchy network that integrates features at multiple granularities for comprehensive restoration.

\subsection{Self-Supervised and Training Paradigm Innovations}

Beyond architectural design, recent work has investigated how the training paradigm itself can be improved for Transformer-based restoration. Zhang and Zhou\cite{Zhang2023Image_1} proposed a self-supervised image denoising method based on a context-aware Transformer, which eliminates the need for paired clean-noisy training data by leveraging blind-spot masking within the Transformer attention mechanism. The context-aware design ensures that each pixel's denoised estimate is computed from its surrounding context without using the noisy pixel itself, satisfying the self-supervised learning constraint while still capturing long-range dependencies. This approach achieves denoising quality competitive with fully supervised Transformer methods on real-world noise benchmarks, demonstrating that the combination of self-supervised learning and Transformer architectures can address the practical challenge of obtaining clean reference images for training.

\subsection{Limitations and the Motivation for Unified Approaches}

Despite their impressive performance, Transformer-based single-task methods suffer from a fundamental limitation: task specificity. Each model is typically trained for a single degradation type (e.g., denoising at a specific noise level, or deblurring with a specific blur kernel family), requiring practitioners to maintain a library of specialized models and to correctly identify the degradation type before selecting the appropriate model. Ali et al.\cite{Ali2023Vision} provided a comprehensive survey of vision Transformers in image restoration, highlighting that while architectural innovations have dramatically improved single-task performance, the proliferation of task-specific models creates significant practical challenges for real-world deployment. Mei et al.\cite{Mei2023Pyramid} and Gao et al.\cite{Gao2023Towards} further demonstrated that even within a single task such as deraining, performance varies significantly depending on rain density and scene complexity, underscoring the difficulty of building robust single-task models.

These limitations directly motivate the development of all-in-one restoration methods discussed in the next section, as well as the agent-based approaches that can dynamically select and compose specialized tools based on observed degradation characteristics.


\section{All-in-One Unified Image Restoration}

All-in-one (AiO) image restoration seeks to handle multiple degradation types with a single model, eliminating the need for degradation-specific architectures. This section traces the evolution of AiO methods from early contrastive learning approaches through prompt-based designs to recent instruction-guided and mixture-of-experts frameworks, analyzing how each addresses the challenge of degradation awareness within a unified architecture.

\subsection{Contrastive and Representation Learning Approaches}

The foundational challenge of AiO restoration is learning degradation-discriminative representations that enable a single network to distinguish and appropriately handle diverse degradation types. AirNet\cite{Li2022AirNet} addresses this through contrastive-based degradation encoder (CBDE) that learns to cluster different degradation types in a compact representation space. By pulling together representations of images with the same degradation type while pushing apart those with different degradations, AirNet enables a single restoration backbone to adaptively process multiple degradation types. Evaluated on denoising, dehazing, and deraining, AirNet demonstrated that contrastive learning can serve as an effective self-supervised mechanism for degradation awareness without requiring explicit degradation labels.

Hu et al.\cite{Hu2025Collaborative} extended the contrastive learning paradigm with collaborative semantic contrastive learning for all-in-one image restoration, proposing a multi-level contrastive framework that simultaneously operates at image-level, feature-level, and patch-level granularities. This hierarchical contrastive design captures both global degradation characteristics and local corruption patterns, yielding improved degradation discrimination compared to single-level contrastive approaches. Wu et al.\cite{Wu2024Learning} proposed a task-agnostic model contrastive learning framework that learns from restoration history, constructing positive and negative pairs from different training stages to continuously improve feature representations.

\subsection{Prompt-Based Restoration Methods}

A major advance in AiO restoration came from adapting the prompt learning paradigm from natural language processing. PromptIR\cite{Potlapalli2023Prompting} introduces learnable prompt vectors that are injected into a Transformer backbone through prompt blocks, enabling the network to implicitly encode degradation-specific information without explicit degradation identification. The prompts serve as soft conditioning signals that modulate the network's behavior for different degradation types, achieving competitive performance across denoising, dehazing, deraining, and deblurring within a single model. Crucially, PromptIR demonstrates that prompt-based conditioning can be more flexible than explicit degradation classification, as the continuous prompt space can represent intermediate and mixed degradation states.

The prompt paradigm has been extensively developed in subsequent works. Ma et al.\cite{Ma2023Exploring} proposed ProRes, which explores degradation-aware visual prompts for universal image restoration. Rather than learning prompts from scratch, ProRes constructs visual prompts from degradation-specific prior knowledge, providing more interpretable and effective conditioning. Li et al.\cite{Li2023Learning} introduced Prompt-In-Prompt learning, embedding fine-grained degradation-specific prompts within coarser task-level prompts through a hierarchical prompt architecture that enables both global task discrimination and local degradation adaptation.

Wu et al.\cite{Wu2025Learning} developed a dynamic prompting mechanism that generates input-adaptive prompts rather than using fixed learned vectors, allowing the prompt content to vary based on the specific characteristics of each input image. Sun et al.\cite{Sun2025Adaptive} proposed AdaPrompt-IR, which adaptively learns to perceive degradation semantics and generates degradation-aware prompts through a lightweight degradation perception module, achieving improved generalization to unseen degradation combinations. Liu et al.\cite{Liu2025When} introduced UP-Restorer, which combines algorithm unrolling with prompts for unified image restoration, demonstrating that integrating model-based priors with prompt-based learning yields both improved performance and enhanced interpretability.

Gao et al.\cite{Gao2024Image} proposed a prompt-based ingredient-oriented all-in-one restoration framework that decomposes each degradation type into constituent ``ingredients''---basic corruption primitives such as frequency attenuation, contrast reduction, and texture loss---and learns ingredient-specific prompts rather than degradation-level prompts. This finer-grained prompt decomposition enables the model to handle novel degradation types as new combinations of known ingredients, substantially improving generalization beyond the training distribution. Wu et al.\cite{Wu2024Frequency} introduced FrePrompter, a frequency self-prompt approach that automatically generates degradation prompts from the frequency spectrum of the input image rather than relying on learned prompt vectors. By extracting spectral signatures that characterize different degradation types---such as the high-frequency attenuation pattern of blur or the broadband noise floor of Gaussian corruption---FrePrompter provides physically grounded prompt information that enhances both the interpretability and effectiveness of prompt-based restoration. Wu et al.\cite{Wu2025Beyond} advanced the prompt learning paradigm further with contrastive prompt learning that goes beyond degradation redundancy, addressing the observation that many degradation types share overlapping feature representations that can confuse prompt-based systems. By explicitly encouraging prompts to capture discriminative rather than redundant degradation features through a contrastive objective, this approach achieves more precise degradation-specific modulation and demonstrates state-of-the-art performance across a broader range of degradation types than prior prompt methods.

\subsection{Instruction-Guided Restoration}

InstructIR\cite{Conde2024InstructIR} represents a paradigm shift by enabling users to control the restoration process through natural language instructions. Instead of relying on implicit degradation detection, users describe the desired restoration in text (e.g., ``remove the noise from this photo'' or ``enhance the contrast and sharpen the details''), and the model adaptively performs the corresponding task. This approach bridges image restoration with natural language understanding, leveraging a text encoder to generate instruction-aware features that condition the restoration network. InstructIR demonstrates strong performance across seven restoration tasks while providing an intuitive human-machine interface that does not require technical knowledge of degradation types.

Jiang et al.\cite{Jiang2025Visual} proposed multi-dimension visual prompt enhanced image restoration, extending the concept of prompting beyond text to include visual prompts derived from degradation maps, quality scores, and semantic segmentation. This multi-dimensional conditioning provides richer degradation information than single-modality prompts. Duan et al.\cite{Duan2024A} developed UniProcessor, a text-induced unified low-level image processor that leverages natural language descriptions of both degradation characteristics and desired restoration outcomes to condition a single processing network. Unlike InstructIR, which uses relatively simple restoration instructions, UniProcessor employs detailed textual descriptions that simultaneously specify the degradation type, its severity, and the target quality attributes, enabling finer-grained control over the restoration process. The text-induced approach also facilitates zero-shot transfer to novel degradation types described in natural language, even when no training examples of those degradation types are available. Xin et al.\cite{Xin2025Parallel} developed a self-collaboration parallel prompt approach using generative adversarial networks for unsupervised all-in-one image restoration, eliminating the need for paired training data by enabling the model to collaboratively refine prompts through an adversarial learning process.

\subsection{Degradation-Aware Architectures and Mixture of Experts}

Another line of research focuses on designing architectures that are inherently degradation-aware. Wu et al.\cite{Wu2024Harmony} proposed ``Harmony in Diversity,'' an all-in-one image restoration approach that uses degradation-aware convolution kernels whose weights are dynamically generated based on estimated degradation parameters. This allows a single network to effectively customize its processing for each input without explicit task switching.

Zamfir et al.\cite{Zamfir2025Complexity} introduced MoCE-IR (Mixture of Complexity Experts), which employs a mixture-of-experts architecture where different experts specialize in degradation types of varying complexity. A gating network routes each input to the appropriate combination of experts, enabling the model to allocate computational resources proportional to the restoration difficulty. This design achieves state-of-the-art results across multiple benchmarks while maintaining computational efficiency through sparse expert activation.

Chen et al.\cite{Chen2025Always} proposed ``Always Clear Days,'' a degradation type and severity-aware framework that explicitly models both the type and severity of degradation through a dual-branch architecture. Tian et al.\cite{Tian2025Feature} introduced degradation-aware feature perturbation, which systematically perturbs features based on estimated degradation characteristics to improve the robustness of all-in-one restoration. Zhu et al.\cite{Zhu2024Image} developed MWFormer, a multi-weather degradation-aware Transformer that handles rain, haze, snow, and their combinations through weather-specific attention modules. Wu et al.\cite{Wu2025Debiased} proposed a debiased all-in-one restoration approach with task uncertainty regularization that addresses the training bias toward certain degradation types by calibrating task-specific uncertainty.

Tang et al.\cite{Tang2025Reasoning} introduced RamIR, which integrates reasoning and action prompting with the Mamba state-space architecture for all-in-one image restoration. Unlike conventional AiO methods that directly map inputs to outputs, RamIR decomposes the restoration process into an explicit reasoning stage---where the model infers degradation type and severity through chain-of-thought-style internal representations---followed by an action stage where the inferred degradation information drives restoration through Mamba's selective state-space mechanism. This reasoning-action decomposition mimics the deliberate analysis process of human experts and achieves improved performance on complex mixed-degradation scenarios where purely reactive approaches falter. Dudhane et al.\cite{Dudhane2024Dynamic} addressed the scalability challenge of AiO restoration through dynamic pre-training, proposing a training strategy where the model is progressively exposed to an expanding set of degradation types during pre-training rather than training on all types simultaneously. This curriculum-based approach enables the model to first learn robust low-level features from simple degradation types before tackling more complex corruptions, resulting in both more efficient training and improved final performance compared to conventional joint training on all degradation types simultaneously.

Li et al.\cite{Li2024An} proposed a multi-weather restoration framework based on efficient prompt-guided convolution, where lightweight degradation-specific prompts modulate the convolution kernel weights at each layer rather than conditioning through separate attention mechanisms. This prompt-guided convolution design is significantly more parameter-efficient than attention-based conditioning, enabling multi-weather restoration with minimal overhead compared to a single-weather baseline. Chen et al.\cite{Chen2024degradation} developed a multi-modal degradation feature learning framework for unified image restoration based on contrastive learning, which learns degradation representations from multiple modalities---including spatial features, frequency characteristics, and statistical moments---and fuses them through cross-modal attention. This multi-modal degradation representation provides a more comprehensive characterization of complex degradation patterns than single-modal approaches, particularly for mixed degradation scenarios where different modalities capture complementary corruption information. Yang et al.\cite{Yang2024Image} proposed IPT-ILR (Image Pyramid Transformer with Information Loss Regularization), which processes images at multiple pyramid levels through a shared Transformer backbone with level-specific adaptation layers. The information loss regularization term explicitly penalizes the loss of structural information across pyramid levels, ensuring that both coarse global structures and fine local details are preserved throughout the multi-scale restoration process.

\subsection{Domain-Specific and Emerging All-in-One Approaches}

The all-in-one paradigm has been extended beyond standard natural image benchmarks to domain-specific applications where specialized degradation patterns demand tailored solutions. Zhang et al.\cite{Zhang2023Image} proposed an all-in-one multi-degradation image restoration network via hierarchical degradation representation, which organizes degradation types into a hierarchical taxonomy---from coarse categories (weather, sensor, compression) to fine-grained subtypes---and learns correspondingly hierarchical representations that enable both broad degradation discrimination and fine-grained restoration adaptation. This hierarchical structure provides a principled framework for scaling AiO models to large numbers of degradation types without proportional growth in model complexity.

Chen et al.\cite{Chen2023aerial} demonstrated the application of all-in-one image enhancement to aerial forest scenes, addressing the unique challenges of airborne imagery including haze, low illumination, and atmospheric scattering that simultaneously degrade canopy visibility. Their forest-specific AiO network incorporates domain knowledge about vegetation spectral characteristics to guide the restoration process, achieving improved species identification accuracy from restored aerial imagery. Zhang et al.\cite{Zhang2025Considering} proposed UniUIR, which formulates underwater image restoration as an all-in-one learning problem, handling the diverse and often co-occurring degradation types encountered in aquatic environments---including color cast from wavelength-dependent absorption, haze from suspended particles, and low contrast from backscatter---within a single model. UniUIR's success in the underwater domain demonstrates that AiO approaches can effectively generalize to degradation distributions substantially different from those encountered in atmospheric natural imaging.

Yang et al.\cite{Yang2024Scene} developed MvKSR (Multi-view Knowledge-guided Scene Recovery), which tackles scene recovery under combined haze and rain degradation by leveraging multi-view knowledge from different degradation-specific perspectives. The multi-view design enables the model to simultaneously analyze the scene from weather removal, detail enhancement, and color correction perspectives, with a knowledge fusion mechanism that combines these complementary views into a coherent restoration. Liu et al.\cite{Liu2024Visibility} addressed real-time multi-scene visibility enhancement for navigational safety under complex weather conditions, proposing a lightweight AiO architecture specifically designed for the latency constraints of vessel navigation systems. Their approach achieves real-time processing speeds while maintaining competitive visibility enhancement across fog, rain, haze, and low-light conditions, demonstrating that efficiency-oriented AiO design can meet the stringent requirements of safety-critical applications.

Tang et al.\cite{Tang2022Generative} explored generative adversarial unsupervised image restoration in hybrid degradation scenes, proposing a GAN-based framework that can handle complex mixtures of degradation types without requiring paired training data. The unsupervised approach learns degradation disentanglement through adversarial training, separating the clean image content from the degradation pattern without explicit degradation supervision. This unsupervised paradigm is particularly valuable for real-world applications where obtaining paired clean-degraded training data is infeasible. Liang et al.\cite{Liang2025Implicit} introduced INP-Net (Implicit Neural Prompting Network) for remote sensing image dehazing, which uses implicit neural representations to generate continuous, spatially-varying prompts that adapt to the non-uniform haze distribution typically observed in remote sensing imagery. Unlike discrete prompt vectors, the implicit neural representation can model smoothly varying degradation intensity across the spatial extent of large-scale remote sensing images.

Zeng et al.\cite{Zeng2025Image} proposed a unified framework for all-in-one image compression and restoration, jointly addressing the traditionally separate problems of lossy compression artifact removal and general degradation restoration within a single model. This joint formulation is motivated by the observation that compressed images in real-world applications frequently exhibit both compression artifacts and other degradation types simultaneously, and treating them independently leads to suboptimal results. Zhang et al.\cite{Zhang2025A} developed a unified accelerator for all-in-one image restoration based on prompt degradation learning, focusing on the hardware implementation perspective. Their work designs an FPGA-based accelerator architecture that efficiently executes prompt-conditioned restoration through custom hardware units for prompt generation, degradation-conditioned convolution, and prompt-guided attention, achieving substantial speedup over GPU implementation while maintaining restoration quality. This hardware-software co-design perspective addresses a critical gap in the AiO restoration literature, where most methods are evaluated solely on GPU platforms without considering deployment on specialized hardware.

\subsection{Deep Unfolding and Model-Based Approaches}

Several works have combined deep learning with model-based optimization for AiO restoration. Tang et al.\cite{Tang2025Optimal} proposed DA-RCOT (Degradation-Aware Residual-Conditioned Optimal Transport), which formulates AiO restoration as an optimal transport problem with degradation-conditioned residual connections. Zeng et al.\cite{Zeng2025Gradient} developed a vision-language gradient descent-driven deep unfolding framework that integrates vision-language models with iterative optimization, using VLM-derived degradation descriptions to guide each unfolding step. Cheng et al.\cite{Cheng2024deep} introduced RDM-IR, a task-adaptive deep unfolding network that combines iterative shrinkage-thresholding with learned degradation-specific parameters for all-in-one restoration.

Lihe et al.\cite{Lihe2025An} proposed Ada4DIR, an adaptive model-driven approach that directly incorporates the imaging physics model into the network architecture, enabling the model to explicitly reason about the degradation process during restoration. These model-based approaches offer improved interpretability compared to purely data-driven methods, as the iterative unfolding steps correspond to optimization iterations with clear physical meaning.

\subsection{Limitations of All-in-One Approaches}

Jiang et al.\cite{Jiang2025A} provided a comprehensive survey on AiO restoration, identifying several persistent limitations. First, most AiO models are trained on a fixed set of degradation types (typically 3--7), and their performance degrades substantially when encountering out-of-distribution degradation types or novel combinations not seen during training. Second, AiO models process the entire image through a single pipeline, ignoring the spatial heterogeneity of degradation. Third, the single forward-pass inference paradigm cannot iteratively refine results based on intermediate quality assessment, leading to suboptimal outcomes for complex degradation scenarios that require multi-step reasoning. These limitations highlight the need for more flexible, adaptive approaches---specifically, agent-based methods that can dynamically perceive degradation, select appropriate tools, and iteratively refine restoration strategies.


\section{Diffusion-Based Generative Image Restoration}

Diffusion probabilistic models have emerged as a powerful generative framework for image restoration, offering the ability to produce perceptually realistic results by leveraging learned image priors from large-scale pretraining. This section examines how diffusion models have been adapted for restoration, analyzes their advantages and limitations compared to discriminative methods, and discusses recent innovations in conditional generation and efficiency improvements.

\subsection{Foundations and Conditional Diffusion for Restoration}

The application of diffusion models to image restoration typically involves conditioning the reverse denoising process on the degraded input image. Saharia et al.\cite{Saharia2022Diffusion} introduced Palette, one of the earliest diffusion-based frameworks for image-to-image translation tasks including colorization, inpainting, and uncropping. Palette demonstrates that a simple conditional diffusion model, trained with paired data, can produce diverse, high-quality outputs that capture the multimodal nature of inverse problems---a fundamental advantage over deterministic methods that produce a single point estimate.

Zhang et al.\cite{Zhang2023A} proposed a unified conditional framework for diffusion-based image restoration that introduces task-adaptive conditioning mechanisms, enabling a single diffusion model to handle multiple restoration tasks by conditioning on task-specific embeddings. This unified approach represents an important step toward combining the generative power of diffusion models with the flexibility of all-in-one methods. Liu et al.\cite{Liu2024Residual} developed Residual Denoising Diffusion Models (RDDM), which reformulate the diffusion process to operate on residuals between clean and degraded images rather than pure noise, significantly reducing the number of sampling steps required while maintaining high restoration quality. Zheng et al.\cite{Zheng2024Selective} proposed a selective hourglass mapping strategy for universal image restoration based on diffusion models, which selectively applies the diffusion process only to the most degraded frequency components of the input while preserving well-maintained components through direct passthrough. The hourglass architecture progressively maps degraded features to clean features through a bottleneck structure that concentrates the diffusion model's generative capacity on the most challenging restoration regions, achieving improved efficiency without sacrificing quality on diverse degradation types. Ding et al.\cite{Ding2024Restoration} formulated image restoration as restoration by generation with constrained priors, introducing explicit constraints during the diffusion sampling process that enforce fidelity to the degraded input while allowing the generative prior to synthesize plausible high-frequency details. The constrained generation framework provides a principled balance between the faithfulness of deterministic methods and the perceptual quality of unconstrained generative approaches, addressing a fundamental tension in diffusion-based restoration.

\subsection{Diffusion Priors from Pre-Trained Models}

A significant recent trend leverages the powerful image priors learned by large pre-trained diffusion models, particularly Stable Diffusion, for restoration tasks. DiffBIR\cite{Lin2024Toward}\cite{Lin2023Towards} pioneered this approach by proposing a two-stage pipeline: first, a restoration module removes primary degradation to produce an intermediate result, then a conditional latent diffusion model refines the result using generative priors from a pre-trained Stable Diffusion model. This design achieves remarkable perceptual quality on blind image restoration tasks, including real-world super-resolution and face restoration, where the diffusion prior effectively hallucinate plausible high-frequency details that deterministic methods cannot recover.

Yang et al.\cite{Yang2024Stable} proposed Pixel-Aware Stable Diffusion (PASD), which introduces a pixel-aware cross-attention module that enables the diffusion model to maintain fidelity to the input image while leveraging generative priors for detail synthesis. Tian et al.\cite{Tian2024Learning} developed a method for learning diffusion texture priors for image restoration, extracting texture-level guidance from diffusion models to improve the quality of fine details without the full cost of diffusion sampling. Xu et al.\cite{Xu2024Boosting} demonstrated that priors from pre-trained models including CLIP and Stable Diffusion can be combined to boost restoration performance across multiple tasks, with CLIP providing semantic guidance and Stable Diffusion providing texture priors.

\subsection{Task-Specific Diffusion Adaptations}

Diffusion models have been successfully adapted to specific restoration tasks with tailored designs. Guo et al.\cite{Guo2023When} proposed ShadowDiffusion, which introduces degradation-aware conditioning for shadow removal by incorporating shadow mask information into the diffusion process. Yi et al.\cite{Yi2023Rethinking} developed Diff-Retinex, which combines Retinex theory with diffusion models for low-light image enhancement, using the decomposition into illumination and reflectance components to guide the diffusion process. Yi et al.\cite{Yi2025Reinforced} further extended this approach in Diff-Retinex++, incorporating reinforcement learning to optimize the diffusion sampling trajectory for improved efficiency and quality.

Shang et al.\cite{Shang2024Combining} introduced ResDiff, which combines CNN-based baseline restoration with a diffusion model that refines residual details, effectively using the CNN to handle the bulk of the restoration while the diffusion model adds perceptually important high-frequency content. Luo et al.\cite{Luo2023Enabling} proposed Refusion for enabling large-size realistic image restoration with diffusion models, addressing the challenge that diffusion models typically operate at fixed low resolutions by introducing a region-aware diffusion strategy. Wang et al.\cite{Wang2023Diffusion} further addressed the resolution limitation with unlimited-size diffusion restoration, proposing a patch-based diffusion strategy with carefully designed overlapping and blending mechanisms that enable diffusion-based restoration to process images of arbitrary resolution without introducing visible seam artifacts at patch boundaries. This approach is particularly important for practical applications such as satellite imagery and medical imaging where images are orders of magnitude larger than the diffusion model's native resolution. Zhao et al.\cite{Zhao2023Towards} developed an iterative diffusion framework for authentic face restoration, demonstrating the effectiveness of diffusion-based iterative refinement for specialized restoration tasks.

Welker et al.\cite{Welker2024Adapting} proposed DriftRec, which adapts diffusion models specifically to blind JPEG restoration by modeling the JPEG compression process as a ``drift'' in the diffusion trajectory. Rather than treating JPEG artifact removal as a generic denoising problem, DriftRec incorporates knowledge of the block-based discrete cosine transform structure of JPEG compression into the diffusion process, enabling the model to precisely reverse compression-induced artifacts while preserving authentic image detail. This degradation-aware diffusion adaptation achieves significant improvements over generic diffusion restoration on JPEG-compressed images, particularly at low quality factors where blocking artifacts are severe. Kang et al.\cite{Kang2024Image} developed an image intrinsic components guided conditional diffusion model for low-light image enhancement, which decomposes the image into illumination and reflectance components following the Retinex model and uses these intrinsic components as conditioning signals for the diffusion process. This physics-informed conditioning provides the diffusion model with explicit structural guidance about the image formation process, leading to enhanced detail preservation and more natural color rendering in the enhanced output compared to unconditional diffusion approaches. Yan et al.\cite{Yan2025Efficient} proposed an efficient diffusion-based approach to image enhancement through frequency-domain priors, where the diffusion process is guided by frequency-domain analysis that identifies which spectral components require generative enhancement versus direct preservation. By concentrating the computationally expensive diffusion sampling on frequency bands that genuinely benefit from generative modeling while directly passing through well-preserved bands, this method achieves substantial computational savings while maintaining the perceptual quality advantages of diffusion-based restoration.

\subsection{All-in-One Diffusion Restoration}

Combining diffusion models with the all-in-one paradigm represents a particularly active research direction. Tu et al.\cite{Tu2026Unifying} proposed an uncertainty-aware diffusion bridge model for unifying heterogeneous degradations, using the diffusion bridge formulation to directly model the transition between degraded and clean distributions conditioned on degradation type. Luo et al.\cite{Luo2025Degradation} developed Defusion, which uses visual-instructed degradation information to guide the diffusion process for all-in-one restoration, achieving strong results on both known and novel degradation types. Zhang et al.\cite{Zhang2025Unleashing} proposed Diff-Restorer, which unleashes visual prompts for diffusion-based universal image restoration by encoding degradation characteristics as visual prompt tokens that condition the diffusion sampling.

Lv et al.\cite{Lv2025Adaptive} introduced adaptive prompt-guided unified image restoration with latent diffusion, where a prompt generation module dynamically creates degradation-aware prompts that are injected into the latent diffusion process. Yue et al.\cite{Yue2024Joint} proposed a joint conditional diffusion model specifically designed for image restoration with mixed degradations, where a single diffusion model simultaneously conditions on multiple degradation descriptors to handle images corrupted by combinations of noise, blur, and compression. The joint conditioning mechanism enables the model to reason about degradation interactions---for instance, how blur affects the optimal noise removal strategy---rather than treating each degradation type independently, yielding improved results on mixed-degradation benchmarks compared to sequentially applying single-degradation diffusion models. These combined approaches leverage both the flexibility of prompt-based degradation conditioning and the generative power of diffusion models, representing the current frontier of unified restoration methods.

\subsection{Limitations of Diffusion-Based Approaches}

Despite their impressive perceptual quality, diffusion-based restoration methods face several persistent challenges. The iterative sampling process requires tens to hundreds of denoising steps, resulting in inference times that are orders of magnitude slower than feed-forward methods. While techniques such as RDDM\cite{Liu2024Residual} and distillation-based acceleration have partially addressed this, real-time diffusion-based restoration remains challenging. Additionally, diffusion models may hallucinate plausible but incorrect details, particularly in regions where the degradation has destroyed all original information, leading to concerns about fidelity in applications such as medical imaging and forensic analysis where accuracy is paramount. The large model sizes and memory requirements of pre-trained diffusion models also limit deployment on edge devices, motivating the compression and distillation approaches discussed in Section~\ref{sec:compression}.


\section{Agent-Based Image Restoration Systems}

Agent-based image restoration represents the most recent and potentially most transformative paradigm in the field, fundamentally reimagining restoration as an intelligent decision-making process rather than a static mapping function. By leveraging large language models (LLMs) and vision-language models (VLMs) as cognitive controllers, these systems can perceive degradation, reason about appropriate restoration strategies, orchestrate specialized tools, evaluate results, and iteratively refine their approach. This section provides a comprehensive analysis of the emerging agent-based restoration paradigm.

\subsection{The AgenticIR Framework}

AgenticIR\cite{Zhu2025AgenticIR} established the foundational framework for agent-based image restoration. The system operates through a five-stage pipeline: (1) \textit{Perception}: a VLM analyzes the input image to identify degradation types and severity levels; (2) \textit{Scheduling}: an LLM generates an ordered sequence of restoration tools based on the perceived degradation; (3) \textit{Execution}: the selected tools are applied to the image in the planned order; (4) \textit{Reflection}: the VLM evaluates the restored result against the original to assess quality improvement; and (5) \textit{Rescheduling}: if the result is unsatisfactory, the system backtracks and explores alternative tool combinations through depth-first search.

This framework represents a fundamental departure from all prior approaches. Unlike single-task models that apply a fixed transformation, or AiO models that process all degradation types through a single pipeline, AgenticIR can dynamically compose arbitrary sequences of specialized tools, each optimized for a specific degradation type. The reflection mechanism enables self-evaluation and iterative refinement, capabilities that no feed-forward model possesses. However, AgenticIR's depth-first search strategy leads to exponential computational cost in the worst case, and its reliance on powerful LLMs and VLMs for real-time perception and reasoning creates significant latency overhead, making deployment challenging for time-sensitive applications.

In a closely related and complementary line of work, Zhu et al.\cite{Zhu2024An} developed an intelligent agentic system for complex image restoration problems that extends the agent-based paradigm along several critical dimensions. While AgenticIR focuses primarily on degradation type identification and tool sequencing, this system introduces a more sophisticated problem decomposition strategy that explicitly addresses the complexity hierarchy of restoration tasks. The system classifies restoration problems into three complexity levels---simple (single, identifiable degradation), moderate (multiple known degradation types), and complex (unknown or spatially varying degradation)---and adapts its reasoning depth accordingly. For simple problems, the agent applies a single-step tool selection, minimizing latency overhead. For moderate problems, it engages in multi-step planning with degradation ordering optimization. For complex problems, the agent activates a full deliberation pipeline that includes spatial analysis, degradation hypothesis generation, and multi-hypothesis verification. This adaptive complexity mechanism represents an important advance toward practical deployment, as it avoids the computational overhead of full deliberation for straightforward restoration tasks while retaining deep reasoning capability for genuinely challenging scenarios. Furthermore, the system incorporates a memory module that accumulates restoration experience across sessions, enabling the agent to recognize previously encountered degradation patterns and retrieve successful restoration strategies without repeating the full reasoning process. This experience-driven approach draws parallels with case-based reasoning in classical AI and suggests a path toward agent systems that improve with deployment experience rather than requiring periodic retraining.

\subsection{Multi-Agent Architectures}

Recognizing the computational limitations of single-agent approaches, recent works have explored multi-agent architectures for more efficient restoration. MAIR\cite{Jiang2025Image} introduces a multi-agent framework with three-stage degradation priors that guides tool selection. The multi-agent design enables parallel processing of different aspects of the restoration task, reducing sequential dependencies and improving efficiency. Specifically, MAIR establishes a degradation prior ordering principle (compression artifacts $\to$ imaging degradation $\to$ scene degradation) that constrains the search space and achieves a 44\% improvement in inference efficiency compared to AgenticIR while maintaining comparable restoration quality.

Tripathi et al.\cite{Tripathi2025Advancements} analyzed recent advancements in agentic AI architectures and prompting strategies, including their application to visual tasks. The multi-agent paradigm enables a hierarchical organization where a global coordinator agent manages high-level strategy while specialized sub-agents handle specific aspects of the restoration task, such as region-specific processing, quality monitoring, and parameter optimization. This hierarchical structure mirrors the organization of complex real-world systems and offers natural scalability as the number of degradation types and tools increases.

\subsection{Quality-Driven Restoration Planning}

RestoreAgent\cite{Chen2024Autonomous} takes a different approach by fine-tuning a multimodal large language model to directly predict restoration workflows. Rather than using an LLM as a general-purpose reasoner that calls specialized tools, RestoreAgent trains the model end-to-end on restoration task-plan pairs, enabling faster inference by eliminating the need for multi-turn reasoning. The model learns to map visual observations directly to tool sequences, effectively compressing the multi-step reasoning process into a single forward pass.

Q-Agent\cite{Zhou2025QAgent} addresses the efficiency challenge from the perspective of search strategy, proposing a quality-driven greedy approach that selects the next restoration tool based on the expected quality improvement, as measured by an image quality assessment model. Unlike AgenticIR's exhaustive depth-first search, Q-Agent's greedy strategy achieves linear computational complexity with respect to the number of available tools, making it significantly more practical for deployment. The key insight is that quality-driven greedy selection, while not guaranteed to find the globally optimal tool sequence, produces results that are competitive with exhaustive search in practice.

\subsection{Spatial Awareness in Agent-Based Systems}

A critical limitation of current agent-based systems is their treatment of the entire image as a single entity. All existing agent-based methods---AgenticIR, RestoreAgent, MAIR, and Q-Agent---apply their restoration plans globally, without considering that degradation may vary dramatically across different spatial regions of an image. This represents a significant missed opportunity, as the agent framework is naturally suited to spatial decomposition: a coordinating agent could segment the image into regions, analyze the degradation characteristics of each region independently, assign specialized sub-agents to handle each region, and then fuse the results.

The tools needed for such spatial awareness already exist. The Segment Anything Model (SAM)\cite{Kirillov2023Segment} provides zero-shot semantic segmentation that can decompose an image into meaningful regions without task-specific training. VLMs such as DepictQA\cite{You2023DepictQA} can analyze individual regions to identify degradation types and severity. Chen et al.\cite{Chen2024Segment} demonstrated RobustSAM, which maintains segmentation quality even on degraded images, addressing a key concern about applying segmentation to corrupted inputs. The combination of region-level segmentation, degradation analysis, and agent-based tool orchestration represents a natural and promising extension of the current paradigm toward spatially-aware multi-agent restoration.


\section{Foundation Models for Degradation Analysis and Quality Assessment}

The effectiveness of agent-based and spatially-aware restoration systems depends critically on two capabilities: accurately perceiving and characterizing image degradation, and reliably assessing restoration quality. Foundation models---including segment anything models and vision-language models---provide the perceptual backbone for these capabilities. This section examines how foundation models enable degradation analysis and quality assessment within restoration pipelines.

\subsection{Segment Anything for Spatial Decomposition}

The Segment Anything Model (SAM)\cite{Kirillov2023Segment}, trained on over one billion masks, provides zero-shot segmentation capabilities that can decompose any image into semantically coherent regions without task-specific fine-tuning. For spatially-aware image restoration, SAM's ability to generate high-quality region masks enables the decomposition of a degraded image into spatial units that can be independently analyzed and processed. Each region inherits semantic meaning (sky, building, vegetation, road) that correlates with likely degradation patterns: sky regions are more susceptible to haze, while road surfaces are more affected by rain-induced reflections.

Ren et al.\cite{Ren2024Grounded} proposed Grounded SAM, which combines text-prompted object detection with SAM's segmentation capability, enabling text-guided region identification. For restoration applications, this allows specifying regions of interest through natural language (e.g., ``the foggy sky region'' or ``the noisy shadow area''), providing an intuitive interface for spatially-aware degradation analysis. Chen et al.\cite{Chen2024Segment} developed RobustSAM, specifically addressing the challenge that SAM's segmentation quality degrades on corrupted images. RobustSAM fine-tunes SAM with degradation-aware training data, maintaining segmentation accuracy on images affected by noise, blur, and other distortions, making it directly applicable to the first stage of spatially-aware restoration pipelines.

\subsection{Vision-Language Models for Degradation Assessment}

Vision-language models (VLMs) enable rich, descriptive assessment of image degradation that goes beyond simple classification. DepictQA\cite{You2023DepictQA} leverages VLMs for depicted image quality assessment, providing detailed natural language descriptions of image quality attributes including specific degradation types, severity levels, and affected regions. You et al.\cite{You2024Depicting} extended this approach with advanced VLM architectures that can perform comparative quality assessment between image pairs, enabling before-and-after evaluation of restoration results. You et al.\cite{You2024Enhancing} further enhanced descriptive image quality assessment with large VLM architectures, demonstrating improved consistency and accuracy in degradation identification.

Wu et al.\cite{Wu2024Improving} developed Q-Instruct, a large-scale instruction-following dataset specifically designed for training VLMs on low-level visual perception tasks. Models fine-tuned on Q-Instruct demonstrate significantly improved ability to identify and describe image degradation compared to general-purpose VLMs, providing the perceptual foundation needed for agent-based restoration systems. The companion Q-Bench benchmark\cite{Wu2023A} and its extension Q-Bench+\cite{Zhang2024A} establish standardized evaluation protocols for VLM-based quality assessment, covering perception, description, and comparison tasks that directly correspond to the requirements of restoration-oriented degradation analysis.

Zhang et al.\cite{Zhang2024On} proposed Q-Boost, which employs a triadic-tone and multi-prompt strategy to enhance VLMs' quality assessment abilities, demonstrating that prompt engineering can significantly improve VLM performance on low-level visual tasks without additional training. Wu et al.\cite{Wu2024A} conducted a comprehensive study of multimodal large language models for image quality assessment, providing systematic comparisons that inform the selection of appropriate VLM backbones for restoration applications.

\subsection{Foundation Model Priors for Real-World Restoration}

A rapidly growing body of work leverages the rich visual and semantic priors encoded in foundation models---including large diffusion models, vision-language models, and pre-trained vision transformers---to address the challenging problem of real-world image restoration, where degradation is complex, spatially varying, and does not conform to simple parametric models. Ai et al.\cite{Ai2024Image} proposed DreamClear, a high-capacity real-world image restoration framework that combines a large-scale diffusion model with a privacy-safe dataset curation pipeline. DreamClear's key insight is that the generative capacity of a diffusion model can be harnessed as a powerful restoration prior when conditioned on degradation-aware features extracted from the input image. The privacy-safe dataset curation approach addresses the practical concern that large-scale restoration datasets may contain identifiable personal information, proposing automated de-identification and filtering mechanisms that enable the construction of high-capacity training sets without compromising individual privacy. DreamClear achieves state-of-the-art results on multiple real-world restoration benchmarks, demonstrating that the combination of large model capacity and large-scale curated data is essential for handling the diversity of real-world degradation.

Luo et al.\cite{Luo2024Image} developed a method for photo-realistic image restoration in the wild using controlled vision-language models, where a VLM is employed not only for degradation perception but also as an active participant in the restoration process through controlled generation. The VLM generates natural language descriptions of the desired restoration outcome, which are then used as conditioning signals for a controlled diffusion model that produces photo-realistic restorations. This VLM-in-the-loop approach enables a form of semantic guidance that ensures restored images are not only technically clean but also semantically coherent---preserving the intended mood, lighting, and composition of the original scene. Chen et al.\cite{Chen2025with} proposed real-world super-resolution with VLM-based degradation prior learning, which uses a vision-language model to predict detailed degradation parameters---including noise level, blur kernel estimation, and compression quality factor---from the input image through a learned mapping between visual features and degradation descriptions. These VLM-predicted degradation priors are then fed to a conditional restoration network, providing significantly more informative conditioning than traditional blind degradation estimation methods that rely on simple neural network regressors.

Cui et al.\cite{Cui2025Enhancing} addressed the specific application of image restoration for autonomous vehicle perception, proposing a multi-scale feature modulation network that enhances images captured under adverse conditions to improve downstream detection and segmentation performance. Their work highlights an important emerging application paradigm: restoration not as an end in itself, but as a preprocessing step whose quality should be measured by its impact on downstream task performance rather than pixel-level fidelity metrics alone. The multi-scale feature modulation design enables the restoration network to selectively enhance features at scales most relevant to the autonomous driving perception tasks, achieving improved detection accuracy under fog, rain, and low-light conditions with minimal computational overhead. These works collectively demonstrate that foundation model priors---whether from diffusion models, vision-language models, or pre-trained vision transformers---provide a powerful and general mechanism for closing the gap between synthetic-data-trained restoration methods and the demands of real-world deployment.

\subsection{No-Reference Image Quality Assessment}

Classical no-reference image quality assessment (NR-IQA) methods remain important components of restoration pipelines, providing efficient quality scores that can guide tool selection and termination decisions. Talebi and Milanfar\cite{Talebi2018Neural} introduced NIMA (Neural Image Assessment), a CNN-based NR-IQA method that predicts the distribution of human quality ratings rather than a single score, providing both a quality estimate and an uncertainty measure. Zhang et al.\cite{Zhang2018The} demonstrated the unreasonable effectiveness of deep features for perceptual quality assessment through LPIPS (Learned Perceptual Image Patch Similarity), which uses features from pre-trained networks to compute perceptual similarity metrics that closely correlate with human judgments.

Zheng et al.\cite{Zheng2021Learning} proposed a conditional knowledge distillation approach for degraded-reference image quality assessment, specifically designed for evaluating image restoration quality by leveraging both the degraded input and the restored output. Li et al.\cite{Li2021Motion} demonstrated the integration of quality assessment guidance into the restoration process itself, using IQA scores as feedback signals to guide motion blur removal, establishing the concept of quality-driven restoration that is central to agent-based approaches like Q-Agent.


\section{Model Compression and Lightweight Deployment}
\label{sec:compression}

The computational demands of advanced restoration methods---particularly Transformer-based, diffusion-based, and agent-based approaches---present significant barriers to deployment on resource-constrained devices. This section examines knowledge distillation and model compression techniques specifically designed for image restoration, with particular attention to trajectory distillation as a mechanism for transferring complex multi-step reasoning into lightweight models.

\subsection{Knowledge Distillation for Restoration Networks}

Knowledge distillation, which transfers knowledge from a large teacher model to a compact student model, has been specifically adapted for image restoration with domain-aware innovations. Zhang et al.\cite{Zhang2025Soft} proposed soft knowledge distillation with multi-dimensional cross-net attention (SKD) for image restoration compression, where the student learns to replicate not only the teacher's outputs but also its intermediate attention patterns across spatial and channel dimensions. This multi-dimensional distillation preserves the teacher's ability to capture both local texture details and global structural coherence, achieving significant compression (e.g., 2--4$\times$ reduction in parameters) with minimal quality degradation.

Yang et al.\cite{Yang2025Image} introduced Mamba-oriented heterogeneous knowledge distillation for image restoration model compression, leveraging the efficiency of the Mamba state-space architecture for the student model while distilling knowledge from a larger Transformer-based teacher. This cross-architecture distillation demonstrates that knowledge can be effectively transferred between fundamentally different model families, enabling the deployment of Transformer-quality restoration through efficient Mamba-based models. Zhang et al.\cite{Zhang2025Knowledge}\cite{Zhang2025Simultaneous} proposed simultaneous learning knowledge distillation (SLKD) for image restoration that jointly optimizes the teacher and student models during distillation, avoiding the two-stage training pipeline and achieving better teacher-student alignment.

Wang et al.\cite{Wang2024Distillation} developed data-free distillation with degradation-prompt diffusion for multi-weather restoration, addressing the practical challenge of knowledge distillation when the original training data is unavailable. By using a diffusion model to generate synthetic degraded images conditioned on degradation prompts, the method enables distillation without access to the teacher's training data, which is particularly valuable for deployment scenarios with data privacy constraints.

\subsection{Trajectory Distillation for Agent-Based Systems}

A particularly promising direction for lightweight deployment of agent-based restoration is trajectory distillation, which compresses the complex multi-step decision-making process of an agent system into a lightweight feedforward network. The concept involves running the full agent system (with LLMs, VLMs, and specialized tools) offline on a large corpus of degraded images, recording the complete decision trajectories---including region segmentation, degradation identification, tool selection, parameter settings, and quality scores---and then training a compact network to directly predict these decisions from the input image alone.

This approach offers several compelling advantages: (1) it eliminates the need for expensive LLM/VLM inference at deployment time, enabling real-time processing; (2) the compact network can be trained with multi-task learning objectives covering segmentation, classification, routing, and parameter prediction simultaneously; and (3) the quality of the training signal is bounded by the agent system's performance, ensuring that the distilled model inherits the agent's ability to handle complex, spatially heterogeneous degradation. The key challenge lies in ensuring that the lightweight model can faithfully replicate the agent's nuanced decision-making, particularly for edge cases and novel degradation combinations not well-represented in the trajectory dataset.


\section{Discussion}

The evolution from single-task restoration to agent-based spatially-aware systems reveals several cross-cutting themes and open challenges that merit deeper examination.

\subsection{The Quality--Efficiency Trade-off}

A persistent tension runs through all restoration paradigms: methods that achieve higher restoration quality tend to require significantly more computational resources. Single-task Transformer models such as Restormer\cite{Zamir2022Efficient} achieve excellent quality--efficiency balance for their specific tasks but cannot handle multiple degradation types. AiO models sacrifice some per-task quality for versatility. Diffusion models achieve the best perceptual quality but at orders-of-magnitude higher computational cost. Agent-based systems offer the most flexible and potentially highest-quality restoration but currently require multiple rounds of LLM/VLM inference.

This progression suggests that the field has not yet found an approach that simultaneously achieves top-tier quality, broad versatility, and low computational cost. Trajectory distillation represents the most promising path toward resolving this trilemma, but its effectiveness depends on the coverage and quality of the trajectory data, the capacity of the lightweight student model, and the ability to generalize to degradation scenarios not encountered during offline agent operation.

\subsection{Synthetic vs. Real-World Degradation}

Most restoration methods are developed and evaluated on synthetically degraded images, where clean ground truth is available for supervised training and quantitative evaluation. However, real-world degradation differs significantly from synthetic models: it is spatially non-uniform, involves complex interactions between multiple degradation types, varies with scene content and imaging conditions, and cannot be precisely characterized by mathematical models. Zhai et al.\cite{Zhai2023A} provided a comprehensive review of real-world image super-resolution, highlighting the substantial performance gap between synthetic and real-world benchmarks. Guan et al.\cite{Guan2025A} contributed WeatherBench, a real-world benchmark for all-in-one weather degradation, providing more realistic evaluation conditions.

Agent-based and spatially-aware approaches are particularly well-suited to bridge this gap, as they can adaptively respond to the specific degradation characteristics observed in each image rather than relying on assumptions about the degradation model. The combination of VLM-based degradation perception and dynamic tool orchestration enables a more flexible and robust response to the unpredictable nature of real-world degradation.

\subsection{Spatial Awareness as the Missing Dimension}

Perhaps the most significant insight emerging from this review is that spatial awareness remains underdeveloped across all restoration paradigms. Single-task and AiO methods process the entire image uniformly. Diffusion models apply the same generative process globally. Even current agent-based systems treat the image as a monolithic entity when making restoration decisions. The few methods that incorporate spatial information do so in limited ways: spatially-varying convolution kernels, region-specific attention mechanisms, or local-global feature fusion.

True spatial awareness requires decomposing the image into meaningful regions, independently analyzing the degradation characteristics of each region, planning region-specific restoration strategies, executing those strategies with appropriate tools and parameters, and seamlessly fusing the results. This capability would address a fundamental limitation of all current approaches and enable significantly improved restoration of real-world images with heterogeneous degradation.


\section{Perspectives and Future Directions}

Based on the analysis presented in this review, we identify several promising research directions that can advance the field toward more intelligent, adaptive, and deployable image restoration systems.

\subsection{Hierarchical Multi-Agent Restoration with Spatial Awareness}

The most immediate opportunity lies in extending agent-based restoration with spatial awareness through a hierarchical multi-agent architecture. A global coordinating agent would analyze the overall image and decompose it into regions using foundation models like SAM. Regional expert agents would independently analyze and restore each region, selecting from a toolkit of specialized restoration models. A quality assessment agent would evaluate both regional and global restoration quality, triggering iterative refinement when needed. This architecture naturally supports the spatial heterogeneity of real-world degradation while maintaining the adaptive flexibility of agent-based systems.

\subsection{Continuous Parameter Optimization}

Current agent-based systems primarily make discrete tool selection decisions: choosing which restoration model to apply. A natural extension is continuous parameter optimization, where agents not only select tools but also tune their operating parameters. For example, a denoising tool might be applied with different noise level estimates for different image regions, or a super-resolution model might use different upscaling factors depending on the local detail content. This continuous parameter space dramatically increases the flexibility and precision of agent-based restoration, but also requires more sophisticated optimization strategies---potentially combining reinforcement learning with quality-driven feedback.

\subsection{Trajectory Distillation for Edge Deployment}

Bridging the gap between the high-quality restoration of agent-based systems and the real-time requirements of edge deployment requires effective knowledge compression. Trajectory distillation, where a lightweight network learns to replicate the agent's decision-making from collected trajectory data, represents the most promising approach. Key research challenges include: designing multi-task student architectures that can simultaneously predict region segmentation, degradation types, tool routing, and continuous parameters; developing trajectory selection and weighting strategies that prioritize diverse and challenging examples; and establishing evaluation protocols that assess not only per-image restoration quality but also decision consistency and robustness.

\subsection{Cross-Domain Generalization and Benchmarking}

The field needs standardized benchmarks for evaluating spatially-aware and agent-based restoration. Current benchmarks primarily evaluate per-task performance on synthetically degraded images, which does not capture the complexity of real-world scenarios. Future benchmarks should include images with spatially varying heterogeneous degradation, evaluation metrics that assess region-level restoration quality in addition to global metrics, and practical considerations such as computational efficiency and deployment constraints. Cross-domain evaluation---testing methods trained on natural images on medical, remote sensing, or industrial imaging tasks---would also reveal the generalization capabilities and limitations of different approaches.


\section{Conclusion}

This review has traced the evolution of image restoration from single-task Transformer architectures through all-in-one unified models and diffusion-based generative approaches to the emerging paradigm of agent-based spatially-aware systems. Each paradigm addresses specific limitations of its predecessors: Transformer architectures introduced global context modeling that surpassed CNNs; all-in-one models eliminated the need for degradation-specific networks; diffusion models achieved unprecedented perceptual quality through generative priors; and agent-based systems introduced dynamic reasoning, tool orchestration, and iterative refinement.

The critical gap that remains is spatial awareness. Real-world images exhibit spatially heterogeneous degradation that cannot be adequately addressed by any approach that treats the image uniformly. The convergence of foundation models for spatial decomposition (SAM and its variants), vision-language models for region-level degradation analysis (DepictQA and related methods), and multi-agent frameworks for coordinated restoration planning creates a unique opportunity to address this gap. Combined with trajectory distillation for lightweight deployment, spatially-aware multi-agent restoration has the potential to establish a new paradigm that is simultaneously more accurate, more adaptive, and more deployable than existing approaches.

The field stands at an inflection point where the integration of perception, reasoning, and action---long the aspiration of artificial intelligence research---is becoming practically achievable for the concrete and important problem of image restoration. The research directions identified in this review---hierarchical multi-agent architectures, continuous parameter optimization, trajectory distillation, and comprehensive benchmarking---provide a roadmap for realizing this potential.

\bibliography{multi_agent_spatially_aware_image_restoration_refs}

\end{document}

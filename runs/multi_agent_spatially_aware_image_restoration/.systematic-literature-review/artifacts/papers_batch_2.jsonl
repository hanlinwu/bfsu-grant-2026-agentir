{"openalex_id": "https://openalex.org/W2895357007", "doi": "https://doi.org/10.1101/gad.314617.118", "title": "Roles of the immune system in cancer: from tumor initiation to metastatic progression", "abstract": "The presence of inflammatory immune cells in human tumors raises a fundamental question in oncology: How do cancer cells avoid the destruction by immune attack? In principle, tumor development can be controlled by cytotoxic innate and adaptive immune cells; however, as the tumor develops from neoplastic tissue to clinically detectable tumors, cancer cells evolve different mechanisms that mimic peripheral immune tolerance in order to avoid tumoricidal attack. Here, we provide an update of recent accomplishments, unifying concepts, and future challenges to study tumor-associated immune cells, with an emphasis on metastatic carcinomas.", "authors": ["Hugo González", "Catharina Hagerling", "Zena Werb"], "year": 2018, "venue": "Genes & Development", "cited_by_count": 2085, "type": "review", "concepts": ["Immune system", "Biology", "Cancer", "Cytotoxic T cell", "Cancer research"], "search_query": "all-in-one image restoration unified model degradation"}
{"openalex_id": "https://openalex.org/W2768033029", "doi": "https://doi.org/10.1083/jcb.201708092", "title": "Senescence and aging: Causes, consequences, and therapeutic avenues", "abstract": "Aging is the major risk factor for cancer, cardiovascular disease, diabetes, and neurodegenerative disorders. Although we are far from understanding the biological basis of aging, research suggests that targeting the aging process itself could ameliorate many age-related pathologies. Senescence is a cellular response characterized by a stable growth arrest and other phenotypic alterations that include a proinflammatory secretome. Senescence plays roles in normal development, maintains tissue homeostasis, and limits tumor progression. However, senescence has also been implicated as a major cause of age-related disease. In this regard, recent experimental evidence has shown that the genetic or pharmacological ablation of senescent cells extends life span and improves health span. Here, we review the cellular and molecular links between cellular senescence and aging and discuss the novel therapeutic avenues that this connection opens.", "authors": ["Domhnall McHugh", "Jesús Gil"], "year": 2017, "venue": "The Journal of Cell Biology", "cited_by_count": 1184, "type": "review", "concepts": ["Senescence", "Disease", "Biology", "Cellular senescence", "Cancer"], "search_query": "all-in-one image restoration unified model degradation"}
{"openalex_id": "https://openalex.org/W1977276601", "doi": "https://doi.org/10.1111/j.1442-8903.2006.00285.x", "title": "The meanings of vegetation condition", "abstract": "The meanings of vegetation condition. David Keith1 and Emma Gorrod1,2 (NSW Department of Environment & Conservation, PO Box 1967, Hurstville, NSW 2220. Tel.: (02) 9585 6498; Fax: (02) 9585 6606; E-mail: [email protected]; 2Botany Department, University of Melbourne, Parkville Vic. 3052, Australia). Key words: biodiversity conservation, conservation policy, natural resource management, uncertainty, vegetation assessment. Introduction. ‘Vegetation condition’ is a concept that has rapidly gained currency in recent land management policy within Australia. Recent applications of the concept have been developed to assist decision-making for incentive payments, clearing approvals and offset actions in the management of native vegetation. In general terms, ‘condition’ means state of being or health. When applied in biology at the scale of individuals, it refers to fitness – the ability of an individual to survive and reproduce (Begon et al. 1986). Coops et al. (2004) provide an example of such an application in which the health of eucalypt trees is assessed using a spectral reflectance algorithm. At higher levels of organization (communities, ecosystems, landscapes), the ecological meaning of ‘condition’ is less clear. We argue that confusion is at least in part due to historical legacies that have led to independent development and use of the term in several related, but different, ecological concepts. Like ‘rarity’, ‘condition’ has gained currency as one term with many meanings (Harper 1981), which is implicit within much policy and management discourse about vegetation condition (Thackway et al. 2005). In this note, we identify three main meanings of vegetation condition that are derived from the concepts of aesthetics, production and biodiversity. We briefly review these meanings with examples and discuss their interrelationships. We conclude that confusion about what ‘good condition’ means for contemporary management of native vegetation can be reduced with: (i) more explicit explanations accompanying usage of the term; (ii) further development of the condition concept to deal with the multidimensional properties of biodiversity; and (iii) explicit links to more robust models of vegetation dynamics than those currently implicit within assessments. Aesthetics, production and biodiversity. Aesthetic values derive from human perceptions about what makes ‘a good patch of bush’. These perceptions are, by definition, subjective and likely to vary between people and with landscape context. Nevertheless, scientific approaches to evaluation of natural aesthetic values have been developed for such purposes as heritage listing (UNESCO 2005) and wilderness identification (Lesslie & Maslen 1995). The concept of naturalness is central to both of these applications (Kirkpatrick & Haney 1980), and an area with high naturalness values may be considered aesthetically to be in ‘good condition’. An assessment methodology proposed by Lesslie et al. (1988) considers two forms of naturalness that are inversely related to the impact of human activities. One of these (aesthetic naturalness) is dependent on the presence of modern human structures, such as fences, tracks, dams, buildings, etc., and the other (biophysical naturalness) is dependent on the history and intensity of human land uses or their effects, such as logging, grazing, weed invasion, etc. These attributes may be mapped using distance functions, remote sensing, tenure data and historical information, which may be synthesized in GIS applications to calculate naturalness values across a landscape (e.g. Lesslie et al. 1988). Although these assessment methodologies were developed for landscapes containing large natural areas, some of the principles are also applicable to finer scales. Relevant indicators of human disturbance, such as cut stumps, fences, tracks, soil compaction, etc., may be used to assess the naturalness of remnant patches. More generally, comparative approaches may be used to assess the aesthetic values of native vegetation by comparing particular sites with reference examples or ‘benchmarks’ that define one or more levels of aesthetic value. Production values derive from the ability of native vegetation to deliver resources for human consumption. A patch of native vegetation may be considered to be in ‘good condition’ if it maintains a high capacity to produce resources such as livestock, timber, water, and other ‘ecosystem services’ (Heal 2000). The scientific basis for assessing these values derives from a long tradition in agriculture, particularly for the assessment of pasture condition. Landscape Function Analysis (Ludwig & Tongway 1992) provides an example of condition assessment with a strong theoretical basis and well-developed practical methodologies. The approach was originally developed in arid and semiarid rangelands used as native pastures for livestock production. Its central concept is based on the capacity of land to retain resources that are essential for plant growth, particularly water, soils and their nutrients. Assessments are focused on surface features (such as cover of perennial plants, soil lichens, leaf litter and woody debris) that interrupt, divert or absorb runoff and transported materials, and hence reduce loss of resources. Sites and landscapes with an abundance of such features are in ‘good condition’, able to sustain production of fodder and are relatively robust to degradation during drought or heavy rain. Landscape function, hence condition, may be degraded by overgrazing, which reduces the abundance of critical surface features and the capacity of the land to retain resources and sustain livestock. Assessments of rangeland condition therefore provide important guidance for sustainable management of livestock grazing. Methodologies are available for site-level assessments (Ludwig & Tongway 1992) and landscape scales using satellite image analysis and modelling of grazing halos around watering points (Bastin & Ludwig 2006). Biodiversity values in part relate to the capacity of native vegetation to sustain local populations of native plants and animals (as well as their genetic diversity and ecological interactions). ‘Good condition’ in this sense is related to high carrying capacity from population theory or high ‘habitat quality’ (Begon et al. 1986). However, unlike these species-specific parameters, the concept of condition for biodiversity applies to all species that may be reasonably expected to use a site (Parkes et al. 2003). Because the habitat requirements of all species are not known, assessment methods are based on surrogates that represent habitat features for some better-known groups of species (e.g. density of large trees), as well as features that, conversely, are indicators of habitat degradation for certain groups of species (e.g. weed abundance). Available site assessment methods (e.g. Gibbons & Freudenberger 2006) differ in their weightings for different attributes, as well as the mathematical details of index calculation. These variations could influence the performance of alternative methods for different biotic groups and habitats. Recent approaches incorporate reference or benchmark values in the assessment to standardize assessments across different types of habitat. Landscape-scale approaches for assessing the biodiversity component of vegetation condition based on remote sensing and spatial modelling are currently under development (e.g. Newell et al. 2006). Confusion in the usage of ‘condition’. Each of the three main facets of vegetation condition share commonalities. For example, in forest and woodland vegetation, large trees contribute to aesthetic value through visual splendour; they contribute to production value if they represent a timber resource and also to biodiversity value by providing habitat for a variety of organisms. An abundance of large trees should therefore be an indicator of good condition across a range of value types. On the other hand, each facet of condition has unique elements that are not shared by others. Tree hollows of a particular size and configuration confer biodiversity value through provision of habitat for particular tree-dwelling vertebrates, but might be negatively related to production value if they render the timber unusable. Thus, any given patch in a landscape may have any combination of high and low values for each of the three components (Fig. 1). Diagrammatic representation of the ‘condition space’ for areas of native vegetation. Site 1 has high values for all three components. Site 2 has high values for production and biodiversity components, but low aesthetic value. Site 3 has low values for all three components. The space that may be identified as vegetation in ‘good condition’ is discussed in the text. Confusion may emerge when different meanings of condition are applied to the values represented in Figure 1. For example, some may interpret a site as being in ‘good condition’ only if it contains high values for each of the aesthetic, production and biodiversity components. Others may interpret a site as being in ‘good condition’ if it contains high values for any one of the three components. Still others may interpret a site as being in ‘good condition’ if it contains high values for one particular component (e.g. aesthetic), irrespective of its values for other components (production and biodiversity). Further scope for confusion emerges when one considers that aesthetic, production and biodiversity values are each comprised of a number of components that may not be well correlated with one another. These simple examples illustrate what Regan et al. (2002) defined as ambiguity – a form of uncertainty in which a term may have more than one meaning. We contend that ambiguity, along with underspecificity, context-dependence, vagueness and other forms of linguistic uncertainty (Regan et al. 2002), are pervasive in both the assessment and planning dialogue for vegetation condition. If the condition concept is to be used effectively in decision support, linguistic uncertainty needs to be reduced by articulating explicitly the scope, context and meaning of ‘condition’ in each application. Beyond semantics: Conceptual difficulties and further development. Although more disciplined usage would benefit practical applications of the term, more fundamental problems stem from the efficacy of ‘condition’ as a model of vegetation state and change. At the heart of its use in policy and management is the presumption that vegetation condition represents a generalized measure of ecosystem function and habitat suitability for native biota, which varies along a dynamic continuum of degradation and restoration. The underlying model of change is implicitly Clementsian (Begon et al. 1986), and has been subject to little scientific scrutiny. One of the main difficulties in applying a concept of vegetation condition stems from its univariate property – condition varies on a single scale from ‘good’ to ‘poor’. The univariate property of condition does not sit well with its common interpretation, which demands generalization across multiple values that may not be well correlated with one another. This is especially evident for biodiversity, which comprises many species with habitat requirements that may be poorly correlated or inversely correlated with one another – what may be good habitat for one species may be poor for another. The possible reduction of suitable habitat (open grassland) for the endangered Plains Wanderer (Pedionomus torquatus) in response to exclusion of grazing required to encourage regeneration of Myall (Acacia pendula) Woodland, also endangered, is a case in point (NPWS 2002). In general, the nature of these correlations within habitat types is poorly understood, but is crucial to understanding the dynamics of degradation, regeneration and restoration in vegetation. Further theoretical development of the condition concept is needed to overcome these practical difficulties. In particular, vegetation condition for biodiversity is in need of a theory of dynamics that unifies its components and defines ecological processes that mediate vegetation change. It is noteworthy that applications of the condition concept that have a strong theoretical basis, including the Eucalypt Canopy Condition Index (Coops et al. 2004) and Landscape Function Analysis (Ludwig & Tongway 1992), deal with established causal links between indicators and a univariate response (leaf productivity and soil productivity, respectively). Existing models of succession and diversity (Connell & Slatyer 1977; Westoby et al. 1989; Silvertown 2004) provide some guidance for future development of the condition concept and represent a considerable advance on simplistic Clementsian thinking that underpins current policies aimed at improving biodiversity values across landscapes.", "authors": ["David A. Keith", "Emma Gorrod"], "year": 2006, "venue": "Ecological Management & Restoration", "cited_by_count": 27, "type": "article", "concepts": ["Vegetation (pathology)", "Clearing", "Payment", "Meaning (existential)", "Incentive"], "search_query": "all-in-one image restoration unified model degradation"}
{"openalex_id": "https://openalex.org/W4226042837", "doi": "https://doi.org/10.1609/aaai.v36i2.20072", "title": "Image-Adaptive YOLO for Object Detection in Adverse Weather Conditions", "abstract": "Though deep learning-based object detection methods have achieved promising results on the conventional datasets, it is still challenging to locate objects from the low-quality images captured in adverse weather conditions. The existing methods either have difficulties in balancing the tasks of image enhancement and object detection, or often ignore the latent information beneficial for detection. To alleviate this problem, we propose a novel Image-Adaptive YOLO (IA-YOLO) framework, where each image can be adaptively enhanced for better detection performance. Specifically, a differentiable image processing (DIP) module is presented to take into account the adverse weather conditions for YOLO detector, whose parameters are predicted by a small convolutional neural network (CNN-PP). We learn CNN-PP and YOLOv3 jointly in an end-to-end fashion, which ensures that CNN-PP can learn an appropriate DIP to enhance the image for detection in a weakly supervised manner. Our proposed IA-YOLO approach can adaptively process images in both normal and adverse weather conditions. The experimental results are very encouraging, demonstrating the effectiveness of our proposed IA-YOLO method in both foggy and low-light scenarios. The source code can be found at https://github.com/wenyyu/Image-Adaptive-YOLO.", "authors": ["Wenyu Liu", "Gaofeng Ren", "Runsheng Yu", "Shi Guo", "Jianke Zhu", "Lei Zhang"], "year": 2022, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 496, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Object detection", "Convolutional neural network", "Code (set theory)"], "search_query": "all-in-one image restoration unified model degradation"}
{"openalex_id": "https://openalex.org/W2004936701", "doi": "https://doi.org/10.1097/01.blo.0000175122.50804.ce", "title": "Local Antibiotic Delivery Systems", "abstract": "The use of local antibiotic delivery systems has become an accepted treatment method that continues to evolve for a variety of reasons. There has been an explosion of new technologies that are designed to facilitate the delivery of local antibiotics in new and creative ways. The primary reason for using these local antibiotic delivery vehicles is the ability to achieve very high local concentrations of antibiotics without associated systemic toxicity. In the typical infected wound environment, which frequently has zones of avascularity, the ability to achieve high levels of antibiotics in these otherwise inaccessible areas is highly desirable.5 Additional reasons for use of these delivery vehicles include the desire to treat remaining planktonic organisms and sessile organisms in biofilms more effectively with high concentrations of antibiotics. Because bone regeneration often is required as a part of the treatment plan, a recent trend has been simultaneously to provide a framework of osteoinductive and osteoconductive materials along with antibiotic delivery.11 Despite the rapid acceptance of these antibiotic delivery vehicles, there are many unanswered questions related to their use, particularly when viewed within the environment of biofilms that were discussed in this symposium. Some of the questions discussed are outlined below. How high do we need to get the concentration of these antibiotics and what is the desirable duration of these high levels? What are the adverse clinical consequences of extremely high levels of antibiotics, particularly at the local level and specifically with regard to the process of bone regeneration? Are new and creative dosing regimens required with multiple combinations of antibiotics to address unique patterns of resistance associated with biofilms? Can the traditional methods of assessing antimicrobial susceptibility of pathogenic organisms be extrapolated to the efficacy obtained with these high concentrations of antibiotics? What types of models are required to study the efficacy of antibiotics on sessile organisms contained within biofilms? Are there nonantimicrobial options to consider that can augment the efficacy of current antimicrobials? And finally, how do clinical investigators actively use these new concepts and rapidly changing technologies when the burdens of federal regulation seem to prevent their use? High Local Antibiotic Concentrations Local antibiotic delivery vehicles are capable of achieving extremely high local tissue concentrations when compared with the antibiotic tissue levels obtained with traditional systemic antibiotic therapy. Antibiotic concentrations with local antibiotic delivery vehicles that reach levels of 3800 μg/mL21 and 4746 μg/mL24 have been reported. Although the symposia participants all agreed that the higher levels obtained with local delivery seem desirable and present the potential for improved efficacy, it was recognized that very little data are available to determine the optimal level of these antibiotics that are required for efficacy against these pathogenic organisms. The optimal antibiotic concentration likely is to be variable depending on the characteristics of the organism, the organism’s bioenvironment, and the specific antibiotic being used. One of the most obvious concerns regarding extremely high levels of local antibiotics is the potential for systemic toxicity, particularly in patients with abnormal renal function. Although there is considerable clinical experience to suggest that systemic toxicity associated with the use of high-dose antibiotic-loaded cement (ABLC) spacers is relatively rare, this remains a notable concern.32 One of the reasons for this concern is the inability to control the variables of antibiotic elution accurately in the postsurgical wound environment. The potential exists for large depots of antibiotics to be released erratically as a result of unexpected degradation patterns with newer biodegradable delivery vehicles. The concern regarding extremely high concentrations of local antibiotics particularly is valid in the face of dose-dependant, adverse consequences of bone regeneration with many currently used antibiotics.15,16,17,18,22 The need to profile the advantages of increasing higher doses of antibiotics must be balanced with the adverse effects of each specific antibiotic so that dosing regimens can be created and understood. It is also recognized that the antibiotic release characteristics of the various delivery systems need to be documented carefully so that the antibiotic concentrations that are achieved are known when these implants are used in vivo. Biofilms It is well recognized that biofilms play an important role in many chronic bacterial infections.6,13 Using tools such as the scanning electron microscope and the confocal laser scanning microscope, it is now understood that biofilms are not unstructured, homogeneous deposits of cells and accumulated slime, but complex communities of surface-associated cells enclosed in a polymer matrix containing open water channels.9 Current intervention strategies against biofilm infections are designed to prevent initial device colonization, to minimize microbial cell attachment to the device, and to penetrate or to disrupt the biofilm matrix to kill the associated cells.9 One of the most effective strategies to deal with biofilms in orthopaedic infection has been to remove the affected device or tissue that harbors the biofilm completely.13 The diagnostic and therapeutic strategies developed for acute bacterial diseases have not yielded accurate data or favorable outcomes when applied to these biofilm diseases.6 Bacteria residing within the biofilms (sessile) are less susceptible to antimicrobial agents than free-living (planktonic) cells. It is also known that young sessile cells are not as resistant to therapy as are older sessile cells associated with chronic infection.2 Several mechanisms have been proposed to explain this phenomenon of resistance within biofilms. Sessile bacteria may survive antibiotic exposure because of delayed penetration of the antimicrobial into the biofilm, marked slowing of growth rate of organisms within biofilms, but also because of an increased frequency of resistance traits within these sessile organisms.7,8 Practical implications of the altered response of sessile organisms within biofilm is that alternative strategies must be devised for susceptibility testing of these organisms and that new antimicrobial dosing regimens may be required to treat organisms within biofilms. Some of the possibilities include using higher concentrations of antibiotic, combination antimicrobial therapy that creates synergy,25 repetitive pulsing of antibiotic delivery,6 and nonantimicrobial strategies that disrupt the biofilm or augment the response of antibiotics against organisms contained within the extracellular matrix.9 These antibiotic delivery strategies need to be developed within the context of known information because simply increasing the concentration of antibiotics may be illogical in some circumstances. For example, cell-wall active antibiotics, such as vancomycin, would not be expected to be very effective against sessile organisms within biofilm, which exhibit a reduced growth rate, and indeed it has been shown in animal models that vancomycin is relatively ineffective when used as single-agent therapy.10,29 In one of these studies, rifampin alone, among 35 antibiotics studied, penetrated the biofilm.10 Importantly, antibiotics of the cell-wall active class (including vancomycin) were synergistic with rifampin whereas some other antibiotics (including aminoglycosides) antagonized rifampin activity. One of the industrial concepts that has been effective in the treatment of organisms within biofilms is to provide intermittent pulsing of high doses of antibiotics as this facilitates attack of sessile cells that have become planktonic.6 Although this technique can be shown to be effective in controlled industrial models, the difficulties associated with using this approach in the setting of large orthopaedic wounds that are dynamic and have considerable variability make this approach relatively unrealistic at this time. The concept of providing high local loading doses of antibiotic and then providing constant sustained levels of antibiotics seems to be an achievable goal with local antibiotic delivery devices and indeed there are many developmental products that use this as a part of the device design. Antimicrobial Susceptibility Testing Traditional culture and susceptibility testing have been based on retrieval, identification, and assessment of planktonic bacteria. Finally, susceptibility patterns based on traditional culture techniques with relatively low levels of antibiotics may or may not correlate with the susceptibility of planktonic or sessile bacteria confronted with antibiotic concentrations are 10-fold or even 1000-fold higher. Based on current knowledge, this information may not be relevant to the susceptibility of any remaining sessile bacteria contained within biomaterial that is left in the wound. In an in vitro study, all Propionibacterium acnes isolates in biofilm showed considerably greater resistance to cefamandole, ciprofloxacin and vancomycin.30 All staphylococcus species biofilm isolates showed large increases in resistance to gentamicin and cefamandole with a considerable increase in resistance to vancomycin; however there was little difference observed with ciprofloxacin.30 Based on these results, it seems that the susceptibility of antibiotics against organisms growing within biofilms should be determined to ensure optimal treatment. An experimental Staphylococcus aureus rat osteomyelitis model was designed to correlate in vitro testing with in vivo findings using cefuroxime, vancomycin, and tobramycin.12 The implants were studied for presence of bacteria by adenosine triphosphate (ATP) bioluminescence after treatment and compared with an in vitro assay using three concentrations of each antibiotic (8, 100, and 500 μg/mL).12 In the in vivo model, cefuroxime reduced the number of bacteria more effectively than vancomycin (p < 0.05) whereas tobramycin had no effect. The in vitro assay directly correlated with the in vivo data as cefuroxime significantly (p < 0.05) reduced the number of viable bacteria at all concentrations, tobramycin did not affect viability, and vancomycin affected viability except at the lowest concentration studied (8 μg/mL).12 These results show the usefulness of such an osteomyelitis model to provide evidence for correlation between the in vitro and in vivo findings on the effect of antibiotics being studied. Methods of Monitoring Antibiotic Effect on Biofilms It generally was accepted by symposium participants that methods to monitor biofilm activity in real time would be helpful to assess the effect of treatment strategies on cells contained within biofilms in experimental models. There currently are many methods being investigated, which include measurements with a specialized respirometer, use of nuclear magnetic resonance imaging, and bioluminescence imaging.6 Some bacteria produce or can be made to produce bioluminescent signals allowing real-time assessment of the physiologic state of the biofilms.19 This method measures the metabolic activity of viable cells nondestructively, which is appealing for drug-efficacy studies of chronic biofilm infections. In an S. aureus isolate made bioluminescent by inserting a modified lux operon into the bacterial chromosome model, treatment with rifampin, tobramycin, and ciprofloxacin was assessed.20 A rapid dose-dependent decline in metabolic activity in rifampin-treated groups was observed and the disappearance of light emission correlated with colony counts. Because the metabolic activities of viable cells and a post antibiotic effect could be detected directly, this methodology is appealing for investigating the effects of antibiotics on biofilms. Adjunctive Biofilm Therapies The application of low-frequency ultrasound to enhance vancomycin activity against coagulase negative staphylococcal (CNS) and Escherichia coli biofilm has been shown to be effective because pulsed ultrasound significantly reduced bacterial viability below that of nontreated biofilms.4,31 Although the CNS biofilms responded favorably to various combinations of ultrasound and vancomycin, longer treatment times were required for CNS than those observed for E. coli biofilms.4 The possible mechanisms of this phenomenon were explored with the enhanced action of gentamicin against Pseudomonas aeruginosa biofilms.28 The dependence on peak power density suggests that acoustic pressure plays a notable role. There is also a strong frequency component that causes the killing effect to decrease as frequency increases. It is possible that stable cavitation and the accompanying microstreaming contribute to the bioacoustic effect.28 Others have investigated the in vitro effects of pulsed electromagnetic fields (PEMF) on the efficacy of antibiotics in CNS biofilms.26 Exposure to a PEMF increased the effectiveness of gentamicin against 5-day CNS biofilms with a reduction of at least 50% in the minimum biofilm inhibitory concentration (p < 0.05). There was no significant effect with vancomycin. In one arm of the experiment, there was a two-log difference in colony count at 160 mg/L of gentamicin indicating that synergy was more pronounced at higher levels of gentamicin. Other study results indicate that application of static magnetic fields may also enhance the activity of gentamicin against biofilm-forming P. aeruginosa.3 The effect seems to be limited to magnetic fields between 5 and 20 G and the effect was independent of substrate surface. One of the intriguing aspects of these observations regarding use of these adjunctive methods to augment the efficacy of certain antibiotics is that these methods could be used to augment the process of bone restoration simultaneously. Although still controversial, it increasingly is accepted that signals from electromagnetic fields (EMF) and ultrasound (US) have a clinically significant effect on bone repair, and both methods are now a common part of the orthopaedist’s armamentarium.1,27 It has been shown that both have common waveform characteristics at the treatment site and therefore can deliver similar doses of electrical stimulation to the bone.24 Further investigations into this area-to unify the effects of EMF and US on antibiotics and bone healing-seem warranted because it may be possible to modulate the bone reparative process by several different mechanisms during the treatment of musculoskeletal infection. The most obvious would be caused by the direct effects of EMF and US on bone healing and also an indirect effect might be realized by limiting the dosage of locally delivered antibiotics because of increased efficacy with EMF or US, reducing the dose related adverse effects of antibiotics on osteoblasts. Regulatory Issues One of the sources of great frustration for clinicians engaged in the treatment of musculoskeletal infection has been the inevitable delays that have occurred with FDA approval for local antibiotic delivery vehicles.23 Only recently, after decades of use as a clinician-directed application, is there now approval of commercial ABLC.23 Unfortunately, these approvals are for low-dose aminoglycoside ABLC, which is suitable for prophylaxis; current treatment strategies for established infection require higher doses and multiple choices of antibiotics.14 As such, physicians who choose to treat established musculoskeletal infection with high-dose ABLC must continue to do so as a clinician-directed application. The emergence of composite local antibiotic delivery vehicles, which have the potential to deliver different types of antibiotics, provide creative methods of antibiotic dosing, and also provide materials that contribute to osteogenesis will need to be used as clinician-directed application for the foreseeable future. Hopefully with careful development of these composite biomaterials combined with efficacy and safety data will enable FDA approval for wider use in the treatment of musculoskeletal infections. There is still much to do regarding the investigation and development of treatment tools for treatment of musculoskeletal infection. However, the potential opportunities appear optimistic particularly if biofilm researchers, infectious disease specialists, microbiologists, and orthopaedic surgeons continue to interact and to communicate openly so that some of their seemingly disparate activities ultimately can be combined in a synergistic manner.", "authors": ["Arlen D. Hanssen", "Douglas R. Osmon", "Robin Patel"], "year": 2005, "venue": "Clinical Orthopaedics and Related Research", "cited_by_count": 67, "type": "article", "concepts": ["Antibiotics", "Medicine", "Intensive care medicine", "Dosing", "Adverse effect"], "search_query": "all-in-one image restoration unified model degradation"}
{"openalex_id": "https://openalex.org/W2890970077", "doi": "https://doi.org/10.1038/s41396-019-0383-2", "title": "Agricultural intensification reduces microbial network complexity and the abundance of keystone taxa in roots", "abstract": "Root-associated microbes play a key role in plant performance and productivity, making them important players in agroecosystems. So far, very few studies have assessed the impact of different farming systems on the root microbiota and it is still unclear whether agricultural intensification influences the structure and complexity of microbial communities. We investigated the impact of conventional, no-till, and organic farming on wheat root fungal communities using PacBio SMRT sequencing on samples collected from 60 farmlands in Switzerland. Organic farming harbored a much more complex fungal network with significantly higher connectivity than conventional and no-till farming systems. The abundance of keystone taxa was the highest under organic farming where agricultural intensification was the lowest. We also found a strong negative association (R<sup>2</sup> = 0.366; P < 0.0001) between agricultural intensification and root fungal network connectivity. The occurrence of keystone taxa was best explained by soil phosphorus levels, bulk density, pH, and mycorrhizal colonization. The majority of keystone taxa are known to form arbuscular mycorrhizal associations with plants and belong to the orders Glomerales, Paraglomerales, and Diversisporales. Supporting this, the abundance of mycorrhizal fungi in roots and soils was also significantly higher under organic farming. To our knowledge, this is the first study to report mycorrhizal keystone taxa for agroecosystems, and we demonstrate that agricultural intensification reduces network complexity and the abundance of keystone taxa in the root microbiome.", "authors": ["Samiran Banerjee", "Florian Walder", "Lucie Büchi", "Marcel Meyer", "Alain Held", "Andreas Gattinger", "Thomas Keller", "Raphaël Charles", "Marcel G. A. van der Heijden"], "year": 2019, "venue": "The ISME Journal", "cited_by_count": 1236, "type": "article", "concepts": ["Biology", "Agroecosystem", "Keystone species", "Abundance (ecology)", "Organic farming"], "search_query": "all-in-one image restoration unified model degradation"}
{"openalex_id": "https://openalex.org/W2050839858", "doi": "https://doi.org/10.1161/01.cir.100.9.999", "title": "Mechanisms and Models in Heart Failure", "abstract": "The people who bind themselves to systems are those who are unable to encompass the whole truth and try to catch", "authors": ["Douglas L. Mann"], "year": 1999, "venue": "Circulation", "cited_by_count": 773, "type": "review", "concepts": ["Medicine", "Heart failure", "Cardiology", "Intensive care medicine", "Internal medicine"], "search_query": "all-in-one image restoration unified model degradation"}
{"openalex_id": "https://openalex.org/W4417248878", "doi": "https://doi.org/10.1109/tpami.2025.3642852", "title": "Beyond Degradation Redundancy: Contrastive Prompt Learning for All-in-One Image Restoration", "abstract": "All-in-one image restoration, addressing diverse degradation types with a unified model, presents significant challenges in designing task-aware prompts that effectively guide restoration across multiple degradation scenarios. While adaptive prompt learning enables end-to-end optimization, it often yields overlapping or redundant task representations. Conversely, explicit prompts derived from pretrained classifiers enhance discriminability but may discard critical visual information for reconstruction. To address these limitations, we introduce Contrastive Prompt Learning (CPL), a novel framework that fundamentally enhances prompt-task alignment through two complementary innovations: a Sparse Prompt Module (SPM) that efficiently captures degradation-specific features while minimizing redundancy, and a Contrastive Prompt Regularization (CPR) that explicitly strengthens task boundaries by incorporating negative prompt samples across different degradation types. Unlike previous approaches that focus primarily on degradation classification, CPL optimizes the critical interaction between prompts and the restoration model itself. Extensive experiments across five comprehensive benchmarks demonstrate that CPL consistently enhances state-of-the-art all-in-one restoration models, achieving significant improvements in both standard multi-task scenarios and challenging composite degradation settings. Our framework establishes new state-of-the-art performance while maintaining parameter efficiency, offering a principled solution for unified image restoration.", "authors": ["Gang Wu", "Junjun Jiang", "Kui Jiang", "Xianming Liu", "Liqiang Nie"], "year": 2025, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "cited_by_count": 1, "type": "article", "concepts": [], "search_query": "all-in-one image restoration unified model degradation"}
{"openalex_id": "https://openalex.org/W2397311736", "doi": "https://doi.org/10.5194/hess-21-589-2017", "title": "MSWEP: 3-hourly 0.25° global gridded precipitation (1979–2015) by merging gauge, satellite, and reanalysis data", "abstract": "Abstract. Current global precipitation (P) datasets do not take full advantage of the complementary nature of satellite and reanalysis data. Here, we present Multi-Source Weighted-Ensemble Precipitation (MSWEP) version 1.1, a global P dataset for the period 1979–2015 with a 3-hourly temporal and 0.25° spatial resolution, specifically designed for hydrological modeling. The design philosophy of MSWEP was to optimally merge the highest quality P data sources available as a function of timescale and location. The long-term mean of MSWEP was based on the CHPclim dataset but replaced with more accurate regional datasets where available. A correction for gauge under-catch and orographic effects was introduced by inferring catchment-average P from streamflow (Q) observations at 13 762 stations across the globe. The temporal variability of MSWEP was determined by weighted averaging of P anomalies from seven datasets; two based solely on interpolation of gauge observations (CPC Unified and GPCC), three on satellite remote sensing (CMORPH, GSMaP-MVK, and TMPA 3B42RT), and two on atmospheric model reanalysis (ERA-Interim and JRA-55). For each grid cell, the weight assigned to the gauge-based estimates was calculated from the gauge network density, while the weights assigned to the satellite- and reanalysis-based estimates were calculated from their comparative performance at the surrounding gauges. The quality of MSWEP was compared against four state-of-the-art gauge-adjusted P datasets (WFDEI-CRU, GPCP-1DD, TMPA 3B42, and CPC Unified) using independent P data from 125 FLUXNET tower stations around the globe. MSWEP obtained the highest daily correlation coefficient (R) among the five P datasets for 60.0 % of the stations and a median R of 0.67 vs. 0.44–0.59 for the other datasets. We further evaluated the performance of MSWEP using hydrological modeling for 9011 catchments (&lt; 50 000 km2) across the globe. Specifically, we calibrated the simple conceptual hydrological model HBV (Hydrologiska Byråns Vattenbalansavdelning) against daily Q observations with P from each of the different datasets. For the 1058 sparsely gauged catchments, representative of 83.9 % of the global land surface (excluding Antarctica), MSWEP obtained a median calibration NSE of 0.52 vs. 0.29–0.39 for the other P datasets. MSWEP is available via http://www.gloh2o.org.", "authors": ["Hylke E. Beck", "Albert I. J. M. van Dijk", "Vincenzo Levizzani", "Jaap Schellekens", "Diego G. Miralles", "Brecht Martens", "Ad de Roo"], "year": 2017, "venue": "Hydrology and earth system sciences", "cited_by_count": 1060, "type": "article", "concepts": ["Orography", "Environmental science", "Satellite", "Meteorology", "Rain gauge"], "search_query": "all-in-one image restoration unified model degradation"}
{"openalex_id": "https://openalex.org/W4409262098", "doi": "https://doi.org/10.1109/wacv61041.2025.00069", "title": "All-in-One Image Compression and Restoration", "abstract": "Visual images corrupted by various types and levels of degradations are commonly encountered in practical image compression. However, most existing image compression methods are tailored for clean images, therefore struggling to achieve satisfying results on these images. Joint compression and restoration methods typically focus on a single type of degradation and fail to address a variety of degradations in practice. To this end, we propose a unified framework for all-in-one image compression and restoration, which incorporates the image restoration capability against various degradations into the process of image compression. The key challenges involve distinguishing authentic image content from degradations, and flexibly eliminating various degradations without prior knowledge. Specifically, the proposed framework approaches these challenges from two perspectives: i.e., content information aggregation, and degradation representation aggregation. Extensive experiments demonstrate the following merits of our model: 1) superior rate-distortion (RD) performance on various degraded inputs while preserving the performance on clean data; 2) strong generalization ability to real-world and unseen scenarios; 3) higher computing efficiency over compared methods. Our code is available at https://github.com/ZeldaM1/All-in-one.", "authors": ["H. Zeng", "Jiacheng Li", "Ziqiang Zheng", "Zhiwei Xiong"], "year": 2025, "venue": "", "cited_by_count": 1, "type": "article", "concepts": ["Image restoration", "Image compression", "Computer science", "Computer vision", "Compression (physics)"], "search_query": "all-in-one image restoration unified model degradation"}
{"openalex_id": "https://openalex.org/W3089028909", "doi": "https://doi.org/10.1109/jproc.2021.3052449", "title": "A Unifying Review of Deep and Shallow Anomaly Detection", "abstract": "Deep learning approaches to anomaly detection (AD) have recently improved the state of the art in detection performance on complex data sets, such as large collections of images or text. These results have sparked a renewed interest in the AD problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review, we aim to identify the common underlying principles and the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic “shallow” and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that are enriched by the use of recent explainability techniques and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in AD.", "authors": ["Lukas Ruff", "Jacob R. Kauffmann", "Robert A. Vandermeulen", "Gregoire Montavon", "Wojciech Samek", "Marius Kloft", "Thomas G. Dietterich", "Klaus-Robert Muller"], "year": 2021, "venue": "Proceedings of the IEEE", "cited_by_count": 755, "type": "article", "concepts": [], "search_query": "all-in-one image restoration unified model degradation"}
{"openalex_id": "https://openalex.org/W1996113298", "doi": "https://doi.org/10.1002/ijc.27316", "title": "Cellular senescence and tumor suppressor gene p16", "abstract": "Abstract Cellular senescence is an irreversible arrest of cell growth. Biochemical and morphological changes occur during cellular senescence, including the formation of a unique cellular morphology such as flattened cytoplasm. Function of mitochondria, endoplasmic reticulum and lysosomes are affected resulting in the inhibition of lysosomal and proteosomal pathways. Cellular senescence can be triggered by a number of factors including, aging, DNA damage, oncogene activation and oxidative stress. While the molecular mechanism of senescence involves p16 and p53 tumor suppressor genes and telomere shortening, this review is focused on the mechanism of p16 control. The p16‐mediated senescence acts through the retinoblastoma (Rb) pathway inhibiting the action of the cyclin dependant kinases leading to G1 cell cycle arrest. Rb is maintained in a hypophosphorylated state resulting in the inhibition of transcription factor E2F1. Regulation of p16 expression is complex and involves epigenetic control and multiple transcription factors. PRC1 (Pombe repressor complex (1) and PRC2 (Pombe repressor complex (2) proteins and histone deacetylases play an important role in the promoter hypermethylation for suppressing p16 expression. While transcription factors YY1 and Id1 suppress p16 expression, transcription factors CTCF, Sp1 and Ets family members activate p16 transcription. Senescence occurs with the inactivation of suppressor elements leading to the enhanced expression of p16.", "authors": ["Hani Rayess", "Marilene B. Wang", "Eri S. Srivatsan"], "year": 2011, "venue": "International Journal of Cancer", "cited_by_count": 793, "type": "review", "concepts": ["Cell biology", "Biology", "Senescence", "Retinoblastoma protein", "E2F1"], "search_query": "all-in-one image restoration unified model degradation"}
{"openalex_id": "https://openalex.org/W4405974083", "doi": "https://doi.org/10.1109/tcsi.2024.3519532", "title": "A Unified Accelerator for All-in-One Image Restoration Based on Prompt Degradation Learning", "abstract": "All-in-one image restoration (IR) recovers images from various unknown distortions by a single model, such as rain, haze, and blur. Transformer-based IR methods have significantly improved the visual effects of the restored images. However, deploying complex IR models on edge devices is challenging due to massive parameters and intensive computations. Moreover, existing accelerators are typically customized for a single task, resulting in severe resource underutilization when executing multiple tasks. Therefore, this paper develops an algorithm-hardware co-design framework to accelerate a novel CNN-Transformer cooperative model for multiple IR tasks. Firstly, on the algorithm level, an Efficient Restoration Foundational Model (ERFM) is proposed to recover corrupted images from various degradations with low model complexity. Secondly, to guide adaptive corruption removal, a novel prompt learning scheme is introduced to fuse context-related degradation cues and boost high-quality reconstruction. Thirdly, on the hardware level, an integer approximation method is proposed to avoid expensive hardware overhead caused by complex nonlinear operations, such as layer normalization and softmax while maintaining comparable IR quality. Moreover, a head stationary dataflow and softmax fusion mechanism are designed to reduce data movement and enhance on-chip resource utilization. Finally, an overall hardware architecture is developed and implemented in TSMC 28 nm CMOS technology. Experimental results show that our ERFM achieves better visual perception than other baselines on seven challenging IR tasks without task-specific fine-tuning. Moreover, compared to other accelerators for vision Transformers, our design can achieve 3.3 <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\times$</tex-math> </inline-formula> and 3.7 <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\times$</tex-math> </inline-formula> improvements in throughput and energy efficiency.", "authors": ["Siyu Zhang", "Qiwei Dong", "Wendong Mao", "Zhongfeng Wang"], "year": 2025, "venue": "IEEE Transactions on Circuits and Systems I Regular Papers", "cited_by_count": 3, "type": "article", "concepts": ["Degradation (telecommunications)", "Image restoration", "Computer science", "Image (mathematics)", "Artificial intelligence"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4381798022", "doi": "https://doi.org/10.48550/arxiv.2306.13090", "title": "PromptIR: Prompting for All-in-One Blind Image Restoration", "abstract": "Image restoration involves recovering a high-quality clean image from its degraded version. Deep learning-based methods have significantly improved image restoration performance, however, they have limited generalization ability to different degradation types and levels. This restricts their real-world application since it requires training individual models for each specific degradation and knowing the input degradation type to apply the relevant model. We present a prompt-based learning approach, PromptIR, for All-In-One image restoration that can effectively restore images from various types and levels of degradation. In particular, our method uses prompts to encode degradation-specific information, which is then used to dynamically guide the restoration network. This allows our method to generalize to different degradation types and levels, while still achieving state-of-the-art results on image denoising, deraining, and dehazing. Overall, PromptIR offers a generic and efficient plugin module with few lightweight prompts that can be used to restore images of various types and levels of degradation with no prior information on the corruptions present in the image. Our code and pretrained models are available here: https://github.com/va1shn9v/PromptIR", "authors": ["Vaishnav Potlapalli", "Syed Waqas Zamir", "Salman Khan", "Fahad Shahbaz Khan"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 34, "type": "preprint", "concepts": ["Image restoration", "Computer science", "Degradation (telecommunications)", "Image (mathematics)", "Generalization"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4405865448", "doi": "https://doi.org/10.1016/j.patcog.2024.111312", "title": "Visual style prompt learning using diffusion models for blind face restoration", "abstract": "", "authors": ["Wanglong Lu", "J P Wang", "Tao Wang", "Kaihao Zhang", "Xianta Jiang", "Hanli Zhao"], "year": 2024, "venue": "Pattern Recognition", "cited_by_count": 39, "type": "article", "concepts": ["Face (sociological concept)", "Style (visual arts)", "Computer science", "Artificial intelligence", "Computer vision"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4396782894", "doi": "https://doi.org/10.1109/tcsvt.2024.3398810", "title": "Prompt-Based Ingredient-Oriented All-in-One Image Restoration", "abstract": "Image restoration aims to recover the high-quality images from their degraded observations. Since most existing methods have been dedicated into single degradation removal, they may not yield optimal results on other types of degradations, which do not satisfy the applications in real world scenarios. In this paper, we propose a novel data ingredient-oriented approach that leverages prompt-based learning to enable a single model to efficiently tackle multiple image degradation tasks. Specifically, we utilize a encoder to capture features and introduce prompts with degradation-specific information to guide the decoder in adaptively recovering images affected by various degradations. In order to model the local invariant properties and non-local information for high-quality image restoration, we combine CNNs operations and Transformers. Simultaneously, we make several key designs in the Transformer blocks (multi-head rearranged attention with prompts and simple-gate feed-forward network) to reduce computational requirements and selectively determines what information should be persevered to facilitate efficient recovery of potentially sharp images. Furthermore, we incorporate a feature fusion mechanism further explores the multi-scale information to improve the aggregated features. The resulting tightly interlinked hierarchy architecture, named as CAPTNet, extensive experiments demonstrate that our method performs competitively to the state-of-the-art. The code and the pre-trained models are released at https://github.com/Tombs98/CAPTNet.", "authors": ["Hu Gao", "Jing Yang", "Ying Zhang", "Ning Wang", "Jingfan Yang", "Depeng Dang"], "year": 2024, "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "cited_by_count": 26, "type": "article", "concepts": ["Ingredient", "Image restoration", "Computer science", "Image processing", "Computer vision"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4404844036", "doi": "https://doi.org/10.1016/j.patcog.2024.111223", "title": "FrePrompter: Frequency self-prompt for all-in-one image restoration", "abstract": "", "authors": ["Zhijian Wu", "Wenhui Liu", "Jingchao Wang", "Jun Li", "Dingjiang Huang"], "year": 2024, "venue": "Pattern Recognition", "cited_by_count": 14, "type": "article", "concepts": ["Image restoration", "Image (mathematics)", "Artificial intelligence", "Computer vision", "Computer science"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4402915930", "doi": "https://doi.org/10.1109/cvprw63382.2024.00645", "title": "PromptCIR: Blind Compressed Image Restoration with Prompt Learning", "abstract": "Blind Compressed Image Restoration (CIR) has garnered significant attention due to its practical applications. It aims to mitigate compression artifacts caused by unknown quality factors, particularly with JPEG codecs. Existing works on blind CIR often seek assistance from a quality factor prediction network to facilitate their network to restore compressed images. However, the predicted numerical quality factor lacks spatial information, preventing network adaptability toward image contents. Recent studies in prompt-learning-based image restoration have showcased the potential of prompts to generalize across varied degradation types and degrees. This motivated us to design a prompt-learning-based compressed image restoration network, dubbed PromptCIR, which can effectively restore images from various compress levels. Specifically, PromptCIR exploits prompts to encode compression information implicitly, where prompts directly interact with soft weights generated from image features, thus providing dynamic content-aware and distortion-aware guidance for the restoration process. The light-weight prompts enable our method to adapt to different compression levels, while introducing minimal parameter overhead. Overall, Prompt-CIR leverages the powerful transformer-based backbone with the dynamic prompt module to proficiently handle blind CIR tasks, winning first place in the NTIRE 2024 challenge of blind compressed image enhancement track. Extensive experiments have validated the effectiveness of our proposed PromptCIR. The code is available at https://github.com/lbc12345/PromptCIR-NTIRE24.", "authors": ["Bingchen Li", "Xin Li", "Yiting Lu", "Ruoyu Feng", "Mengxi Guo", "Shijie Zhao", "Zhang Li", "Zhibo Chen"], "year": 2024, "venue": "", "cited_by_count": 17, "type": "article", "concepts": ["Computer science", "Image restoration", "Artificial intelligence", "Computer vision", "Image (mathematics)"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4379141198", "doi": "https://doi.org/10.1016/j.eswa.2023.120646", "title": "Exposing low-quality deepfake videos of Social Network Service using Spatial Restored Detection Framework", "abstract": "", "authors": ["Ying Li", "Shan Bian", "Chuntao Wang", "Kemal Polat", "Adi Alhudhaif", "Fayadh Alenezi"], "year": 2023, "venue": "Expert Systems with Applications", "cited_by_count": 26, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Block (permutation group theory)", "Feature (linguistics)", "Quality (philosophy)"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4406092414", "doi": "https://doi.org/10.1016/j.inffus.2025.102930", "title": "Ada4DIR: An adaptive model-driven all-in-one image restoration network for remote sensing images", "abstract": "", "authors": ["Ziyang Lihe", "Qiangqiang Yuan", "Jiang He", "Xianyu Jin", "Yi Xiao", "Yuzeng Chen", "Huanfeng Shen", "Liangpei Zhang"], "year": 2025, "venue": "Information Fusion", "cited_by_count": 5, "type": "article", "concepts": ["Computer science", "Image (mathematics)", "Artificial intelligence", "Image restoration", "Computer vision"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4387486933", "doi": "https://doi.org/10.1109/icamechs59878.2023.10272811", "title": "An Attempt at Zero-shot Ancient Documents Restoration Based on Diffusion Models", "abstract": "The virtual restoration of ancient documents using deep learning is an emergency and an expected work. However, GANs-based image-to-image translation approaches hit the degradation data shortage, a hardness to build one-to-many restoration models, and a limitation for large deformation. In this study, we apply zero-shot restoration based on Diffusion models to ancient degraded documents, specifically, leverage inpainting of Denoing Diffusion Restoration Models (DDRM) for missing ancient characters. Furthermore, we introduce a noise masking method, which limits the attention area of predicted noise images in the reverse process. Noise masking forces DDRM to generate faithful objects following mask images, so that has high usability without re-training of deep neural networks. The zero-shot restoration and noise masking prompt GUI-connecting restoration of missing characters, leading to realizing a cooperative application with humans for ancient document restoration.", "authors": ["Hayata Kaneko", "Yuuya Yoshizu", "Ryuto Ishibashi", "Lin Meng"], "year": 2023, "venue": "", "cited_by_count": 5, "type": "article", "concepts": ["Computer science", "Image restoration", "Masking (illustration)", "Artificial intelligence", "Noise (video)"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4404176632", "doi": "https://doi.org/10.1007/978-3-031-72855-6_11", "title": "UniProcessor: A Text-Induced Unified Low-Level Image Processor", "abstract": "", "authors": ["Huiyu Duan", "Xiongkuo Min", "Sijing Wu", "Wei Shen", "Guangtao Zhai"], "year": 2024, "venue": "Lecture notes in computer science", "cited_by_count": 6, "type": "book-chapter", "concepts": ["Uniprocessor system", "Computer science", "Image (mathematics)", "Artificial intelligence", "Parallel computing"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W2971398159", "doi": "https://doi.org/10.1111/rec.13035", "title": "International principles and standards for the practice of ecological restoration. Second edition", "abstract": "The Society for Ecological Restoration (SER) is an international non-profit organization with members in 70 countries. SER advances the science, practice and policy of ecological restoration to sustain biodiversity, improve resilience in a changing climate, and re-establish an ecologically healthy relationship between nature and culture. SER is a dynamic global network, linking researchers, practitioners, land managers, community leaders and decision-makers to restore ecosystems and the human communities that depend on them. Via its members, publications, conferences, policy work, and outreach, SER defines and delivers excellence in the field of ecological restoration.", "authors": ["George D. Gann", "Tein McDonald", "Bethanie Walder", "James Aronson", "Cara R. Nelson", "Justin Jonson", "James G. Hallett", "Cristina Eisenberg", "Manuel R. Guariguata", "Junguo Liu", "Fangyuan Hua", "Cristián Echeverría", "Emily K. Gonzales", "Nancy L. Shaw", "Kris Decleer", "Kingsley W. Dixon"], "year": 2019, "venue": "Restoration Ecology", "cited_by_count": 1302, "type": "article", "concepts": ["Restoration ecology", "Ecology", "Geography", "Environmental resource management", "Environmental ethics"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4411047697", "doi": "https://doi.org/10.1016/j.patcog.2025.111875", "title": "AdaPrompt-IR: Adaptive learning to perceive degradation semantic and prompting for all-in-one image restoration", "abstract": "", "authors": ["Wei Sun", "Qianzhou Wang", "Yaqi Wang", "Zhiqiang Hou", "Qingsen Yan", "Shuicheng Yan"], "year": 2025, "venue": "Pattern Recognition", "cited_by_count": 3, "type": "article", "concepts": ["Degradation (telecommunications)", "Computer science", "Artificial intelligence", "Image (mathematics)", "Image restoration"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4401444389", "doi": "https://doi.org/10.1007/s11760-024-03493-7", "title": "Rethinking all-in-one adverse weather removal for object detection", "abstract": "", "authors": ["Yufeng Li", "Jiayu Chen", "Chuanlong Xie", "Hongming Chen"], "year": 2024, "venue": "Signal Image and Video Processing", "cited_by_count": 4, "type": "article", "concepts": ["Adverse weather", "Object (grammar)", "Computer science", "Artificial intelligence", "Environmental science"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4407677384", "doi": "https://doi.org/10.1016/j.engappai.2025.110267", "title": "Adaptive prompt guided unified image restoration with latent diffusion model", "abstract": "", "authors": ["Xiang Lv", "Mingwen Shao", "Yecong Wan", "Yuanjian Qiao", "Changzhong Wang"], "year": 2025, "venue": "Engineering Applications of Artificial Intelligence", "cited_by_count": 4, "type": "article", "concepts": ["Computer science", "Diffusion", "Image (mathematics)", "Image restoration", "Artificial intelligence"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4402915827", "doi": "https://doi.org/10.1109/tcsvt.2024.3469190", "title": "Multi-Weather Restoration: An Efficient Prompt-Guided Convolution Architecture", "abstract": "Addressing degraded weather conditions plays a vital role in practical applications. Many existing restoration approaches are limited to specific weather types, which limits their applicability to different weather scenarios. Advanced technologies, encompassing Transformer and diffusion model, have been harnessed to confront this challenge. However, these methods often heighten network complexity and prolong inference duration. To this end, we present MW-ConvNet, a U-shaped convolution-based network for multi-weather restoration. Specifically, the MW-Enc block and MW-Dec block are introduced to achieve simple yet strong feature extraction, which rely entirely on traditional 2D convolution. To improve adaptability to multiple weather conditions, a prompt generation module is designed to generate a representative weather prompt at the encoder’s terminus. Drawing inspiration from style transfer, the weather prompt is used to guide the decoder learning through a progressive restoration procedure. For future high-fidelity restoration, we introduce frequency separation through wavelet pooling blocks in encoder phase and corresponding up-sampling blocks in decoder phase. The segregated treatment of low-frequency and high-frequency features curbs the loss of textural information during network computation. It also future improves the quality and accuracy of generated weather prompt. Extensive experiments demonstrate that the proposed MW-ConvNet obtains superior performance compared to state-of-the-art methods across both weather-specific and real-world restoration tasks. Significantly, our method achieves an impressive inference speed of 0.12 seconds per <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$256\\times 256$ </tex-math></inline-formula> image, outpacing transformer-based and diffusion-based models.", "authors": ["Chengyang Li", "Fangwei Sun", "Heng Zhou", "Yongqiang Xie", "Zhongbo Li", "Li Zhu"], "year": 2024, "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "cited_by_count": 6, "type": "article", "concepts": ["Architecture", "Convolution (computer science)", "Computer science", "Artificial intelligence", "Geography"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4311263589", "doi": "https://doi.org/10.5281/zenodo.3553579", "title": "Summary for policymakers of the global assessment report on biodiversity and ecosystem services", "abstract": "IPBES is an independent intergovernmental body comprising over 130 member Governments. Established by Governments in 2012, IPBES provides policymakers with objective scientific assessments about the state of knowledge regarding the planet’s biodiversity, ecosystems and the contributions they make to people, as well as options and actions to protect and sustainably use these vital natural assets. The IPBES Global Assessment of Biodiversity and Ecosystem Services represents the landmark product of the first work programme of IPBES (2014-2018). The Global Assessment was initiated following a decision from the IPBES Plenary at its fourth session (IPBES 4, Kuala Lumpur, 2016), and considered by the IPBES Plenary at its seventh session (IPBES 7, Paris, 2019). It is composed of a summary for policymakers, which was approved at IPBES 7, and six chapters, which were accepted at IPBES 7.", "authors": ["IPBES"], "year": 2019, "venue": "Zenodo (CERN European Organization for Nuclear Research)", "cited_by_count": 1078, "type": "report", "concepts": ["Ecosystem services", "Biodiversity", "Ecosystem", "Environmental resource management", "Business"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4404635286", "doi": "https://doi.org/10.1016/j.neucom.2024.128955", "title": "Multi-modal degradation feature learning for unified image restoration based on contrastive learning", "abstract": "In this paper, we address the unified image restoration challenge by reframing it as a contrastive learning-based classification problem. Despite the significant strides made by deep learning methods in enhancing image restoration quality, their limited capacity to generalize across diverse degradation types and intensities necessitates the training of separate models for each specific degradation scenario. We proposes an all-encompassing approach that can restore images from various unknown corruption types and levels. We devise a method that learns representations of the latent sharp image’s degradation and accompanying textual features (such as dataset categories and image content descriptions), converting these into prompts which are then embedded within a reconstruction network model to enhance cross-database restoration performance. This culminates in a unified image reconstruction framework. The study involves two stages: In the first stage, we design a MultiContentNet that learns multi-modal features (MMFs) of the latent sharp image. This network encodes the visual degradation expressions and contextual text features into latent variables, thereby exerting a guided classification effect. Specifically, MultiContentNet is trained as an auxiliary controller capable of taking the degraded input image and, through contrastive learning , extracts MMFs of the latent target image. This effectively generates natural classifiers tailored for different degradation types. The second phase integrates the learned MMFs into an image restoration network via cross-attention mechanisms. This guides the restoration model to learn high-fidelity image recovery. Experiments conducted on six blind image restoration tasks demonstrate that the proposed method achieves state-of-the-art performance, highlighting the potential significance of large-scale pretrained vision-language models’ MMFs in advancing high-quality unified image reconstruction.", "authors": ["Lei Chen", "Qibing Xiong", "Wei Zhang", "Xiaoli Liang", "Zhihua Gan", "Lianqing Li", "Xin He"], "year": 2024, "venue": "Neurocomputing", "cited_by_count": 3, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Feature (linguistics)", "Modal", "Degradation (telecommunications)"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4415124039", "doi": "https://doi.org/10.1109/tits.2025.3617507", "title": "Frequency-Prompted Image Restoration to Enhance Perception in Intelligent Transportation Systems", "abstract": "High perceptual image quality is crucial for intelligent transportation systems (ITS), including autonomous vehicles, digital twins, and surveillance infrastructure. However, images captured in adverse weather conditions or dynamic environments often suffer from various visibility degradations. To address this issue, image restoration aims to recover missing details and remove distortions from degraded observations, thereby enhancing the usability of visual data in intelligent transportation applications. Inspired by the success of prompt learning in natural language processing, recent studies have explored prompt-based approaches for various image restoration tasks. However, most of these methods operate in the spatial domain. Given the importance of frequency learning in image restoration, particularly in reducing the spectral discrepancy between degraded and sharp image pairs, this study investigates the use of frequency prompts through a plug-and-play mechanism consisting of a prompt generation module and a prompt integration module. Specifically, the prompt generation module encodes frequency information by aggregating pre-defined learnable parameters, guided by the implicitly decomposed spectra of the input features. The learned prompts are then integrated into the feature spectra via dual-dimensional attention, dynamically guiding the reconstruction process and enabling more effective frequency-aware learning. To validate the effectiveness of the proposed plug-in module, we integrate it into both CNN-based and Transformer-based backbones. Extensive experiments demonstrate that the CNN-based variant achieves state-of-the-art performance on 15 datasets across five representative image restoration tasks. Furthermore, it generalizes well to composite degradation scenarios. The Transformer-based model performs competitively with state-of-the-art methods under two all-in-one image restoration settings. Finally, the effectiveness of our models in enhancing perception for ITS is empirically verified.", "authors": ["Yuning Cui", "Mingyu Liu", "Xiongfei Su", "Alois Knoll"], "year": 2025, "venue": "IEEE Transactions on Intelligent Transportation Systems", "cited_by_count": 2, "type": "article", "concepts": [], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4394617226", "doi": "https://doi.org/10.3389/fmars.2024.1382147", "title": "Learning degradation-aware visual prompt for maritime image restoration under adverse weather conditions", "abstract": "Adverse weather conditions such as rain and haze often lead to a degradation in the quality of maritime images, which is crucial for activities like navigation, fishing, and search and rescue. Therefore, it is of great interest to develop an effective algorithm to recover high-quality maritime images under adverse weather conditions. This paper proposes a prompt-based learning method with degradation perception for maritime image restoration, which contains two key components: a restoration module and a prompting module. The former is employed for image restoration, whereas the latter encodes weather-related degradation-specific information to modulate the restoration module, enhancing the recovery process for improved results. Inspired by the recent trend of prompt learning in artificial intelligence, this paper adopts soft-prompt technology to generate learnable visual prompt parameters for better perceiving the degradation-conditioned cues. Extensive experimental results on several benchmarks show that our approach achieves superior restoration performance in maritime image dehazing and deraining tasks.", "authors": ["Xin He", "Tong Jia", "Junjie Li"], "year": 2024, "venue": "Frontiers in Marine Science", "cited_by_count": 1, "type": "article", "concepts": ["Adverse weather", "Degradation (telecommunications)", "Environmental science", "Computer science", "Meteorology"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4399740511", "doi": "https://doi.org/10.3390/electronics13122346", "title": "Underwater Fish Object Detection with Degraded Prior Knowledge", "abstract": "Understanding fish distribution, behavior, and abundance is crucial for marine ecological research, fishery management, and environmental monitoring. However, the distinctive features of the underwater environment, including low visibility, light attenuation, water turbidity, and strong currents, significantly impact the quality of data gathered by underwater imaging systems, posing considerable challenges in accurately detecting fish objects. To address this challenge, our study proposes an innovative fish detection network based on prior knowledge of image degradation. In our research process, we first delved into the intrinsic relationship between visual image quality restoration and detection outcomes, elucidating the obstacles the underwater environment poses to object detection. Subsequently, we constructed a dataset optimized for object detection using image quality evaluation metrics. Building upon this foundation, we designed a fish object detection network that integrates a prompt-based degradation feature learning module and a two-stage training scheme, effectively incorporating prior knowledge of image degradation. To validate the efficacy of our approach, we develop a multi-scene Underwater Fish image Dataset (UFD2022). The experimental results demonstrate significant improvements of 2.4% and 2.5%, respectively, in the mAP index compared to the baseline methods ResNet50 and ResNetXT101. This outcome robustly confirms the effectiveness and superiority of our process in addressing the challenge of fish object detection in underwater environments.", "authors": ["Shijian Zheng", "Rujing Wang", "Liusan Wang"], "year": 2024, "venue": "Electronics", "cited_by_count": 5, "type": "article", "concepts": ["Underwater", "Fish <Actinopterygii>", "Object (grammar)", "Computer science", "Object detection"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4404338208", "doi": "https://doi.org/10.48550/arxiv.2410.18666", "title": "DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation", "abstract": "Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets. To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (DiT)-based image restoration model. GenIR, our pioneering contribution, is a dual-prompt learning pipeline that overcomes the limitations of existing datasets, which typically comprise only a few thousand images and thus offer limited generalizability for larger models. GenIR streamlines the process into three stages: image-text pair construction, dual-prompt based fine-tuning, and data generation &amp; filtering. This approach circumvents the laborious data crawling process, ensuring copyright compliance and providing a cost-effective, privacy-safe solution for IR dataset construction. The result is a large-scale dataset of one million high-quality images. Our second contribution, DreamClear, is a DiT-based image restoration model. It utilizes the generative priors of text-to-image (T2I) diffusion models and the robust perceptual capabilities of multi-modal large language models (MLLMs) to achieve photorealistic restoration. To boost the model's adaptability to diverse real-world degradations, we introduce the Mixture of Adaptive Modulator (MoAM). It employs token-wise degradation priors to dynamically integrate various restoration experts, thereby expanding the range of degradations the model can address. Our exhaustive experiments confirm DreamClear's superior performance, underlining the efficacy of our dual strategy for real-world image restoration. Code and pre-trained models are available at: https://github.com/shallowdream204/DreamClear.", "authors": ["Yuang Ai", "Xiaoqiang Zhou", "Huaibo Huang", "Xiaotian Han", "Zhengyu Chen", "Quanzeng You", "Hongxia Yang"], "year": 2024, "venue": "arXiv (Cornell University)", "cited_by_count": 2, "type": "preprint", "concepts": ["Computer science", "Image (mathematics)", "Privacy protection", "Internet privacy", "Data science"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W3171835979", "doi": "https://doi.org/10.1038/s41573-021-00219-z", "title": "Noncoding RNA therapeutics — challenges and potential solutions", "abstract": "", "authors": ["Melanie Winkle", "Sherien M. El‐Daly", "Muller Fabbri", "George A. Călin"], "year": 2021, "venue": "Nature Reviews Drug Discovery", "cited_by_count": 1504, "type": "review", "concepts": ["microRNA", "Non-coding RNA", "RNA", "Long non-coding RNA", "Computational biology"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4387583542", "doi": "https://doi.org/10.1109/etfa54631.2023.10275673", "title": "Non-Contact Heart Rate Measurement from Deteriorated Videos", "abstract": "Remote photoplethysmography (rPPG) offers a state-of-the-art, non-contact methodology for estimating human pulse by analyzing facial videos. Despite its potential, rPPG methods can be susceptible to various artifacts, such as noise, occlusions, and other obstructions caused by sunglasses, masks, or even involuntary face touching. In this study, we apply image processing transformations to intentionally degrade video quality, mimicking these challenging conditions, and subsequently evaluate the performance of both non-learning and learning-based rPPG methods on the deteriorated data. Our results reveal a significant decrease in accuracy in the presence of these artifacts, prompting us to propose the application of restoration techniques, such as denoising and inpainting, to improve heart-rate estimation outcomes. By addressing these challenging conditions and occlusion artifacts, our approach aims to make rPPG methods more robust and adaptable to real-world situations. To assess the effectiveness of our proposed methods, we undertake comprehensive experiments on three publicly available datasets, encompassing a wide range of scenarios and artifact types. Our findings underscore the potential to construct a robust rPPG system by employing an optimal combination of restoration algorithms and rPPG techniques. Moreover, our study contributes to the advancement of privacy-conscious rPPG methodologies, thereby bolstering the overall utility and impact of this innovative technology in the field of remote heart-rate estimation under realistic and diverse conditions.", "authors": ["Nhi Nguyen", "Le Ngu Nguyen", "Constantino Álvarez Casado", "Olli Sílven", "Miguel Bordallo López"], "year": 2023, "venue": "", "cited_by_count": 4, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Artifact (error)", "Inpainting", "Computer vision"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4392783067", "doi": "https://doi.org/10.1038/s41392-024-01743-1", "title": "Microbiota–gut–brain axis and its therapeutic applications in neurodegenerative diseases", "abstract": "The human gastrointestinal tract is populated with a diverse microbial community. The vast genetic and metabolic potential of the gut microbiome underpins its ubiquity in nearly every aspect of human biology, including health maintenance, development, aging, and disease. The advent of new sequencing technologies and culture-independent methods has allowed researchers to move beyond correlative studies toward mechanistic explorations to shed light on microbiome-host interactions. Evidence has unveiled the bidirectional communication between the gut microbiome and the central nervous system, referred to as the \"microbiota-gut-brain axis\". The microbiota-gut-brain axis represents an important regulator of glial functions, making it an actionable target to ameliorate the development and progression of neurodegenerative diseases. In this review, we discuss the mechanisms of the microbiota-gut-brain axis in neurodegenerative diseases. As the gut microbiome provides essential cues to microglia, astrocytes, and oligodendrocytes, we examine the communications between gut microbiota and these glial cells during healthy states and neurodegenerative diseases. Subsequently, we discuss the mechanisms of the microbiota-gut-brain axis in neurodegenerative diseases using a metabolite-centric approach, while also examining the role of gut microbiota-related neurotransmitters and gut hormones. Next, we examine the potential of targeting the intestinal barrier, blood-brain barrier, meninges, and peripheral immune system to counteract glial dysfunction in neurodegeneration. Finally, we conclude by assessing the pre-clinical and clinical evidence of probiotics, prebiotics, and fecal microbiota transplantation in neurodegenerative diseases. A thorough comprehension of the microbiota-gut-brain axis will foster the development of effective therapeutic interventions for the management of neurodegenerative diseases.", "authors": ["Jian Sheng Loh", "Wen Qi Mak", "Li Tan", "Chu Xin Ng", "Hong Hao Chan", "Shiau Hueh Yeow", "Jhi Biau Foo", "Yong Sze Ong", "Chee Wun How", "Kooi Yeong Khaw"], "year": 2024, "venue": "Signal Transduction and Targeted Therapy", "cited_by_count": 730, "type": "review", "concepts": ["Gut–brain axis", "Microbiome", "Gut flora", "Neurodegeneration", "Biology"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4403556433", "doi": "https://doi.org/10.48550/arxiv.2409.03455", "title": "Data-free Distillation with Degradation-prompt Diffusion for Multi-weather Image Restoration", "abstract": "Multi-weather image restoration has witnessed incredible progress, while the increasing model capacity and expensive data acquisition impair its applications in memory-limited devices. Data-free distillation provides an alternative for allowing to learn a lightweight student model from a pre-trained teacher model without relying on the original training data. The existing data-free learning methods mainly optimize the models with the pseudo data generated by GANs or the real data collected from the Internet. However, they inevitably suffer from the problems of unstable training or domain shifts with the original data. In this paper, we propose a novel Data-free Distillation with Degradation-prompt Diffusion framework for multi-weather Image Restoration (D4IR). It replaces GANs with pre-trained diffusion models to avoid model collapse and incorporates a degradation-aware prompt adapter to facilitate content-driven conditional diffusion for generating domain-related images. Specifically, a contrast-based degradation prompt adapter is firstly designed to capture degradation-aware prompts from web-collected degraded images. Then, the collected unpaired clean images are perturbed to latent features of stable diffusion, and conditioned with the degradation-aware prompts to synthesize new domain-related degraded images for knowledge distillation. Experiments illustrate that our proposal achieves comparable performance to the model distilled with original training data, and is even superior to other mainstream unsupervised methods.", "authors": ["Pei Wang", "Xiaotong Luo", "Yuan Xie", "Yanyun Qu"], "year": 2024, "venue": "arXiv (Cornell University)", "cited_by_count": 1, "type": "preprint", "concepts": ["Degradation (telecommunications)", "Distillation", "Environmental science", "Diffusion", "Image (mathematics)"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4409367756", "doi": "https://doi.org/10.1609/aaai.v39i5.32587", "title": "UP-Restorer: When Unrolling Meets Prompts for Unified Image Restoration", "abstract": "All-in-one restoration needs to implicitly distinguish between different degradation conditions and apply specific prior constraints accordingly. To fulfill this goal, our work makes the first effort to create an all-in-one restoration via unrolling from the typical maximum a-posterior optimization function. This unrolling framework naturally leads to the construction of progressively solving models, which are equivalent to a diffusion enhancer taking as input dynamically generated prompts. Under a score-based diffusion model, the prompts are integrated for propogating and updating several context-related variables, i.e. transmission map, atmospheric light map and noise or rain map progressively. Such learned prompt generation process, which simulates the nonlinear operations in the unrolled solution, is combined with linear operations owning clear physics implications to make the diffusion models well reguarlized and more effective in learning degradation-related visual priors. Experimental results demonstrate that our method achieves significant performance improvements across various image restoration tasks, realizing true all-in-one image restoration.", "authors": ["Minghao Liu", "Wenhan Yang", "Jinyi Luo", "Jiaying Liu"], "year": 2025, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 1, "type": "article", "concepts": ["Image (mathematics)", "Computer science", "Psychology", "Artificial intelligence"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4392972181", "doi": "https://doi.org/10.48550/arxiv.2403.11157", "title": "Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model", "abstract": "Universal image restoration is a practical and potential computer vision task for real-world applications. The main challenge of this task is handling the different degradation distributions at once. Existing methods mainly utilize task-specific conditions (e.g., prompt) to guide the model to learn different distributions separately, named multi-partite mapping. However, it is not suitable for universal model learning as it ignores the shared information between different tasks. In this work, we propose an advanced selective hourglass mapping strategy based on diffusion model, termed DiffUIR. Two novel considerations make our DiffUIR non-trivial. Firstly, we equip the model with strong condition guidance to obtain accurate generation direction of diffusion model (selective). More importantly, DiffUIR integrates a flexible shared distribution term (SDT) into the diffusion algorithm elegantly and naturally, which gradually maps different distributions into a shared one. In the reverse process, combined with SDT and strong condition guidance, DiffUIR iteratively guides the shared distribution to the task-specific distribution with high image quality (hourglass). Without bells and whistles, by only modifying the mapping strategy, we achieve state-of-the-art performance on five image restoration tasks, 22 benchmarks in the universal setting and zero-shot generalization setting. Surprisingly, by only using a lightweight model (only 0.89M), we could achieve outstanding performance. The source code and pre-trained models are available at https://github.com/iSEE-Laboratory/DiffUIR", "authors": ["Dian Zheng", "Xiaoming Wu", "Shuzhou Yang", "Jian Zhang", "Jian–Fang Hu", "Wei‐Shi Zheng"], "year": 2024, "venue": "arXiv (Cornell University)", "cited_by_count": 3, "type": "preprint", "concepts": ["Hourglass", "Image restoration", "Image (mathematics)", "Computer vision", "Computer science"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W2529869467", "doi": "https://doi.org/10.1016/j.scitotenv.2016.09.228", "title": "The EU Water Framework Directive: From great expectations to problems with implementation", "abstract": "The Water Framework Directive 2000/60/EC (WFD) is widely accepted as the most substantial and ambitious piece of European environmental legislation to date. It has been referred to as a once in a generation opportunity to restore Europe's waters and a potential template for future environmental regulations. However, fifteen years since it was adopted, and with many problems and delays in its implementation, the WFD has not delivered its main objectives of non-deterioration of water status and the achievement of good status for all EU waters. Putting aside the daunting technical and organisational challenges of its implementation, this paper aims to shed light on why the great expectations that came with the WFD have not yet been fully realised. It reviews how the Directive has been interpreted, focusing on its intentions and how they were applied. The findings reveal the absence of the paradigm shift towards the systems (integrated) thinking that the WFD was grounded on, as a fundamental problem with its implementation. This is also evident in cases where the Directive has been criticised as a policy tool or when implementation efforts were reviewed, indicating misunderstandings even of its core principles. This inherent departure from the Directive's systemic intention and methodological approach needs further investigation, as it could be the reason behind many of its problems and delays. Unless current implementation efforts are reviewed or revised in light of this, enabling the paradigm shift required to ensure a more sustainable and holistic approach to water management, the fading aspirations of the initial great expectations that came with the Directive could disappear for good.", "authors": ["Nikolaos Voulvoulis", "Karl Dominic Arpon", "Theodoros Giakoumis"], "year": 2016, "venue": "The Science of The Total Environment", "cited_by_count": 521, "type": "article", "concepts": ["Water Framework Directive", "Directive", "Legislation", "Environmental planning", "Paradigm shift"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W2904110151", "doi": "https://doi.org/10.1002/wcc.565", "title": "Explaining differential vulnerability to climate change: A social science review", "abstract": "The varied effects of recent extreme weather events around the world exemplify the uneven impacts of climate change on populations, even within relatively small geographic regions. Differential human vulnerability to environmental hazards results from a range of social, economic, historical, and political factors, all of which operate at multiple scales. While adaptation to climate change has been the dominant focus of policy and research agendas, it is essential to ask as well why some communities and peoples are disproportionately exposed to and affected by climate threats. The cases and synthesis presented here are organized around four key themes (resource access, governance, culture, and knowledge), which we approach from four social science fields (cultural anthropology, archaeology, human geography, and sociology). Social scientific approaches to human vulnerability draw vital attention to the root causes of climate change threats and the reasons that people are forced to adapt to them. Because vulnerability is a multidimensional process rather than an unchanging state, a dynamic social approach to vulnerability is most likely to improve mitigation and adaptation planning efforts. This article is categorized under:Vulnerability and Adaptation to Climate Change > Values-Based Approach to Vulnerability and Adaptation.", "authors": ["Kimberley Anh Thomas", "Dean Hardy", "Heather Lazrus", "Michael Méndez", "Ben Orlove", "Isabel Rivera‐Collazo", "J. Timmons Roberts", "Marcy Rockman", "Benjamin P. Warner", "Robert Winthrop"], "year": 2018, "venue": "Wiley Interdisciplinary Reviews Climate Change", "cited_by_count": 794, "type": "review", "concepts": ["Vulnerability (computing)", "Climate change", "Social vulnerability", "Vulnerability assessment", "Adaptation (eye)"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4411445611", "doi": "https://doi.org/10.2139/ssrn.5311960", "title": "Multi-Dimension Visual Prompt Enhanced Image Restoration Network Via Mamba-Transformer Aggregation", "abstract": "", "authors": ["Aiwen Jiang", "Hourong Chen", "Jihua Ye", "Mingwen Wang", "Bo Liu"], "year": 2025, "venue": "SSRN Electronic Journal", "cited_by_count": 1, "type": "preprint", "concepts": ["Transformer", "Dimension (graph theory)", "Image (mathematics)", "Computer science", "Mathematics"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W2336023654", "doi": "https://doi.org/10.5751/es-02716-130240", "title": "Adaptive Capacity and Traps", "abstract": "Adaptive capacity is the ability of a living system, such as a social-ecological system, to adjust responses to changing internal demands and external drivers. Although adaptive capacity is a frequent topic of study in the resilience literature, there are few formal models. This paper introduces such a model and uses it to explore adaptive capacity by contrast with the opposite condition, or traps. In a socialecological rigidity trap, strong self-reinforcing controls prevent the flexibility needed for adaptation. In the model, too much control erodes adaptive capacity and thereby increases the risk of catastrophic breakdown. In a social-ecological poverty trap, loose connections prevent the mobilization of ideas and resources to solve problems. In the model, too little control impedes the focus needed for adaptation. Fluctuations of internal demand or external shocks generate pulses of adaptive capacity, which may gain traction and pull the system out of the poverty trap. The model suggests some general properties of traps in social-ecological systems. It is general and flexible, so it can be used as a building block in more specific and detailed models of adaptive capacity for a particular region.", "authors": ["Stephen R. Carpenter", "William A. Brock"], "year": 2008, "venue": "Ecology and Society", "cited_by_count": 475, "type": "article", "concepts": ["Adaptive capacity", "Environmental resource management", "Geography", "Climate change", "Business"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4412488893", "doi": "https://doi.org/10.1109/tpami.2025.3589606", "title": "Re-Boosting Self-Collaboration Parallel Prompt GAN for Unsupervised Image Restoration", "abstract": "Deep learning methods have demonstrated state-of-the-art performance in image restoration, especially when trained on large-scale paired datasets. However, acquiring paired data in real-world scenarios poses a significant challenge. Unsupervised restoration approaches based on generative adversarial networks (GANs) offer a promising solution without requiring paired datasets. Yet, these GAN-based approaches struggle to surpass the performance of conventional unsupervised GAN-based frameworks without significantly modifying model structures or increasing the computational complexity. To address these issues, we propose a self-collaboration (SC) strategy for existing restoration models. This strategy utilizes information from the previous stage as feedback to guide subsequent stages, achieving significant performance improvement without increasing the framework's inference complexity. The SC strategy comprises a prompt learning (PL) module and a restorer ($Res$Res). It iteratively replaces the previous less powerful fixed restorer $\\overline{Res}$Res¯ in the PL module with a more powerful $Res$Res. The enhanced PL module generates better pseudo-degraded/clean image pairs, leading to a more powerful $Res$Res for the next iteration. Our SC can significantly improve the $Res$Res 's performance by over 1.5 dB without adding extra parameters or computational complexity during inference. Meanwhile, existing self-ensemble (SE) and our SC strategies enhance the performance of pre-trained restorers from different perspectives. As SE increases computational complexity during inference, we propose a re-boosting module to the SC (Reb-SC) to improve the SC strategy further by incorporating SE into SC without increasing inference time. This approach further enhances the restorer's performance by approximately 0.3 dB. Additionally, we present a baseline framework that includes parallel generative adversarial branches with complementary \"self-synthesis\" and \"unpaired-synthesis\" constraints, ensuring the effectiveness of the training framework. Extensive experimental results on restoration tasks demonstrate that the proposed model performs favorably against existing state-of-the-art unsupervised restoration methods.", "authors": ["Lin Xin", "Yuyan Zhou", "Jingtong Yue", "Chao Ren", "Kelvin C. K. Chan", "Qi Lu", "Ming-Hsuan Yang"], "year": 2025, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "cited_by_count": 2, "type": "article", "concepts": ["Boosting (machine learning)", "Inference", "Computer science", "Computational complexity theory", "Artificial intelligence"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W3009526937", "doi": "https://doi.org/10.1002/pan3.10080", "title": "Action needed for the EU Common Agricultural Policy to address sustainability challenges", "abstract": "Making agriculture sustainable is a global challenge. In the European Union (EU), the Common Agricultural Policy (CAP) is failing with respect to biodiversity, climate, soil, land degradation as well as socio-economic challenges.The European Commission's proposal for a CAP post-2020 provides a scope for enhanced sustainability. However, it also allows Member States to choose low-ambition implementation pathways. It therefore remains essential to address citizens' demands for sustainable agriculture and rectify systemic weaknesses in the CAP, using the full breadth of available scientific evidence and knowledge.Concerned about current attempts to dilute the environmental ambition of the future CAP, and the lack of concrete proposals for improving the CAP in the draft of the European Green Deal, we call on the European Parliament, Council and Commission to adopt 10 urgent action points for delivering sustainable food production, biodiversity conservation and climate mitigation.Knowledge is available to help moving towards evidence-based, sustainable European agriculture that can benefit people, nature and their joint futures.The statements made in this article have the broad support of the scientific community, as expressed by above 3,600 signatories to the preprint version of this manuscript. The list can be found here (https://doi.org/10.5281/zenodo.3685632).", "authors": ["Guy Pe’er", "Aletta Bonn", "Helge Bruelheide", "Petra Dieker", "Nico Eisenhauer", "Peter H. Feindt", "Gregor Hagedorn", "Bernd Hansjürgens", "Irina Herzon", "Ângela Lomba", "Elisabeth Marquard", "Francisco Moreira", "Heike Nitsch", "Rainer Oppermann", "Andrea Perino", "Norbert Röder", "Christian Schleyer", "Stefan Schindler", "Christine Wolf", "Yves Zinngrebe", "Sebastian Lakner"], "year": 2020, "venue": "People and Nature", "cited_by_count": 548, "type": "article", "concepts": ["Common Agricultural Policy", "Sustainability", "European union", "Business", "Parliament"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W2604754546", "doi": "https://doi.org/10.1038/ncomms14727", "title": "Dopamine neuronal loss contributes to memory and reward dysfunction in a model of Alzheimer’s disease", "abstract": "", "authors": ["Annalisa Nobili", "Emanuele Claudio Latagliata", "Maria Teresa Viscomi", "Virve Cavallucci", "Debora Cutuli", "Giacomo Giacovazzo", "Paraskevi Krashia", "Francesca Romana Rizzo", "Ramona Marino", "Mauro Federici", "Paola De Bartolo", "Daniela Aversa", "Maria Concetta Dell’Acqua", "Alberto Cordella", "Marco Sancandi", "Flavio Keller", "Laura Petrosini", "Stefano Puglisi‐Allegra", "Nicola Biagio Mercuri", "Roberto Coccurello", "Nicola Berretta", "Marcello D’Amelio"], "year": 2017, "venue": "Nature Communications", "cited_by_count": 487, "type": "article", "concepts": ["Ventral tegmental area", "Pars compacta", "Neuroscience", "Substantia nigra", "Dopamine"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4292488030", "doi": "https://doi.org/10.1038/s41591-022-01923-y", "title": "Cellular senescence and senolytics: the path to the clinic", "abstract": "", "authors": ["Selim Chaib", "Tamar Tchkonia", "James L. Kirkland"], "year": 2022, "venue": "Nature Medicine", "cited_by_count": 941, "type": "review", "concepts": ["Senescence", "Cellular senescence", "Medicine", "Clinical trial", "Psychological intervention"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W2775636951", "doi": "https://doi.org/10.1016/j.preteyeres.2017.11.003", "title": "Optical coherence tomography angiography", "abstract": "", "authors": ["Richard F. Spaide", "James G. Fujimoto", "Nadia K. Waheed", "Srinivas R. Sadda", "Giovanni Staurenghi"], "year": 2017, "venue": "Progress in Retinal and Eye Research", "cited_by_count": 1618, "type": "review", "concepts": ["Choroid", "Medicine", "Optical coherence tomography", "Macular degeneration", "Retina"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W3000049009", "doi": "https://doi.org/10.3390/land9010028", "title": "A Bibliometric Analysis on Land Degradation: Current Status, Development, and Future Directions", "abstract": "Land degradation is a global issue receiving much attention currently. In order to objectively reveal the research situation of land degradation, bibliometrix and biblioshiny software packages have been used to conduct data mining and quantitative analysis on research papers in the fields of land degradation during 1990–2019 (data update time was 8 April 2019) in the Web of Science core collection database. The results show that: (1) during the past 20 years, the number of papers on land degradation has increased. According to the number of articles, it is divided into four stages: a low-production exploration period, a developmental sprout period, expansion of the promotion period, and a high-yield active period. (2) Land-degradation research covers 93 countries or regions. The top five countries in terms of research volume are China, the United States, the United Kingdom, Germany, and Australia. China, the United States, and the United Kingdom are the most important countries for international cooperation in the field of land degradation. However, cooperation between countries is not very close overall. (3) Land degradation, degradation, desertification, remote sensing, soil erosion, and soil degradation are high-frequency keywords in the field of land degradation in recent years. (4) The research hotspots in the field of land degradation mainly focus on research directions such as restoration and reconstruction of land degradation, and sustainable management of land resources. (5) The themes of various periods in the field of land degradation are diversified, and the evolutionary relationship is complex. There are 15 evolutionary paths with regard to dynamic monitoring of land degradation, environmental governance of land degradation, and responses of land degradation to land-use change. Finally, the paper concludes that the research directions on land degradation in future include the process, mechanism, and effect of land degradation, the application of new technologies, new monitoring methods for land degradation, theory enhancement, methods and models of ecological restoration, reconstruction of degraded land, multidisciplinary integrated system research, constructing a policy guarantee system for the reconstruction of degraded land, and strengthening research on land resource engineering.", "authors": ["Hualin Xie", "Yanwei Zhang", "Zhilong Wu", "Tiangui Lv"], "year": 2020, "venue": "Land", "cited_by_count": 406, "type": "article", "concepts": ["Land degradation", "Desertification", "Sustainable land management", "China", "Land management"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W3169893408", "doi": "https://doi.org/10.1007/s40747-021-00428-4", "title": "Methods for image denoising using convolutional neural network: a review", "abstract": "Abstract Image denoising faces significant challenges, arising from the sources of noise. Specifically, Gaussian, impulse, salt, pepper, and speckle noise are complicated sources of noise in imaging. Convolutional neural network (CNN) has increasingly received attention in image denoising task. Several CNN methods for denoising images have been studied. These methods used different datasets for evaluation. In this paper, we offer an elaborate study on different CNN techniques used in image denoising. Different CNN methods for image denoising were categorized and analyzed. Popular datasets used for evaluating CNN image denoising methods were investigated. Several CNN image denoising papers were selected for review and analysis. Motivations and principles of CNN methods were outlined. Some state-of-the-arts CNN image denoising methods were depicted in graphical forms, while other methods were elaborately explained. We proposed a review of image denoising with CNN. Previous and recent papers on image denoising with CNN were selected. Potential challenges and directions for future research were equally fully explicated.", "authors": ["Ademola E. Ilesanmi", "Taiwo Ilesanmi"], "year": 2021, "venue": "Complex & Intelligent Systems", "cited_by_count": 348, "type": "review", "concepts": ["Noise reduction", "Convolutional neural network", "Computer science", "Artificial intelligence", "Non-local means"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W7117495353", "doi": "https://doi.org/10.1109/tgrs.2025.3649014", "title": "INP-Net: Implicit Neural Prompting Network for Remote Sensing Image Dehazing", "abstract": "Restoring high-quality images from hazy observations is crucial for visual perception and downstream detection in remote sensing applications. Recent deep learning-based methods have achieved remarkable progress in dehazing, however, they often suffer from either unreliable prompting guidance or inadequate global modeling. To bridge these gaps, we propose INP-Net, an Implicit Neural Prompting Network for remote sensing image dehazing. INP-Net introduces an implicit neural prompting mechanism that exploits learnable implicit neural representations (INR) in the YCbCr color space as degradation-insensitive prompts, which are dynamically injected into the RGB decoding pipeline. In addition, we design the Adaptive Sampling and Prior-enhanced (ASP) Transformer Block to enrich global feature diversity. The ASP comprises two core components: Adaptive Soft Sampling Self-Attention (ASSA), which performs probabilistic token selection to eliminate channel redundancy and enhance discriminative representations; and the Prior-Modulated Mixed-Scale Feed-Forward Network (PMFN), which leverages haze prior knowledge as a multi-scale modulator to guide local feature restoration. Extensive experiments on multiple benchmark datasets demonstrate that the proposed INP-Net surpasses state-of-the-art methods both quantitatively and qualitatively. Our method achieves PSNR gains of 0.47 dB and 0.75 dB over state-of-the-art methods on the challenging StateHaze1K-thick and NH-Haze datasets, respectively.", "authors": ["Shan Liang", "Tao Gao", "Ting Chen", "Yuanbo Wen", "Qianxi Zhang", "Xiao Wang"], "year": 2025, "venue": "IEEE Transactions on Geoscience and Remote Sensing", "cited_by_count": 1, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Redundancy (engineering)", "Computer vision", "Decoding methods"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W2114655563", "doi": "https://doi.org/10.3389/fnagi.2013.00034", "title": "A delicate balance: Iron metabolism and diseases of the brain", "abstract": "Iron is the most abundant transition metal within the brain, and is vital for a number of cellular processes including neurotransmitter synthesis, myelination of neurons, and mitochondrial function. Redox cycling between ferrous and ferric iron is utilized in biology for various electron transfer reactions essential to life, yet this same chemistry mediates deleterious reactions with oxygen that induce oxidative stress. Consequently, there is a precise and tightly controlled mechanism to regulate iron in the brain. When iron is dysregulated, both conditions of iron overload and iron deficiencies are harmful to the brain. This review focuses on how iron metabolism is maintained in the brain, and how an alteration to iron and iron metabolism adversely affects neurological function.", "authors": ["Dominic J. Hare", "Scott Ayton", "Ashley I. Bush", "Peng Lei"], "year": 2013, "venue": "Frontiers in Aging Neuroscience", "cited_by_count": 429, "type": "article", "concepts": ["Ferrous", "Metabolism", "Oxidative stress", "Brain function", "Ferric"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W3202997660", "doi": "https://doi.org/10.1038/s43587-021-00121-8", "title": "Strategies for targeting senescent cells in human disease", "abstract": "", "authors": ["Nathan Gasek", "George A. Kuchel", "James L. Kirkland", "Ming Xu"], "year": 2021, "venue": "Nature Aging", "cited_by_count": 455, "type": "review", "concepts": ["Senescence", "Paracrine signalling", "Biology", "Carcinogenesis", "Disease"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4404431554", "doi": "https://doi.org/10.1016/j.jenvman.2024.123336", "title": "Characterizing and modeling spatiotemporal trends in rangelands: Prosopis juliflora impact in middle Awash Basin, Ethiopia", "abstract": "", "authors": ["Kalid Hassen Yasin"], "year": 2024, "venue": "Journal of Environmental Management", "cited_by_count": 3, "type": "article", "concepts": ["Rangeland", "Environmental science", "Vegetation (pathology)", "Edaphic", "Prosopis juliflora"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4392775085", "doi": "https://doi.org/10.1038/s41392-024-01745-z", "title": "Nanotechnology’s frontier in combatting infectious and inflammatory diseases: prevention and treatment", "abstract": "Inflammation-associated diseases encompass a range of infectious diseases and non-infectious inflammatory diseases, which continuously pose one of the most serious threats to human health, attributed to factors such as the emergence of new pathogens, increasing drug resistance, changes in living environments and lifestyles, and the aging population. Despite rapid advancements in mechanistic research and drug development for these diseases, current treatments often have limited efficacy and notable side effects, necessitating the development of more effective and targeted anti-inflammatory therapies. In recent years, the rapid development of nanotechnology has provided crucial technological support for the prevention, treatment, and detection of inflammation-associated diseases. Various types of nanoparticles (NPs) play significant roles, serving as vaccine vehicles to enhance immunogenicity and as drug carriers to improve targeting and bioavailability. NPs can also directly combat pathogens and inflammation. In addition, nanotechnology has facilitated the development of biosensors for pathogen detection and imaging techniques for inflammatory diseases. This review categorizes and characterizes different types of NPs, summarizes their applications in the prevention, treatment, and detection of infectious and inflammatory diseases. It also discusses the challenges associated with clinical translation in this field and explores the latest developments and prospects. In conclusion, nanotechnology opens up new possibilities for the comprehensive management of infectious and inflammatory diseases.", "authors": ["Yujing Huang", "Xiaohan Guo", "Yi Wu", "Xingyu Chen", "Lixiang Feng", "Na Xie", "Guobo Shen"], "year": 2024, "venue": "Signal Transduction and Targeted Therapy", "cited_by_count": 368, "type": "article", "concepts": ["Medicine", "Drug development", "Immunogenicity", "Drug", "Intensive care medicine"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W2553239728", "doi": "https://doi.org/10.1523/jneurosci.2351-16.2016", "title": "Casting a Wide Net: Role of Perineuronal Nets in Neural Plasticity", "abstract": "Perineuronal nets (PNNs) are unique extracellular matrix structures that wrap around certain neurons in the CNS during development and control plasticity in the adult CNS. They appear to contribute to a wide range of diseases/disorders of the brain, are involved in recovery from spinal cord injury, and are altered during aging, learning and memory, and after exposure to drugs of abuse. Here the focus is on how a major component of PNNs, chondroitin sulfate proteoglycans, control plasticity, and on the role of PNNs in memory in normal aging, in a tauopathy model of Alzheimer's disease, and in drug addiction. Also discussed is how altered extracellular matrix/PNN formation during development may produce synaptic pathology associated with schizophrenia, bipolar disorder, major depression, and autism spectrum disorders. Understanding the molecular underpinnings of how PNNs are altered in normal physiology and disease will offer insights into new treatment approaches for these diseases.", "authors": ["Barbara A. Sorg", "Sabina Berretta", "Jordan M. Blacktop", "James W. Fawcett", "Hiroshi Kitagawa", "Jessica C. F. Kwok", "Marta Miquel"], "year": 2016, "venue": "Journal of Neuroscience", "cited_by_count": 453, "type": "review", "concepts": ["Perineuronal net", "Neuroscience", "Neuroplasticity", "Synaptic plasticity", "Biology"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W2023606650", "doi": "https://doi.org/10.1016/j.echo.2004.07.013", "title": "American Society of Echocardiography recommendations for use of echocardiography in clinical trials", "abstract": "", "authors": ["John S. Gottdiener", "James Bednarz", "Richard B. Devereux", "Julius M. Gardin", "Allan L. Klein", "Warren J. Manning", "Annitta Morehead", "Dalane W. Kitzman", "Jae K. Oh", "Miguel A. Quiñones", "Nelson B. Schiller", "James H. Stein", "Neil J. Weissman"], "year": 2004, "venue": "Journal of the American Society of Echocardiography", "cited_by_count": 906, "type": "review", "concepts": ["Medicine", "Reproducibility", "Cardiology", "Internal medicine", "Doppler echocardiography"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4387533377", "doi": "https://doi.org/10.1016/j.jenvman.2023.119230", "title": "A systematic review of industrial wastewater management: Evaluating challenges and enablers", "abstract": "", "authors": ["Bikram Jit Singh", "Ayon Chakraborty", "Rippin Sehgal"], "year": 2023, "venue": "Journal of Environmental Management", "cited_by_count": 454, "type": "review", "concepts": ["Stakeholder", "Sustainability", "Business", "Circular economy", "Industrial wastewater treatment"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4388461325", "doi": "https://doi.org/10.1016/j.ecoinf.2023.102365", "title": "The first inventory of gullies in the Upper Taquari River Basin (Brazil) and its agreement with land use classes", "abstract": "", "authors": ["Rômullo Oliveira Louzada", "Ivan Bergier", "Fábio de Oliveira Roque"], "year": 2023, "venue": "Ecological Informatics", "cited_by_count": 3, "type": "article", "concepts": ["Context (archaeology)", "Vegetation (pathology)", "Hydrology (agriculture)", "Land use", "Terrain"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W2084852600", "doi": "https://doi.org/10.1007/s13280-012-0256-7", "title": "A Review of the Elements of Human Well-Being with an Emphasis on the Contribution of Ecosystem Services", "abstract": "", "authors": ["James K. Summers", "Lisa M. Smith", "Jason L. Case", "Rick A. Linthurst"], "year": 2012, "venue": "AMBIO", "cited_by_count": 377, "type": "review", "concepts": ["Emphasis (telecommunications)", "Ecosystem services", "Ecosystem", "Environmental resource management", "Business"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4304780007", "doi": "https://doi.org/10.1038/s41586-022-05318-4", "title": "A function-based typology for Earth’s ecosystems", "abstract": "", "authors": ["David A. Keith", "José R. Ferrer‐Paris", "Emily Nicholson", "Melanie J. Bishop", "Beth Polidoro", "Eva Ramírez-Llodra", "Mark G. Tozer", "Jeanne Nel", "Ralph Mac Nally", "Edward J. Gregr", "Kate E. Watermeyer", "Franz Essl", "Don Faber‐Langendoen", "Janet Franklin", "Caroline E. R. Lehmann", "Andrés Etter", "Dirk J. Roux", "Jonathan S. Stark", "Jessica A. Rowland", "Neil Brummitt", "U. Fernández-Arcaya", "Iain M. Suthers", "Susan K. Wiser", "Ian Donohue", "Leland J. Jackson", "R. Toby Pennington", "Thomas M. Iliffe", "Vasilis Gerovasileiou", "Paul S. Giller", "Belinda J. Robson", "Nathalie Pettorelli", "Ángela Andrade", "Arild Lindgaard", "Teemu Tahvanainen", "Aleks Terauds", "Michael A. Chadwick", "Nicholas Murray", "Justin Moat", "Patricio Pliscoff", "Irene Zager", "Richard T. Kingsford"], "year": 2022, "venue": "Nature", "cited_by_count": 302, "type": "article", "concepts": ["Ecosystem services", "Environmental resource management", "Convention on Biological Diversity", "Ecosystem management", "Total human ecosystem"], "search_query": "prompt-based image restoration learning degradation"}
{"openalex_id": "https://openalex.org/W4225672218", "doi": "https://doi.org/10.1109/cvpr52688.2022.00564", "title": "Restormer: Efficient Transformer for High-Resolution Image Restoration", "abstract": "Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shortcomings of CNNs (i.e., limited receptive field and inadaptability to input content), its computational complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to most image restoration tasks involving high-resolution images. In this work, we propose an efficient Transformer model by making several key designs in the building blocks (multi-head attention and feed-forward network) such that it can capture long-range pixel interactions, while still remaining applicable to large images. Our model, named Restoration Transformer (Restormer), achieves state-of-the-art results on several image restoration tasks, including image deraining, single-image motion deblurring, defocus deblurring (single-image and dual-pixel data), and image denoising (Gaussian grayscale/color denoising, and real image denoising). The source code and pre-trained models are available at https://github.com/swz30/Restormer.", "authors": ["Syed Waqas Zamir", "Aditya Arora", "Salman Khan", "Munawar Hayat", "Fahad Shahbaz Khan", "Ming–Hsuan Yang"], "year": 2022, "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "cited_by_count": 3123, "type": "article", "concepts": ["Deblurring", "Computer science", "Artificial intelligence", "Image restoration", "Computer vision"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4312812783", "doi": "https://doi.org/10.1109/cvpr52688.2022.01716", "title": "Uformer: A General U-Shaped Transformer for Image Restoration", "abstract": "In this paper, we present Uformer, an effective and efficient Transformer-based architecture for image restoration, in which we build a hierarchical encoder-decoder network using the Transformer block. In Uformer, there are two core designs. First, we introduce a novel locally-enhanced window (LeWin) Transformer block, which performs non-overlapping window-based self-attention instead of global self-attention. It significantly reduces the computational complexity on high resolution feature map while capturing local context. Second, we propose a learnable multi-scale restoration modulator in the form of a multi-scale spatial bias to adjust features in multiple layers of the Uformer decoder. Our modulator demonstrates superior capability for restoring details for various image restoration tasks while introducing marginal extra parameters and computational cost. Powered by these two designs, Uformer enjoys a high capability for capturing both local and global dependencies for image restoration. To evaluate our approach, extensive experiments are conducted on several image restoration tasks, including image denoising, motion deblurring, defocus deblurring and deraining. Without bells and whistles, our Uformer achieves superior or comparable performance compared with the state-of-the-art algorithms. The code and models are available at https://github.com/ZhendongWang6/Uformer.", "authors": ["Zhendong Wang", "Xiaodong Cun", "Jianmin Bao", "Wengang Zhou", "Jianzhuang Liu", "Houqiang Li"], "year": 2022, "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "cited_by_count": 1866, "type": "article", "concepts": ["Deblurring", "Image restoration", "Computer science", "Transformer", "Encoder"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4386075642", "doi": "https://doi.org/10.1109/cvpr52729.2023.00570", "title": "Efficient Frequency Domain-based Transformers for High-Quality Image Deblurring", "abstract": "We present an effective and efficient method that explores the properties of Transformers in the frequency domain for high-quality image deblurring. Our method is motivated by the convolution theorem that the correlation or convolution of two signals in the spatial domain is equivalent to an element-wise product of them in the frequency domain. This inspires us to develop an efficient frequency domain-based self-attention solver (FSAS) to estimate the scaled dot-product attention by an element-wise product operation instead of the matrix multiplication in the spatial domain. In addition, we note that simply using the naive feed-forward network (FFN) in Transformers does not generate good deblurred results. To overcome this problem, we propose a simple yet effective discriminative frequency domain-based FFN (DFFN), where we introduce a gated mechanism in the FFN based on the Joint Photographic Experts Group (JPEG) compression algorithm to discriminatively determine which low- and high-frequency information of the features should be preserved for latent clear image restoration. We formulate the proposed FSAS and DFFN into an asymmetrical network based on an encoder and decoder architecture, where the FSAS is only used in the decoder module for better image deblurring. Experimental results show that the proposed method performs favorably against the state-of-the-art approaches.", "authors": ["Lingshun Kong", "Jiangxin Dong", "Jianjun Ge", "Mingqiang Li", "Jinshan Pan"], "year": 2023, "venue": "", "cited_by_count": 263, "type": "article", "concepts": ["Deblurring", "Computer science", "Frequency domain", "Transformer", "Image quality"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4321497943", "doi": "https://doi.org/10.3390/s23052385", "title": "Vision Transformers in Image Restoration: A Survey", "abstract": "The Vision Transformer (ViT) architecture has been remarkably successful in image restoration. For a while, Convolutional Neural Networks (CNN) predominated in most computer vision tasks. Now, both CNN and ViT are efficient approaches that demonstrate powerful capabilities to restore a better version of an image given in a low-quality format. In this study, the efficiency of ViT in image restoration is studied extensively. The ViT architectures are classified for every task of image restoration. Seven image restoration tasks are considered: Image Super-Resolution, Image Denoising, General Image Enhancement, JPEG Compression Artifact Reduction, Image Deblurring, Removing Adverse Weather Conditions, and Image Dehazing. The outcomes, the advantages, the limitations, and the possible areas for future research are detailed. Overall, it is noted that incorporating ViT in the new architectures for image restoration is becoming a rule. This is due to some advantages compared to CNN, such as better efficiency, especially when more data are fed to the network, robustness in feature extraction, and a better feature learning approach that sees better the variances and characteristics of the input. Nevertheless, some drawbacks exist, such as the need for more data to show the benefits of ViT over CNN, the increased computational cost due to the complexity of the self-attention block, a more challenging training process, and the lack of interpretability. These drawbacks represent the future research direction that should be targeted to increase the efficiency of ViT in the image restoration domain.", "authors": ["Anas M. Ali", "Bilel Benjdira", "Anis Koubâa", "Walid El‐Shafai", "Zahid Khan", "Wadii Boulila"], "year": 2023, "venue": "Sensors", "cited_by_count": 109, "type": "article", "concepts": ["Image restoration", "Deblurring", "Computer science", "Artificial intelligence", "Convolutional neural network"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4360887305", "doi": "https://doi.org/10.1109/tip.2023.3261747", "title": "CTCNet: A CNN-Transformer Cooperation Network for Face Image Super-Resolution", "abstract": "Recently, deep convolution neural networks (CNNs) steered face super-resolution methods have achieved great progress in restoring degraded facial details by joint training with facial priors. However, these methods have some obvious limitations. On the one hand, multi-task joint learning requires additional marking on the dataset, and the introduced prior network will significantly increase the computational cost of the model. On the other hand, the limited receptive field of CNN will reduce the fidelity and naturalness of the reconstructed facial images, resulting in suboptimal reconstructed images. In this work, we propose an efficient CNN-Transformer Cooperation Network (CTCNet) for face super-resolution tasks, which uses the multi-scale connected encoder-decoder architecture as the backbone. Specifically, we first devise a novel Local-Global Feature Cooperation Module (LGCM), which is composed of a Facial Structure Attention Unit (FSAU) and a Transformer block, to promote the consistency of local facial detail and global facial structure restoration simultaneously. Then, we design an efficient Feature Refinement Module (FRM) to enhance the encoded features. Finally, to further improve the restoration of fine facial details, we present a Multi-scale Feature Fusion Unit (MFFU) to adaptively fuse the features from different stages in the encoder procedure. Extensive evaluations on various datasets have assessed that the proposed CTCNet can outperform other state-of-the-art methods significantly. Source code will be available at https://github.com/IVIPLab/CTCNet.", "authors": ["Guangwei Gao", "Zixiang Xu", "Juncheng Li", "Jian Yang", "Tieyong Zeng", "Guo-Jun Qi"], "year": 2023, "venue": "IEEE Transactions on Image Processing", "cited_by_count": 151, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Computer vision", "Face (sociological concept)", "Image resolution"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4406457474", "doi": "https://doi.org/10.1109/tcsvt.2025.3530090", "title": "VmambaIR: Visual State Space Model for Image Restoration", "abstract": "Image restoration is a critical task in low-level computer vision, aiming to restore high-quality images from degraded inputs. Various models, such as convolutional neural networks (CNNs), generative adversarial networks (GANs), transformers, and diffusion models (DMs), have been employed to address this problem with significant impact. However, CNNs have limitations in capturing long-range dependencies. DMs require large prior models and computationally intensive denoising steps. Transformers have powerful modeling capabilities but face challenges due to quadratic complexity with input image size. To tackle these challenges, we propose VmambaIR, one of the first works to introduce State Space Models (SSMs) with linear complexity into comprehensive image restoration tasks. Specifically, we utilize a Unet architecture to stack our proposed Omni Selective Scan (OSS) blocks, consisting of an OSS module and an Efficient Feed-Forward Network (EFFN). Our proposed omni selective scan mechanism overcomes the unidirectional modeling limitation of SSMs by efficiently modeling image information flows in all six directions to better exploit surrounding restoration information. Furthermore, we conducted a comprehensive evaluation of our VmambaIR across multiple image restoration tasks, including image deraining, single image super-resolution, and real-world image super-resolution. Extensive experimental results demonstrate that our proposed VmambaIR achieves state-of-the-art (SOTA) performance with much fewer computational resources and parameters. Our research highlights the potential of state space models as promising alternatives to the transformer and CNN architectures in serving as foundational frameworks for next-generation low-level visual tasks.", "authors": ["Yuan Shi", "Bin Xia", "Xiaoyu Jin", "Xing Wang", "Tianyu Zhao", "Xin Xia", "Xuefeng Xiao", "Wenming Yang"], "year": 2025, "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "cited_by_count": 93, "type": "article", "concepts": ["Image restoration", "Computer vision", "Computer science", "Artificial intelligence", "Image (mathematics)"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W2984522085", "doi": "https://doi.org/10.1109/tci.2019.2911881", "title": "Deep Spatial–Spectral Representation Learning for Hyperspectral Image Denoising", "abstract": "Deep learning has found successful applications in restoration of two-dimensional (2-D) images including denoising, dehazing, and superresolution. However, existing deep convolutional neural network (DCNN) architecture cannot fully exploit spatial-spectral correlations in three-dimensional (3-D) hyperspectral images (HSIs) (directly extending 2-D DCNN into 3-D will significantly increase computational complexity); meantime, unlike 2-D images, there is an obstacle caused by the shortage of training data for HSIs. To meet those challenges, we present a novel, deep-learning framework for 3-D HSI denoising with the following contributions. First, inspired by the success of U-net in low-dose current-transformer denoising, we propose a novel approach of encoding rich multi-scale information of HSIs by a modified 3-D U-net. Second, we present a computationally efficient implementation of 3-D U-net based on the strategy of separable filtering. By decomposing 3-D filtering into 2-D spatial filtering and 1-D spectral filtering, we can achieve substantial savings on the number of network parameters to keep the computational complexity low. Third, we have developed a transfer learning approach of synthetically generating HSIs from RGB images as supplementary training data. The synthesized HSIs are used for the initial training of the modified 3-D U-net denoising network, which will be fine-tuned on real HSI images. Experimental results have shown that the proposed 3-D U-net denoising method significantly outperforms existing model-based HSI denoising methods.", "authors": ["Weisheng Dong", "Huan Wang", "Fangfang Wu", "Guangming Shi", "Xin Li"], "year": 2019, "venue": "IEEE Transactions on Computational Imaging", "cited_by_count": 127, "type": "article", "concepts": ["Hyperspectral imaging", "Artificial intelligence", "Noise reduction", "Computer science", "Deep learning"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4404654752", "doi": "https://doi.org/10.1007/978-3-031-72995-9_22", "title": "When Fast Fourier Transform Meets Transformer for Image Restoration", "abstract": "", "authors": ["Xingyu Jiang", "Xiuhui Zhang", "Ning Gao", "Yue Deng"], "year": 2024, "venue": "Lecture notes in computer science", "cited_by_count": 36, "type": "book-chapter", "concepts": ["Computer science", "Fourier transform", "Image restoration", "Transformer", "Computer vision"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4404687984", "doi": "https://doi.org/10.1109/tip.2024.3501855", "title": "MWFormer: Multi-Weather Image Restoration Using Degradation-Aware Transformers", "abstract": "Restoring images captured under adverse weather conditions is a fundamental task for many computer vision applications. However, most existing weather restoration approaches are only capable of handling a specific type of degradation, which is often insufficient in real-world scenarios, such as rainy-snowy or rainy-hazy weather. Towards being able to address these situations, we propose a multi-weather Transformer, or MWFormer for short, which is a holistic vision Transformer that aims to solve multiple weather-induced degradations using a single, unified architecture. MWFormer uses hyper-networks and feature-wise linear modulation blocks to restore images degraded by various weather types using the same set of learned parameters. We first employ contrastive learning to train an auxiliary network that extracts content-independent, distortion-aware feature embeddings that efficiently represent predicted weather types, of which more than one may occur. Guided by these weather-informed predictions, the image restoration Transformer adaptively modulates its parameters to conduct both local and global feature processing, in response to multiple possible weather. Moreover, MWFormer allows for a novel way of tuning, during application, to either a single type of weather restoration or to hybrid weather restoration without any retraining, offering greater controllability than existing methods. Our experimental results on multi-weather restoration benchmarks show that MWFormer achieves significant performance improvements compared to existing state-of-the-art methods, without requiring much computational cost. Moreover, we demonstrate that our methodology of using hyper-networks can be integrated into various network architectures to further boost their performance. The code is available at: https://github.com/taco-group/MWFormer.", "authors": ["Ruoxi Zhu", "Zhengzhong Tu", "Jiaming Liu", "Alan C. Bovik", "Yibo Fan"], "year": 2024, "venue": "IEEE Transactions on Image Processing", "cited_by_count": 28, "type": "article", "concepts": ["Image restoration", "Computer science", "Image processing", "Degradation (telecommunications)", "Computer vision"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4221151978", "doi": "https://doi.org/10.1145/3528223.3530127", "title": "Instant neural graphics primitives with a multiresolution hash encoding", "abstract": "Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.", "authors": ["Thomas Müller", "Alex Evans", "Christoph Schied", "Alexander Keller"], "year": 2022, "venue": "ACM Transactions on Graphics", "cited_by_count": 3366, "type": "article", "concepts": ["Computer science", "Hash function", "Speedup", "Rendering (computer graphics)", "Artificial neural network"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4372266706", "doi": "https://doi.org/10.1109/icassp49357.2023.10095242", "title": "Sandformer: CNN and Transformer under Gated Fusion for Sand Dust Image Restoration", "abstract": "Although Convolutional Neural Networks (CNN) have made good progress in image restoration, the intrinsic equivalence and locality of convolutions still constrain further improvements in image quality. Recent vision transformer and selfattention have achieved promising results on various computer vision tasks. However, directly utilizing Transformer for image restoration is a challenging task. In this paper, we introduce an effective hybrid architecture for sand image restoration tasks, which leverages local features from CNN and long-range dependencies captured by transformer to improve the results further. We propose an efficient hybrid structure for sand dust image restoration to solve the feature inconsistency issue between Transformer and CNN. The framework complements each representation by modulating features from the CNN-based and Transformer-based branches rather than simply adding or concatenating features. Experiments demonstrate that SandFormer achieves significant performance improvements in synthetic and real dust scenes compared to previous sand image restoration methods.", "authors": ["Jun Shi", "Bingcai Wei", "Gang Zhou", "Liye Zhang"], "year": 2023, "venue": "", "cited_by_count": 10, "type": "article", "concepts": ["Transformer", "Computer science", "Image restoration", "Convolutional neural network", "Locality"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4382173267", "doi": "https://doi.org/10.48550/arxiv.2306.13653", "title": "ProRes: Exploring Degradation-aware Visual Prompt for Universal Image Restoration", "abstract": "Image restoration aims to reconstruct degraded images, e.g., denoising or deblurring. Existing works focus on designing task-specific methods and there are inadequate attempts at universal methods. However, simply unifying multiple tasks into one universal architecture suffers from uncontrollable and undesired predictions. To address those issues, we explore prompt learning in universal architectures for image restoration tasks. In this paper, we present Degradation-aware Visual Prompts, which encode various types of image degradation, e.g., noise and blur, into unified visual prompts. These degradation-aware prompts provide control over image processing and allow weighted combinations for customized image restoration. We then leverage degradation-aware visual prompts to establish a controllable and universal model for image restoration, called ProRes, which is applicable to an extensive range of image restoration tasks. ProRes leverages the vanilla Vision Transformer (ViT) without any task-specific designs. Furthermore, the pre-trained ProRes can easily adapt to new tasks through efficient prompt tuning with only a few images. Without bells and whistles, ProRes achieves competitive performance compared to task-specific methods and experiments can demonstrate its ability for controllable restoration and adaptation for new tasks. The code and models will be released in \\url{https://github.com/leonmakise/ProRes}.", "authors": ["Jiaqi Ma", "Tianheng Cheng", "Guoli Wang", "Qian Zhang", "Xinggang Wang", "Lefei Zhang"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 14, "type": "preprint", "concepts": ["Deblurring", "Image restoration", "Computer science", "Artificial intelligence", "Leverage (statistics)"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4302013160", "doi": "https://doi.org/10.48550/arxiv.2210.01069", "title": "Dual-former: Hybrid Self-attention Transformer for Efficient Image Restoration", "abstract": "Recently, image restoration transformers have achieved comparable performance with previous state-of-the-art CNNs. However, how to efficiently leverage such architectures remains an open problem. In this work, we present Dual-former whose critical insight is to combine the powerful global modeling ability of self-attention modules and the local modeling ability of convolutions in an overall architecture. With convolution-based Local Feature Extraction modules equipped in the encoder and the decoder, we only adopt a novel Hybrid Transformer Block in the latent layer to model the long-distance dependence in spatial dimensions and handle the uneven distribution between channels. Such a design eliminates the substantial computational complexity in previous image restoration transformers and achieves superior performance on multiple image restoration tasks. Experiments demonstrate that Dual-former achieves a 1.91dB gain over the state-of-the-art MAXIM method on the Indoor dataset for single image dehazing while consuming only 4.2% GFLOPs as MAXIM. For single image deraining, it exceeds the SOTA method by 0.1dB PSNR on the average results of five datasets with only 21.5% GFLOPs. Dual-former also substantially surpasses the latest desnowing method on various datasets, with fewer parameters.", "authors": ["Sixiang Chen", "Ye Tian", "Yun Liu", "Erkang Chen"], "year": 2022, "venue": "arXiv (Cornell University)", "cited_by_count": 11, "type": "preprint", "concepts": ["FLOPS", "Computer science", "Transformer", "Encoder", "Leverage (statistics)"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W3155072588", "doi": "https://doi.org/10.1109/tpami.2022.3204461", "title": "Image Super-Resolution Via Iterative Refinement", "abstract": "We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models (Ho et al. 2020), (Sohl-Dickstein et al. 2015) to image-to-image translation, and performs super-resolution through a stochastic iterative denoising process. Output images are initialized with pure Gaussian noise and iteratively refined using a U-Net architecture that is trained on denoising at various noise levels, conditioned on a low-resolution input image. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8× face super-resolution task on CelebA-HQ for which SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GAN baselines do not exceed a fool rate of 34%. We evaluate SR3 on a 4× super-resolution task on ImageNet, where SR3 outperforms baselines in human evaluation and classification accuracy of a ResNet-50 classifier trained on high-resolution images. We further show the effectiveness of SR3 in cascaded image generation, where a generative model is chained with super-resolution models to synthesize high-resolution images with competitive FID scores on the class-conditional 256×256 ImageNet generation challenge.", "authors": ["Chitwan Saharia", "Jonathan Ho", "William Chan", "Tim Salimans", "David J. Fleet", "Mohammad Norouzi"], "year": 2022, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "cited_by_count": 1551, "type": "article", "concepts": ["Artificial intelligence", "Computer science", "Pattern recognition (psychology)", "Image resolution", "Computer vision"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4391667544", "doi": "https://doi.org/10.48550/arxiv.2402.04139", "title": "U-shaped Vision Mamba for Single Image Dehazing", "abstract": "Currently, Transformer is the most popular architecture for image dehazing, but due to its large computational complexity, its ability to handle long-range dependency is limited on resource-constrained devices. To tackle this challenge, we introduce the U-shaped Vision Mamba (UVM-Net), an efficient single-image dehazing network. Inspired by the State Space Sequence Models (SSMs), a new deep sequence model known for its power to handle long sequences, we design a Bi-SSM block that integrates the local feature extraction ability of the convolutional layer with the ability of the SSM to capture long-range dependencies. Extensive experimental results demonstrate the effectiveness of our method. Our method provides a more highly efficient idea of long-range dependency modeling for image dehazing as well as other image restoration tasks. The URL of the code is \\url{https://github.com/zzr-idam/UVM-Net}. Our method takes only \\textbf{0.009} seconds to infer a $325 \\times 325$ resolution image (100FPS) without I/O handling time.", "authors": ["Zhuoran Zheng", "Chen Wu"], "year": 2024, "venue": "arXiv (Cornell University)", "cited_by_count": 15, "type": "preprint", "concepts": ["Computer vision", "Image (mathematics)", "Artificial intelligence", "Computer science", "Geology"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4391417547", "doi": "https://doi.org/10.48550/arxiv.2401.15235", "title": "CascadedGaze: Efficiency in Global Context Extraction for Image Restoration", "abstract": "Image restoration tasks traditionally rely on convolutional neural networks. However, given the local nature of the convolutional operator, they struggle to capture global information. The promise of attention mechanisms in Transformers is to circumvent this problem, but it comes at the cost of intensive computational overhead. Many recent studies in image restoration have focused on solving the challenge of balancing performance and computational cost via Transformer variants. In this paper, we present CascadedGaze Network (CGNet), an encoder-decoder architecture that employs Global Context Extractor (GCE), a novel and efficient way to capture global information for image restoration. The GCE module leverages small kernels across convolutional layers to learn global dependencies, without requiring self-attention. Extensive experimental results show that our computationally efficient approach performs competitively to a range of state-of-the-art methods on synthetic image denoising and single image deblurring tasks, and pushes the performance boundary further on the real image denoising task.", "authors": ["Amirhosein Ghasemabadi", "Mohammad Salameh", "Muhammad Kamran Janjua", "Chunhua Zhou", "Fengyu Sun", "Di Niu"], "year": 2024, "venue": "arXiv (Cornell University)", "cited_by_count": 8, "type": "preprint", "concepts": ["Image restoration", "Context (archaeology)", "Extraction (chemistry)", "Image (mathematics)", "Computer science"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4399747822", "doi": "https://doi.org/10.1016/j.compag.2024.109169", "title": "Low-light wheat image enhancement using an explicit inter-channel sparse transformer", "abstract": "", "authors": ["Yu Wang", "Fei Wang", "Kun Li", "Xuping Feng", "Wenhui Hou", "Lu Liu", "Liqing Chen", "Yong He", "Yuwei Wang", "Yuwei Wang", "Yuwei Wang"], "year": 2024, "venue": "Computers and Electronics in Agriculture", "cited_by_count": 14, "type": "article", "concepts": ["Transformer", "Artificial intelligence", "Computer science", "Computer vision", "Pattern recognition (psychology)"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4377971563", "doi": "https://doi.org/10.2139/ssrn.4458244", "title": "Towards an Effective and Efficient Transformer for Rain-by-Snow Weather Removal", "abstract": "", "authors": ["Tao Gao", "Yuanbo Wen", "Kaihao Zhang", "Peng Cheng", "Ting Chen"], "year": 2023, "venue": "SSRN Electronic Journal", "cited_by_count": 8, "type": "preprint", "concepts": ["Snow", "Rain and snow mixed", "Environmental science", "Meteorology", "Transformer"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W2979040987", "doi": "https://doi.org/10.1109/access.2019.2945545", "title": "Comprehensive Review of Artificial Neural Network Applications to Pattern Recognition", "abstract": "The era of artificial neural network (ANN) began with a simplified application in many fields and remarkable success in pattern recognition (PR) even in manufacturing industries. Although significant progress achieved and surveyed in addressing ANN application to PR challenges, nevertheless, some problems are yet to be resolved like whimsical orientation (the unknown path that cannot be accurately calculated due to its directional position). Other problem includes; object classification, location, scaling, neurons behavior analysis in hidden layers, rule, and template matching. Also, the lack of extant literature on the issues associated with ANN application to PR seems to slow down research focus and progress in the field. Hence, there is a need for state-of-the-art in neural networks application to PR to urgently address the above-highlights problems for more successes. The study furnishes readers with a clearer understanding of the current, and new trend in ANN models that effectively addresses PR challenges to enable research focus and topics. Similarly, the comprehensive review reveals the diverse areas of the success of ANN models and their application to PR. In evaluating the performance of ANN models, some statistical indicators for measuring the performance of the ANN model in many studies were adopted. Such as the use of mean absolute percentage error (MAPE), mean absolute error (MAE), root mean squared error (RMSE), and variance of absolute percentage error (VAPE). The result shows that the current ANN models such as GAN, SAE, DBN, RBM, RNN, RBFN, PNN, CNN, SLP, MLP, MLNN, Reservoir computing, and Transformer models are performing excellently in their application to PR tasks. Therefore, the study recommends the research focus on current models and the development of new models concurrently for more successes in the field.", "authors": ["Oludare Isaac Abiodun", "Muhammad Ubale Kiru", "Aman Jantan", "Abiodun Esther Omolara", "Kemi Victoria Dada", "Abubakar Malah Umar", "Okafor Uchenwa Linus", "Humaira Arshad", "Abdullahi Aminu Kazaure", "Usman M. Gana"], "year": 2019, "venue": "IEEE Access", "cited_by_count": 688, "type": "article", "concepts": ["Mean squared error", "Artificial neural network", "Computer science", "Mean absolute percentage error", "Artificial intelligence"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4401824304", "doi": "https://doi.org/10.3847/1538-4357/ad5954", "title": "Deeper, Sharper, Faster: Application of Efficient Transformer to Galaxy Image Restoration", "abstract": "Abstract The Transformer architecture has revolutionized the field of deep learning over the past several years in diverse areas, including natural language processing, code generation, image recognition, and time-series forecasting. We propose to apply Zamir et al.'s efficient transformer to perform deconvolution and denoising to enhance astronomical images. We conducted experiments using pairs of high-quality images and their degraded versions, and our deep learning model demonstrates exceptional restoration of photometric, structural, and morphological information. When compared with the ground-truth James Webb Space Telescope images, the enhanced versions of our Hubble Space Telescope–quality images reduce the scatter of isophotal photometry, Sérsic index, and half-light radius by factors of 4.4, 3.6, and 4.7, respectively, with Pearson correlation coefficients approaching unity. The performance is observed to degrade when input images exhibit correlated noise, point-like sources, and artifacts. We anticipate that this deep learning model will prove valuable for a number of scientific applications, including precision photometry, morphological analysis, and shear calibration.", "authors": ["Hyosun Park", "Yongsik Jo", "Seokun Kang", "T. Kim", "M. James Jee"], "year": 2024, "venue": "The Astrophysical Journal", "cited_by_count": 8, "type": "article", "concepts": ["Physics", "Galaxy", "Astronomy", "Astrophysics", "Transformer"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4295122555", "doi": "https://doi.org/10.3390/app12188972", "title": "Deep Residual Learning for Image Recognition: A Survey", "abstract": "Deep Residual Networks have recently been shown to significantly improve the performance of neural networks trained on ImageNet, with results beating all previous methods on this dataset by large margins in the image classification task. However, the meaning of these impressive numbers and their implications for future research are not fully understood yet. In this survey, we will try to explain what Deep Residual Networks are, how they achieve their excellent results, and why their successful implementation in practice represents a significant advance over existing techniques. We also discuss some open questions related to residual learning as well as possible applications of Deep Residual Networks beyond ImageNet. Finally, we discuss some issues that still need to be resolved before deep residual learning can be applied on more complex problems.", "authors": ["Muhammad Shafiq", "Zhaoquan Gu"], "year": 2022, "venue": "Applied Sciences", "cited_by_count": 838, "type": "article", "concepts": ["Residual", "Deep learning", "Computer science", "Artificial intelligence", "Deep neural networks"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4392884081", "doi": "https://doi.org/10.1016/j.jvcir.2024.104117", "title": "GoLDFormer: A global–local deformable window transformer for efficient image restoration", "abstract": "", "authors": ["Quan Chen", "Bolun Zheng", "Chenggang Yan", "Zunjie Zhu", "Tingyu Wang", "Greg Slabaugh", "Shanxin Yuan"], "year": 2024, "venue": "Journal of Visual Communication and Image Representation", "cited_by_count": 4, "type": "article", "concepts": ["Transformer", "Computer science", "Computation", "Window (computing)", "Computer vision"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4391944774", "doi": "https://doi.org/10.1007/s11042-024-18550-z", "title": "Underwater image enhancement using lightweight vision transformer", "abstract": "", "authors": ["Muneeba Daud", "Hammad Afzal", "Khawir Mahmood"], "year": 2024, "venue": "Multimedia Tools and Applications", "cited_by_count": 6, "type": "article", "concepts": ["Computer science", "Underwater", "Artificial intelligence", "Transformer", "Computer vision"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4382240242", "doi": "https://doi.org/10.1609/aaai.v37i3.25364", "title": "Ultra-High-Definition Low-Light Image Enhancement: A Benchmark and Transformer-Based Method", "abstract": "As the quality of optical sensors improves, there is a need for processing large-scale images. In particular, the ability of devices to capture ultra-high definition (UHD) images and video places new demands on the image processing pipeline. In this paper, we consider the task of low-light image enhancement (LLIE) and introduce a large-scale database consisting of images at 4K and 8K resolution. We conduct systematic benchmarking studies and provide a comparison of current LLIE algorithms. As a second contribution, we introduce LLFormer, a transformer-based low-light enhancement method. The core components of LLFormer are the axis-based multi-head self-attention and cross-layer attention fusion block, which significantly reduces the linear complexity. Extensive experiments on the new dataset and existing public datasets show that LLFormer outperforms state-of-the-art methods. We also show that employing existing LLIE methods trained on our benchmark as a pre-processing step significantly improves the performance of downstream tasks, e.g., face detection in low-light conditions. The source code and pre-trained models are available at https://github.com/TaoWangzj/LLFormer.", "authors": ["Tao Wang", "Kaihao Zhang", "Tianrun Shen", "Wenhan Luo", "Björn Stenger", "Tong Lu"], "year": 2023, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 359, "type": "article", "concepts": ["Computer science", "Benchmarking", "Benchmark (surveying)", "Artificial intelligence", "Transformer"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4366091323", "doi": "https://doi.org/10.1007/s10462-023-10466-8", "title": "Deep learning modelling techniques: current progress, applications, advantages, and challenges", "abstract": "Abstract Deep learning (DL) is revolutionizing evidence-based decision-making techniques that can be applied across various sectors. Specifically, it possesses the ability to utilize two or more levels of non-linear feature transformation of the given data via representation learning in order to overcome limitations posed by large datasets. As a multidisciplinary field that is still in its nascent phase, articles that survey DL architectures encompassing the full scope of the field are rather limited. Thus, this paper comprehensively reviews the state-of-art DL modelling techniques and provides insights into their advantages and challenges. It was found that many of the models exhibit a highly domain-specific efficiency and could be trained by two or more methods. However, training DL models can be very time-consuming, expensive, and requires huge samples for better accuracy. Since DL is also susceptible to deception and misclassification and tends to get stuck on local minima, improved optimization of parameters is required to create more robust models. Regardless, DL has already been leading to groundbreaking results in the healthcare, education, security, commercial, industrial, as well as government sectors. Some models, like the convolutional neural network (CNN), generative adversarial networks (GAN), recurrent neural network (RNN), recursive neural networks, and autoencoders, are frequently used, while the potential of other models remains widely unexplored. Pertinently, hybrid conventional DL architectures have the capacity to overcome the challenges experienced by conventional models. Considering that capsule architectures may dominate future DL models, this work aimed to compile information for stakeholders involved in the development and use of DL models in the contemporary world.", "authors": ["Shams Forruque Ahmed", "Md. Sakib Bin Alam", "Maruf Hassan", "Mahtabin Rodela Rozbu", "Taoseef Ishtiak", "Nazifa Rafa", "M. Mofijur", "A. B. M. Shawkat Ali", "Amir H. Gandomi"], "year": 2023, "venue": "Artificial Intelligence Review", "cited_by_count": 790, "type": "article", "concepts": ["Computer science", "Deep learning", "Artificial intelligence", "Machine learning", "Field (mathematics)"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4408456561", "doi": "https://doi.org/10.3390/s25061803", "title": "Adaptive Vectorial Restoration from Dynamic Speckle Patterns Through Biological Scattering Media Based on Deep Learning", "abstract": "Imaging technologies based on vector optical fields hold significant potential in the biomedical field, particularly for non-invasive scattering imaging of anisotropic biological tissues. However, the dynamic and anisotropic nature of biological tissues poses severe challenges to the propagation and reconstruction of vector optical fields due to light scattering. To address this, we propose a deep learning-based polarization-resolved restoration method aimed at achieving the efficient and accurate imaging reconstruction from speckle patterns generated after passing through anisotropic and dynamic time-varying biological scattering media. By innovatively leveraging the two orthogonal polarization components of vector optical fields, our approach significantly enhances the robustness of imaging reconstruction in dynamic and anisotropic biological scattering media, benefiting from the additional information dimension of vectorial optical fields and the powerful learning capacity of a deep neural network. For the first time, a hybrid network model is designed that integrates convolutional neural networks (CNN) with a Transformer architecture for capturing local and global features of a speckle image, enabling adaptive vectorial restoration of dynamically time-varying speckle patterns. The experimental results demonstrate that the model exhibits excellent robustness and generalization capabilities in reconstructing the two orthogonal polarization components from dynamic speckle patterns behind anisotropic biological media. This study not only provides an efficient solution for scattering imaging of dynamic anisotropic biological tissues but also advances the application of vector optical fields in dynamic scattering environments through the integration of deep learning and optical technologies.", "authors": ["Yu–Chen Chen", "Shixuan Mi", "Yaping Tian", "Xiaobo Hu", "Qiangqiang Yuan", "Khian‐Hooi Chew", "Rui‐Pin Chen"], "year": 2025, "venue": "Sensors", "cited_by_count": 5, "type": "article", "concepts": ["Speckle pattern", "Robustness (evolution)", "Computer science", "Artificial intelligence", "Scattering"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W2923477440", "doi": "https://doi.org/10.3390/inventions4010022", "title": "Internet of Things in Smart Grid: Architecture, Applications, Services, Key Technologies, and Challenges", "abstract": "Internet of Things (IoT) is a connection of people and things at any time, in any place, with anyone and anything, using any network and any service. Thus, IoT is a huge dynamic global network infrastructure of Internet-enabled entities with web services. One of the most important applications of IoT is the Smart Grid (SG). SG is a data communications network which is integrated with the power grid to collect and analyze data that are acquired from transmission lines, distribution substations, and consumers. In this paper, we talk about IoT and SG and their relationship. Some IoT architectures in SG, requirements for using IoT in SG, IoT applications and services in SG, and challenges and future work are discussed.", "authors": ["Alireza Ghasempour"], "year": 2019, "venue": "Inventions", "cited_by_count": 629, "type": "article", "concepts": ["Internet of Things", "Smart grid", "Computer science", "Key (lock)", "Architecture"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4392642845", "doi": "https://doi.org/10.1109/access.2024.3375360", "title": "Decomformer: Decompose Self-Attention of Transformer for Efficient Image Restoration", "abstract": "A transformer architecture achieves outstanding performance in computer vision tasks based on the ability to capture long-range dependencies. However, a quadratic increase in complexity with respect to spatial resolution makes it impractical to apply for image restoration tasks. In this paper, we propose a Decomformer that efficiently captures global relationship by decomposing self-attention into linear combination of vectors and coefficients to reduce the heavy computational cost. This approximation not only reduces the complexity linearly, but also preserves the globality of the vanilla self-attention properly. Moreover, we apply a linear simple gate to represent the complex self-attention mechanism as the proposed decomposition directly. To show the effectiveness of our approach, we apply it to image restoration tasks including denoising, deblurring and deraining. The proposed decomposing scheme for self-attention in the Transformer achieves better or comparable results with state-of-the-arts as well as much more efficiency than most of previous approaches.", "authors": ["Eunho Lee", "Youngbae Hwang"], "year": 2024, "venue": "IEEE Access", "cited_by_count": 3, "type": "article", "concepts": ["Deblurring", "Computer science", "Image restoration", "Transformer", "Globality"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4403754946", "doi": "https://doi.org/10.48550/arxiv.2409.13094", "title": "DenoMamba: A fused state-space model for low-dose CT denoising", "abstract": "Low-dose computed tomography (LDCT) lower potential risks linked to radiation exposure while relying on advanced denoising algorithms to maintain diagnostic quality in reconstructed images. The reigning paradigm in LDCT denoising is based on neural network models that learn data-driven image priors to separate noise evoked by dose reduction from underlying tissue signals. Naturally, the fidelity of these priors depend on the model's ability to capture the broad range of contextual features evident in CT images. Earlier convolutional neural networks (CNN) are highly adept at efficiently capturing short-range spatial context, but their limited receptive fields reduce sensitivity to interactions over longer distances. Although transformers based on self-attention mechanisms have recently been posed to increase sensitivity to long-range context, they can suffer from suboptimal performance and efficiency due to elevated model complexity, particularly for high-resolution CT images. For high-quality restoration of LDCT images, here we introduce DenoMamba, a novel denoising method based on state-space modeling (SSM), that efficiently captures short- and long-range context in medical images. Following an hourglass architecture with encoder-decoder stages, DenoMamba employs a spatial SSM module to encode spatial context and a novel channel SSM module equipped with a secondary gated convolution network to encode latent features of channel context at each stage. Feature maps from the two modules are then consolidated with low-level input features via a convolution fusion module (CFM). Comprehensive experiments on LDCT datasets with 25\\% and 10\\% dose reduction demonstrate that DenoMamba outperforms state-of-the-art denoisers with average improvements of 1.4dB PSNR, 1.1% SSIM, and 1.6% RMSE in recovered image quality.", "authors": ["Şaban Öztürk", "Özge Duran", "Tolga Çukur"], "year": 2024, "venue": "arXiv (Cornell University)", "cited_by_count": 7, "type": "preprint", "concepts": ["Noise reduction", "Space (punctuation)", "State space", "State (computer science)", "Nuclear medicine"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4410729663", "doi": "https://doi.org/10.1007/s00371-025-03947-0", "title": "An asymmetric calibrated transformer network for underwater image restoration", "abstract": "", "authors": ["Xiaojiao Guo", "Shenghong Luo", "Yihang Dong", "Zexiao Liang", "Zimeng Li", "Xiujun Zhang", "Xuhang Chen"], "year": 2025, "venue": "The Visual Computer", "cited_by_count": 2, "type": "article", "concepts": ["Underwater", "Transformer", "Computer graphics", "Computer science", "Artificial intelligence"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4367055910", "doi": "https://doi.org/10.1038/s41746-023-00811-0", "title": "Self-supervised learning for medical image classification: a systematic review and implementation guidelines", "abstract": "", "authors": ["Shih-Cheng Huang", "Anuj Pareek", "Malte Jensen", "Matthew P. Lungren", "Serena Yeung", "Akshay Chaudhari"], "year": 2023, "venue": "npj Digital Medicine", "cited_by_count": 356, "type": "review", "concepts": ["Artificial intelligence", "Computer science", "Machine learning", "Deep learning", "Supervised learning"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4406030314", "doi": "https://doi.org/10.1007/s11760-024-03798-7", "title": "Multi-scale representation for image deraining with state space model", "abstract": "", "authors": ["Yufeng Li", "Chuanlong Xie", "Hongming Chen"], "year": 2025, "venue": "Signal Image and Video Processing", "cited_by_count": 3, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Feature (linguistics)", "Convolutional neural network", "Block (permutation group theory)"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4404275081", "doi": "https://doi.org/10.2139/ssrn.5018208", "title": "Joint Multi-Dimensional Dynamic Attention and Transformer for General Image Restoration", "abstract": "", "authors": ["Huan Zhang", "Xu Zhang", "Nian Cai", "Jianglei Di", "Yun Zhang"], "year": 2024, "venue": "SSRN Electronic Journal", "cited_by_count": 2, "type": "preprint", "concepts": ["Joint (building)", "Transformer", "Image restoration", "Computer science", "Computer vision"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4283801824", "doi": "https://doi.org/10.1038/s41551-022-00898-y", "title": "Shifting machine learning for healthcare from development to deployment and from models to data", "abstract": "In the past decade, the application of machine learning (ML) to healthcare has helped drive the automation of physician tasks as well as enhancements in clinical capabilities and access to care. This progress has emphasized that, from model development to model deployment, data play central roles. In this Review, we provide a data-centric view of the innovations and challenges that are defining ML for healthcare. We discuss deep generative models and federated learning as strategies to augment datasets for improved model performance, as well as the use of the more recent transformer models for handling larger datasets and enhancing the modelling of clinical text. We also discuss data-focused problems in the deployment of ML, emphasizing the need to efficiently deliver data to ML models for timely clinical predictions and to account for natural data shifts that can deteriorate model performance.", "authors": ["Angela Zhang", "Lei Xing", "James Zou", "Joseph C. Wu"], "year": 2022, "venue": "Nature Biomedical Engineering", "cited_by_count": 373, "type": "review", "concepts": ["Software deployment", "Computer science", "Artificial intelligence", "Health care", "Machine learning"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4411899995", "doi": "https://doi.org/10.1007/s00521-025-11440-x", "title": "LDFormer: lightweight dehazing transformer", "abstract": "", "authors": ["D. Pushpalatha", "P. Prithvi"], "year": 2025, "venue": "Neural Computing and Applications", "cited_by_count": 3, "type": "article", "concepts": ["Computational Science and Engineering", "Computer science", "Transformer", "Machine learning", "Electrical engineering"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4382568119", "doi": "https://doi.org/10.2139/ssrn.4495224", "title": "Blind Face Restoration: Benchmark Datasets and a Baseline Model", "abstract": "", "authors": ["Puyang Zhang", "Kaihao Zhang", "Wenhan Luo", "Changsheng Li", "Guoren Wang"], "year": 2023, "venue": "SSRN Electronic Journal", "cited_by_count": 5, "type": "preprint", "concepts": ["Benchmark (surveying)", "Baseline (sea)", "Computer science", "Face (sociological concept)", "Artificial intelligence"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4205129187", "doi": "https://doi.org/10.1109/access.2022.3142859", "title": "Particle Swarm Optimization: A Comprehensive Survey", "abstract": "Particle swarm optimization (PSO) is one of the most well-regarded swarm-based algorithms in the literature. Although the original PSO has shown good optimization performance, it still severely suffers from premature convergence. As a result, many researchers have been modifying it resulting in a large number of PSO variants with either slightly or significantly better performance. Mainly, the standard PSO has been modified by four main strategies: modification of the PSO controlling parameters, hybridizing PSO with other well-known meta-heuristic algorithms such as genetic algorithm (GA) and differential evolution (DE), cooperation and multi-swarm techniques. This paper attempts to provide a comprehensive review of PSO, including the basic concepts of PSO, binary PSO, neighborhood topologies in PSO, recent and historical PSO variants, remarkable engineering applications of PSO, and its drawbacks. Moreover, this paper reviews recent studies that utilize PSO to solve feature selection problems. Finally, eight potential research directions that can help researchers further enhance the performance of PSO are provided.", "authors": ["Tareq M. Shami", "Ayman A. El‐Saleh", "Mohammed Alswaitti", "Qasem Al-Tashi", "Mhd Amen Summakieh", "Seyedali Mirjalili"], "year": 2022, "venue": "IEEE Access", "cited_by_count": 1158, "type": "article", "concepts": ["Particle swarm optimization", "Premature convergence", "Computer science", "Swarm behaviour", "Mathematical optimization"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4409019412", "doi": "https://doi.org/10.1109/jsen.2025.3554650", "title": "Weather-Robust Spatial-Frequency Decoupling Transformer for Crack Segmentation", "abstract": "Using edge devices such as drones for road inspections is critical for monitoring the structural integrity of transportation infrastructure. However, the limited resources of these devices constrain the deployment of high-performance models, while dynamic weather conditions require greater model stability. To address the challenge of efficient crack detection across various environments, we propose the spatial-frequency decoupling transformer (SFDFormer). Our model integrates an all-inone restoration module and a frequency decoupling module. The restoration module, combining AdverseGAN and stacked attention-integrated iterative recovery (AIIR) modules, restores images degraded by adverse weather. The frequency decoupling module employs a lightweight parallel network architecture to establish a correspondence between frequency-domain components and spatial-domain pixels, enabling direct extraction of crack edges and region information from the raw image. This approach effectively leverages semantic details from the frequency domain for pixel-level crack segmentation. Extensive experiments across six benchmark datasets demonstrate that SFDFormer consistently outperforms existing models, setting a new standard for crack detection on edge devices under harsh weather conditions.", "authors": ["Senyang Li", "Siqi Cai", "Siohong Teng", "Siwei Wei", "Jingling Yuan", "Xian Zhong"], "year": 2025, "venue": "IEEE Sensors Journal", "cited_by_count": 3, "type": "article", "concepts": ["Decoupling (probability)", "Transformer", "Segmentation", "Computer science", "Electronic engineering"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4285065952", "doi": "https://doi.org/10.1109/access.2022.3179517", "title": "A Review of Wavelet Analysis and Its Applications: Challenges and Opportunities", "abstract": "As a general and rigid mathematical tool, wavelet theory has found many applications and is constantly developing. This article reviews the development history of wavelet theory, from the construction method to the discussion of wavelet properties. Then it focuses on the design and expansion of wavelet transform. The main models and algorithms of wavelet transform are discussed. The construction of rational wavelet transform (RWT) is provided by examples emphasizing the advantages of RWT over traditional wavelet transform through a review of the literature. The combination of wavelet theory and neural networks is one of the key points of the review. The review covers the evolution of Wavelet Neural Network (WNN), the system architecture and algorithm implementation. The review of the literature indicates the advantages and a clear trend of fast development inWNNthat can be combined with existing neural network algorithms. This article also introduces the categories of wavelet-based applications. The advantages of wavelet analysis are summarized in terms of application scenarios with a comparison of results. Through the review, new research challenges and gaps have been clarified, which will serve as a guide for potential wavelet-based applications and new system designs.", "authors": ["Tiantian Guo", "Tongpo Zhang", "Eng Gee Lim", "Miguel López‐Benítez", "Fei Ma", "Limin Yu"], "year": 2022, "venue": "IEEE Access", "cited_by_count": 446, "type": "review", "concepts": ["Wavelet", "Wavelet transform", "Lifting scheme", "Computer science", "Wavelet packet decomposition"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4393150006", "doi": "https://doi.org/10.1609/aaai.v38i2.27907", "title": "Omni-Kernel Network for Image Restoration", "abstract": "Image restoration aims to reconstruct a high-quality image from a degraded low-quality observation. Recently, Transformer models have achieved promising performance on image restoration tasks due to their powerful ability to model long-range dependencies. However, the quadratically growing complexity with respect to the input size makes them inapplicable to practical applications. In this paper, we develop an efficient convolutional network for image restoration by enhancing multi-scale representation learning. To this end, we propose an omni-kernel module that consists of three branches, i.e., global, large, and local branches, to learn global-to-local feature representations efficiently. Specifically, the global branch achieves a global perceptive field via the dual-domain channel attention and frequency-gated mechanism. Furthermore, to provide multi-grained receptive fields, the large branch is formulated via different shapes of depth-wise convolutions with unusually large kernel sizes. Moreover, we complement local information using a point-wise depth-wise convolution. Finally, the proposed network, dubbed OKNet, is established by inserting the omni-kernel module into the bottleneck position for efficiency. Extensive experiments demonstrate that our network achieves state-of-the-art performance on 11 benchmark datasets for three representative image restoration tasks, including image dehazing, image desnowing, and image defocus deblurring. The code is available at https://github.com/c-yn/OKNet.", "authors": ["Yuning Cui", "Wenqi Ren", "Alois Knoll"], "year": 2024, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 183, "type": "article", "concepts": ["Image restoration", "Kernel (algebra)", "Computer science", "Artificial intelligence", "Image (mathematics)"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W4375827355", "doi": "https://doi.org/10.1007/978-3-031-31417-9_40", "title": "A Transformer-Based U-Net Architecture for Fast and Efficient Image Demoireing", "abstract": "", "authors": ["Densen Puthussery", "P. S. Hrishikesh", "C. V. Jiji"], "year": 2023, "venue": "Communications in computer and information science", "cited_by_count": 1, "type": "book-chapter", "concepts": ["Deblurring", "Computer science", "Transformer", "Architecture", "Artificial intelligence"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W2997150500", "doi": "https://doi.org/10.1609/aaai.v34i07.6693", "title": "Channel Attention Is All You Need for Video Frame Interpolation", "abstract": "Prevailing video frame interpolation techniques rely heavily on optical flow estimation and require additional model complexity and computational cost; it is also susceptible to error propagation in challenging scenarios with large motion and heavy occlusion. To alleviate the limitation, we propose a simple but effective deep neural network for video frame interpolation, which is end-to-end trainable and is free from a motion estimation network component. Our algorithm employs a special feature reshaping operation, referred to as PixelShuffle, with a channel attention, which replaces the optical flow computation module. The main idea behind the design is to distribute the information in a feature map into multiple channels and extract motion information by attending the channels for pixel-level frame synthesis. The model given by this principle turns out to be effective in the presence of challenging motion and occlusion. We construct a comprehensive evaluation benchmark and demonstrate that the proposed approach achieves outstanding performance compared to the existing models with a component for optical flow computation.", "authors": ["Myungsub Choi", "Heewon Kim", "Bohyung Han", "Ning Xu", "Kyoung Mu Lee"], "year": 2020, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 325, "type": "article", "concepts": ["Computer science", "Optical flow", "Interpolation (computer graphics)", "Channel (broadcasting)", "Motion interpolation"], "search_query": "image restoration transformer architecture efficient"}
{"openalex_id": "https://openalex.org/W3133049266", "doi": "https://doi.org/10.1109/jstsp.2021.3053364", "title": "Editorial: Introduction to the Issue on Deep Learning for Image/Video Restoration and Compression", "abstract": "The papers in this special issue focus on deep learning for image/video restoration and compression. The huge success of deep-learning-based approaches in computer vision has inspired research in learned solutions to classic image/video processing problems, such as denoising, deblurring, dehazing, deraining, super-resolution (SR), and compression. Hence, learning-based methods have emerged as a promising nonlinear signal-processing framework for image/ video restoration and compression. Recent works have shown that learned models can achieve significant performance gains, especially in terms of perceptual quality measures, over traditional methods. Hence, the state of the art in image restoration and compression is getting redefined. This special issue covers the state of the art in learned image/video restoration and compression to promote further progress in innovative architectures and training methods for effective and efficient networks for image/video restoration and compression.", "authors": ["A. Murat Tekalp", "Michele Covell", "Radu Timofte", "Chao Dong"], "year": 2021, "venue": "IEEE Journal of Selected Topics in Signal Processing", "cited_by_count": 7, "type": "editorial", "concepts": ["Deblurring", "Image restoration", "Computer science", "Compression artifact", "Artificial intelligence"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4389599809", "doi": "https://doi.org/10.48550/arxiv.2312.05038", "title": "Prompt-In-Prompt Learning for Universal Image Restoration", "abstract": "Image restoration, which aims to retrieve and enhance degraded images, is fundamental across a wide range of applications. While conventional deep learning approaches have notably improved the image quality across various tasks, they still suffer from (i) the high storage cost needed for various task-specific models and (ii) the lack of interactivity and flexibility, hindering their wider application. Drawing inspiration from the pronounced success of prompts in both linguistic and visual domains, we propose novel Prompt-In-Prompt learning for universal image restoration, named PIP. First, we present two novel prompts, a degradation-aware prompt to encode high-level degradation knowledge and a basic restoration prompt to provide essential low-level information. Second, we devise a novel prompt-to-prompt interaction module to fuse these two prompts into a universal restoration prompt. Third, we introduce a selective prompt-to-feature interaction module to modulate the degradation-related feature. By doing so, the resultant PIP works as a plug-and-play module to enhance existing restoration models for universal image restoration. Extensive experimental results demonstrate the superior performance of PIP on multiple restoration tasks, including image denoising, deraining, dehazing, deblurring, and low-light enhancement. Remarkably, PIP is interpretable, flexible, efficient, and easy-to-use, showing promising potential for real-world applications. The code is available at https://github.com/longzilicart/pip_universal.", "authors": ["Zilong Li", "Yiming Lei", "Chenglong Ma", "Junping Zhang", "Hongming Shan"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 6, "type": "preprint", "concepts": ["Deblurring", "Computer science", "Image restoration", "Flexibility (engineering)", "Feature (linguistics)"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4385667669", "doi": "https://doi.org/10.1007/s11263-023-01843-5", "title": "Pyramid Attention Network for Image Restoration", "abstract": "", "authors": ["Yiqun Mei", "Yuchen Fan", "Yulun Zhang", "Jiahui Yu", "Yuqian Zhou", "Ding Liu", "Yun Fu", "Thomas S. Huang", "Humphrey Shi"], "year": 2023, "venue": "International Journal of Computer Vision", "cited_by_count": 112, "type": "article", "concepts": ["Artificial intelligence", "Computer science", "Pyramid (geometry)", "Computer vision", "Image restoration"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4323022514", "doi": "https://doi.org/10.1109/access.2023.3250616", "title": "A Comprehensive Review of Deep Learning-Based Real-World Image Restoration", "abstract": "Real-world imagery does not always exhibit good visibility and clean content, but often suffers from various kinds of degradations (e.g., noise, blur, rain drops, fog, color distortion, etc.), which severely affect vision-driven tasks (e.g., image classification, target recognition, and tracking, etc.). Thus, restoring the true scene from such degraded images is of significance. In recent years, a large body of deep learning-based image processing works has been exploited due to the advances in deep neural networks. This paper aims to make a comprehensive review of real-world image restoration algorithms and beyond. More specifically, this review provides overviews of critical benchmark datasets, image quality assessment methods, and four major categories of deep learning-based image restoration methods, i.e., based on convolutional neural network (CNN), generative adversarial network (GAN), Transformer, and multi-layer perceptron (MLP). The paper highlights the latest developments and advances in each category of network architecture to provide an up-to-date overview. Moreover, the representative state-of-the-art image restoration methods are compared visually and numerically. Finally, for real-world image restoration, the current situations are objectively assessed, challenges are discussed, and future directions and trends are presented.", "authors": ["Lujun Zhai", "Yonghui Wang", "Suxia Cui", "Yu Zhou"], "year": 2023, "venue": "IEEE Access", "cited_by_count": 62, "type": "review", "concepts": ["Computer science", "Image restoration", "Artificial intelligence", "Convolutional neural network", "Deep learning"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4400238700", "doi": "https://doi.org/10.3390/app14135683", "title": "A Survey on Visual Mamba", "abstract": "State space models (SSM) with selection mechanisms and hardware-aware architectures, namely Mamba, have recently shown significant potential in long-sequence modeling. Since the complexity of transformers’ self-attention mechanism is quadratic with image size, as well as increasing computational demands, researchers are currently exploring how to adapt Mamba for computer vision tasks. This paper is the first comprehensive survey that aims to provide an in-depth analysis of Mamba models within the domain of computer vision. It begins by exploring the foundational concepts contributing to Mamba’s success, including the SSM framework, selection mechanisms, and hardware-aware design. Then, we review these vision Mamba models by categorizing them into foundational models and those enhanced with techniques including convolution, recurrence, and attention to improve their sophistication. Furthermore, we investigate the widespread applications of Mamba in vision tasks, which include their use as a backbone in various levels of vision processing. This encompasses general visual tasks, medical visual tasks (e.g., 2D/3D segmentation, classification, image registration, etc.), and remote sensing visual tasks. In particular, we introduce general visual tasks from two levels: high/mid-level vision (e.g., object detection, segmentation, video classification, etc.) and low-level vision (e.g., image super-resolution, image restoration, visual generation, etc.). We hope this endeavor will spark additional interest within the community to address current challenges and further apply Mamba models in computer vision.", "authors": ["Hanwei Zhang", "Ying Zhu", "Dan Wang", "Lijun Zhang", "Tianxiang Chen", "Ziyang Wang", "Zi Ye"], "year": 2024, "venue": "Applied Sciences", "cited_by_count": 132, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Sophistication", "Computer vision", "Segmentation"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4221160948", "doi": "https://doi.org/10.1109/jbhi.2022.3187103", "title": "RFormer: Transformer-Based Generative Adversarial Network for Real Fundus Image Restoration on a New Clinical Benchmark", "abstract": "Ophthalmologists have used fundus images to screen and diagnose eye diseases. However, different equipments and ophthalmologists pose large variations to the quality of fundus images. Low-quality (LQ) degraded fundus images easily lead to uncertainty in clinical screening and generally increase the risk of misdiagnosis. Thus, real fundus image restoration is worth studying. Unfortunately, real clinical benchmark has not been explored for this task so far. In this paper, we investigate the real clinical fundus image restoration problem. Firstly, We establish a clinical dataset, Real Fundus (RF), including 120 low- and high-quality (HQ) image pairs. Then we propose a novel Transformer-based Generative Adversarial Network (RFormer) to restore the real degradation of clinical fundus images. The key component in our network is the Window-based Self-Attention Block (WSAB) which captures non-local self-similarity and long-range dependencies. To produce more visually pleasant results, a Transformer-based discriminator is introduced. Extensive experiments on our clinical benchmark show that the proposed RFormer significantly outperforms the state-of-the-art (SOTA) methods. In addition, experiments of downstream tasks such as vessel segmentation and optic disc/cup detection demonstrate that our proposed RFormer benefits clinical fundus image analysis and applications.", "authors": ["Zhuo Deng", "Yuanhao Cai", "Lu Chen", "Zheng Gong", "Qiqi Bao", "Xue Yao", "Fang Dong", "Wenming Yang", "Shaochong Zhang", "Lan Ma"], "year": 2022, "venue": "IEEE Journal of Biomedical and Health Informatics", "cited_by_count": 76, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Discriminator", "Fundus (uterus)", "Benchmark (surveying)"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W3154672751", "doi": "https://doi.org/10.1155/2021/5541134", "title": "Deep CNN and Deep GAN in Computational Visual Perception‐Driven Image Analysis", "abstract": "Computational visual perception, also known as computer vision, is a field of artificial intelligence that enables computers to process digital images and videos in a similar way as biological vision does. It involves methods to be developed to replicate the capabilities of biological vision. The computer vision’s goal is to surpass the capabilities of biological vision in extracting useful information from visual data. The massive data generated today is one of the driving factors for the tremendous growth of computer vision. This survey incorporates an overview of existing applications of deep learning in computational visual perception. The survey explores various deep learning techniques adapted to solve computer vision problems using deep convolutional neural networks and deep generative adversarial networks. The pitfalls of deep learning and their solutions are briefly discussed. The solutions discussed were dropout and augmentation. The results show that there is a significant improvement in the accuracy using dropout and data augmentation. Deep convolutional neural networks’ applications, namely, image classification, localization and detection, document analysis, and speech recognition, are discussed in detail. In‐depth analysis of deep generative adversarial network applications, namely, image‐to‐image translation, image denoising, face aging, and facial attribute editing, is done. The deep generative adversarial network is unsupervised learning, but adding a certain number of labels in practical applications can improve its generating ability. However, it is challenging to acquire many data labels, but a small number of data labels can be acquired. Therefore, combining semisupervised learning and generative adversarial networks is one of the future directions. This article surveys the recent developments in this direction and provides a critical review of the related significant aspects, investigates the current opportunities and future challenges in all the emerging domains, and discusses the current opportunities in many emerging fields such as handwriting recognition, semantic mapping, webcam‐based eye trackers, lumen center detection, query‐by‐string word, intermittently closed and open lakes and lagoons, and landslides.", "authors": ["R. Abirami", "P. M. Durai Raj Vincent", "Kathiravan Srinivasan", "Usman Tariq", "Chuan‐Yu Chang"], "year": 2021, "venue": "Complexity", "cited_by_count": 77, "type": "article", "concepts": ["Artificial intelligence", "Computer science", "Image (mathematics)", "Perception", "Deep learning"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4283820866", "doi": "https://doi.org/10.1609/aaai.v36i2.20033", "title": "Close the Loop: A Unified Bottom-Up and Top-Down Paradigm for Joint Image Deraining and Segmentation", "abstract": "In this work, we focus on a very practical problem: image segmentation under rain conditions. Image deraining is a classic low-level restoration task, while image segmentation is a typical high-level understanding task. Most of the existing methods intuitively employ the bottom-up paradigm by taking deraining as a preprocessing step for subsequent segmentation. However, our statistical analysis indicates that not only deraining would benefit segmentation (bottom-up), but also segmentation would further improve deraining performance (top-down) in turn. This motivates us to solve the rainy image segmentation task within a novel top-down and bottom-up unified paradigm, in which two sub-tasks are alternatively performed and collaborated with each other. Specifically, the bottom-up procedure yields both clearer images and rain-robust features from both image and feature domains, so as to ease the segmentation ambiguity caused by rain streaks. The top-down procedure adopts semantics to adaptively guide the restoration for different contents via a novel multi-path semantic attentive module (SAM). Thus the deraining and segmentation could boost the performance of each other cooperatively and progressively. Extensive experiments and ablations demonstrate that the proposed method outperforms the state-of-the-art on rainy image segmentation.", "authors": ["Yi Li", "Yi Chang", "Chang‐Feng Yu", "Luxin Yan"], "year": 2022, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 28, "type": "article", "concepts": ["Segmentation", "Computer science", "Artificial intelligence", "Task (project management)", "Scale-space segmentation"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W3033307992", "doi": "https://doi.org/10.1109/access.2020.2999750", "title": "MSGAN: Generative Adversarial Networks for Image Seasonal Style Transfer", "abstract": "Although Generative Adversarial Networks (GANs) have shown remarkable successes in various computer vision tasks, they still face challenges in image season style transfer task. In this paper, we propose a multi-season Generative Adversarial Networks (MSGANs) aimed to transfer input images into other season styles. To improve the quality of the simulated images generated by the proposed MSGAN, we propose a novel loss function to guide the optimization direction of the network. Besides, we adopt the saliency information to guide the seasonal style transformation task, so as to ensure that different image contents can have different optimization weights in MSGAN. The experimental results show that the proposed MSGAN can generate high-quality simulated images from real images, and is superior to other latest methods. Not only that, the synthetic image generated by the proposed method also be used to perform depth estimation task so that prove that the synthetic images can be well applied to other computer vision tasks.", "authors": ["Fuquan Zhang", "Chuansheng Wang"], "year": 2020, "venue": "IEEE Access", "cited_by_count": 24, "type": "article", "concepts": ["Computer science", "Task (project management)", "Artificial intelligence", "Adversarial system", "Image (mathematics)"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4393153879", "doi": "https://doi.org/10.1609/aaai.v38i6.28412", "title": "Learning from History: Task-agnostic Model Contrastive Learning for Image Restoration", "abstract": "Contrastive learning has emerged as a prevailing paradigm for high-level vision tasks, which, by introducing properly negative samples, has also been exploited for low-level vision tasks to achieve a compact optimization space to account for their ill-posed nature. However, existing methods rely on manually predefined and task-oriented negatives, which often exhibit pronounced task-specific biases. To address this challenge, our paper introduces an innovative method termed 'learning from history', which dynamically generates negative samples from the target model itself. Our approach, named Model Contrastive Learning for Image Restoration (MCLIR), rejuvenates latency models as negative models, making it compatible with diverse image restoration tasks. We propose the Self-Prior guided Negative loss (SPN) to enable it. This approach significantly enhances existing models when retrained with the proposed model contrastive paradigm. The results show significant improvements in image restoration across various tasks and architectures. For example, models retrained with SPN outperform the original FFANet and DehazeFormer by 3.41 and 0.57 dB on the RESIDE indoor dataset for image dehazing. Similarly, they achieve notable improvements of 0.47 dB on SPA-Data over IDT for image deraining and 0.12 dB on Manga109 for a 4x scale super-resolution over lightweight SwinIR, respectively. Code and retrained models are available at https://github.com/Aitical/MCLIR.", "authors": ["Gang Wu", "Junjun Jiang", "Kui Jiang", "Xianming Liu"], "year": 2024, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 11, "type": "article", "concepts": ["Task (project management)", "Computer science", "Artificial intelligence", "Image (mathematics)", "Image restoration"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W3043357501", "doi": "https://doi.org/10.1109/access.2020.3008324", "title": "FastDerainNet: A Deep Learning Algorithm for Single Image Deraining", "abstract": "Existing neural network-based methods for de-raining single images exhibit dissatisfactory results owing to the inefficient propagation of features when objects with sizes and shapes similar to those of rain streaks are present in images. Furthermore, existing methods do not consider that the abundant information included in rain streaked images could interfere with the training process. To overcome these limitations, in this paper, we propose a deep residual learning algorithm called FastDerainNet for removing rain streaks from single images. We design a deep convolutional neural network architecture, based on a deep residual network called the share-source residual module (SSRM), by substituting the origins of all shortcut connections for one point. To further improve the de-raining performance, we adopt the SSRM as the parameter layers in FastDerainNet and use image decomposition to modify the loss function. Finally, we train FastDerainNet on a synthetic dataset. By learning the residual mapping between rainy and clean image detail layers, it is able to reduce the mapping range and simplify the training process. Experiments on both synthetic and real-world images demonstrate that the proposed method achieves increased performance with regard to de-raining, in addition to preserving original details, in comparison with other state-of-the-art methods.", "authors": ["Xiuwen Wang", "Zhiwei Li", "Hongtao Shan", "Zhiyuan Tian", "Yuanhong Ren", "Wuneng Zhou"], "year": 2020, "venue": "IEEE Access", "cited_by_count": 17, "type": "article", "concepts": ["Computer science", "Residual", "Convolutional neural network", "Deep learning", "Artificial intelligence"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W2971374671", "doi": "https://doi.org/10.1109/access.2019.2939578", "title": "Color Filter Array Demosaicking Using Densely Connected Residual Network", "abstract": "Deep convolutional neural networks have been used extensively in recent image processing research, exhibiting drastically improved performance. In this study, we apply convolutional neural networks to color filter array demosaicking, which plays an essential role in single-sensor digital cameras. Contrary to conventional convolutional neural network-based demosaicking models, the proposed model does not require any initial interpolation step for mosaicked input images, which increases the computational complexity. Using a mosaicked image as input, the proposed model is trained in an end-to-end manner to generate demosaicked images outputs. Many deep neural networks experience vanishing-gradient problem, which makes models hard to be trained. To solve this problem, we apply residual learning and densely connected convolutional neural network. Moreover, we apply block-wise convolutional neural networks to consider local features. Finally, we apply a sub-pixel interpolation layer to generate demosaicked output images more efficiently and accurately. Experimental results show that our proposed model outperforms conventional solutions and state-of-the-art models.", "authors": ["Bumjun Park", "Jechang Jeong"], "year": 2019, "venue": "IEEE Access", "cited_by_count": 16, "type": "article", "concepts": ["Demosaicing", "Convolutional neural network", "Computer science", "Artificial intelligence", "Residual"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W2949349317", "doi": "https://doi.org/10.48550/arxiv.1609.07769", "title": "Deep Joint Rain Detection and Removal from a Single Image", "abstract": "In this paper, we address a rain removal problem from a single image, even in the presence of heavy rain and rain streak accumulation. Our core ideas lie in the new rain image models and a novel deep learning architecture. We first modify an existing model comprising a rain streak layer and a background layer, by adding a binary map that locates rain streak regions. Second, we create a new model consisting of a component representing rain streak accumulation (where individual streaks cannot be seen, and thus visually similar to mist or fog), and another component representing various shapes and directions of overlapping rain streaks, which usually happen in heavy rain. Based on the first model, we develop a multi-task deep learning architecture that learns the binary rain streak map, the appearance of rain streaks, and the clean background, which is our ultimate output. The additional binary map is critically beneficial, since its loss function can provide additional strong information to the network. To handle rain streak accumulation (again, a phenomenon visually similar to mist or fog) and various shapes and directions of overlapping rain streaks, we propose a recurrent rain detection and removal network that removes rain streaks and clears up the rain accumulation iteratively and progressively. In each recurrence of our method, a new contextualized dilated network is developed to exploit regional contextual information and outputs better representation for rain detection. The evaluation on real images, particularly on heavy rain, shows the effectiveness of our novel models and architecture, outperforming the state-of-the-art methods significantly. Our codes and data sets will be publicly available.", "authors": ["Wenhan Yang", "Robby T. Tan", "Jiashi Feng", "Jiaying Liu", "Zongming Guo", "Shuicheng Yan"], "year": 2016, "venue": "arXiv (Cornell University)", "cited_by_count": 10, "type": "preprint", "concepts": ["Streak", "Computer science", "Mist", "Binary number", "Image (mathematics)"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4321594307", "doi": "https://doi.org/10.48550/arxiv.2302.09554", "title": "Mixed Hierarchy Network for Image Restoration", "abstract": "Image restoration is a long-standing low-level vision problem, e.g., deblurring and deraining. In the process of image restoration, it is necessary to consider not only the spatial details and contextual information of restoration to ensure the quality, but also the system complexity. Although many methods have been able to guarantee the quality of image restoration, the system complexity of the state-of-the-art (SOTA) methods is increasing as well. Motivated by this, we present a mixed hierarchy network that can balance these competing goals. Our main proposal is a mixed hierarchy architecture, that progressively recovers contextual information and spatial details from degraded images while we design intra-blocks to reduce system complexity. Specifically, our model first learns the contextual information using encoder-decoder architectures, and then combines them with high-resolution branches that preserve spatial detail. In order to reduce the system complexity of this architecture for convenient analysis and comparison, we replace or remove the nonlinear activation function with multiplication and use a simple network structure. In addition, we replace spatial convolution with global self-attention for the middle block of encoder-decoder. The resulting tightly interlinked hierarchy architecture, named as MHNet, delivers strong performance gains on several image restoration tasks, including image deraining, and deblurring.", "authors": ["Hu Gao", "Depeng Dang"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 7, "type": "preprint", "concepts": ["Deblurring", "Computer science", "Image restoration", "Hierarchy", "Image (mathematics)"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4361208495", "doi": "https://doi.org/10.3389/fpls.2023.1154176", "title": "All-in-one aerial image enhancement network for forest scenes", "abstract": "Drone monitoring plays an irreplaceable and significant role in forest firefighting due to its characteristics of wide-range observation and real-time messaging. However, aerial images are often susceptible to different degradation problems before performing high-level visual tasks including but not limited to smoke detection, fire classification, and regional localization. Recently, the majority of image enhancement methods are centered around particular types of degradation, necessitating the memory unit to accommodate different models for distinct scenarios in practical applications. Furthermore, such a paradigm requires wasted computational and storage resources to determine the type of degradation, making it difficult to meet the real-time and lightweight requirements of real-world scenarios. In this paper, we propose an All-in-one Image Enhancement Network (AIENet) that can restore various degraded images in one network. Specifically, we design a new multi-scale receptive field image enhancement block, which can better reconstruct high-resolution details of target regions of different sizes. In particular, this plug-and-play module enables it to be embedded in any learning-based model. And it has better flexibility and generalization in practical applications. This paper takes three challenging image enhancement tasks encountered in drone monitoring as examples, whereby we conduct task-specific and all-in-one image enhancement experiments on a synthetic forest dataset. The results show that the proposed AIENet outperforms the state-of-the-art image enhancement algorithms quantitatively and qualitatively. Furthermore, extra experiments on high-level vision detection also show the promising performance of our method compared with some recent baselines.", "authors": ["Zhaoqi Chen", "Chuansheng Wang", "Fuquan Zhang", "Ling Zhang", "Antoni Grau", "Edmundo Guerra"], "year": 2023, "venue": "Frontiers in Plant Science", "cited_by_count": 9, "type": "article", "concepts": ["Computer science", "Drone", "Aerial image", "Block (permutation group theory)", "Artificial intelligence"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4387490618", "doi": "https://doi.org/10.1109/tcsvt.2023.3323483", "title": "Joint Pixel and Frequency Feature Learning and Fusion via Channel-Wise Transformer for High-Efficiency Learned In-Loop Filter in VVC", "abstract": "Block-based video codecs such as Versatile Video Coding (VVC)/H.266, High Efficiency Video Coding (HEVC)/H.265, Advanced Video Coding (AVC)/H.264 etc. inherently introduces compression artifacts. Although these codecs have in-loop filters to correct these distortions, they are not always effective due to the complexity of the noise. Recently, deep-learning approaches emerged as a promising solution for in-loop filtering. However, most of the previous approaches were designed solely for learning from images and neglected the high-frequency signals present in the reconstructed video frames. Furthermore, some previous methods employed a multi-level feature-extraction and feature-fusion strategy to enhance performance. However, they utilized complex feature-extractors while relying on naive feature-fusion methods. In this article, we propose a novel framework called <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TSF-Net</i> , which jointly learns from both the pixel (spatial) and frequency-decomposed information and through powerful capability of a channel-wise transformer, it fuses both these information to improve performance. Our approach deviates from previous approaches by employing a simple feature-extractor coupled with an advanced transformer-based feature-fusion module. Simultaneously, <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TSF-Net</i> introduces a few fundamental modifications in the multi-head self-attention module of the channel-wise transformer to make it computationally efficient. Our experimental results show that the proposed <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TSF-Net</i> achieves a Bjøntegaard Delta (BD) - bitrate saving of up to 10.258% for the luma (Y) component under all-intra (AI) profile outperforming the VVC baseline and other state-of-the-art methods. Moreover, the proposed <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TSF-Net</i> with an efficient channel-wise transformer is twice as efficient as <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TSF-Net</i> with a vanilla channel-wise transformer.", "authors": ["Birendra Kathariya", "Zhu Li", "Geert Van der Auwera"], "year": 2023, "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "cited_by_count": 20, "type": "article", "concepts": ["Computer science", "Feature extraction", "Artificial intelligence", "Feature learning", "Transformer"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4320713067", "doi": "https://doi.org/10.1109/access.2023.3245150", "title": "Blind Text Image Deblurring Algorithm Based on Multi-Scale Fusion and Sparse Priors", "abstract": "The goal of blind text image deblurring is to obtain a clean text image from the given blurry text image without knowing the blur kernel. Sparsity-based methods have been shown their effectiveness in various blind text image deblurring models. However, the blur kernel estimation methods based on sparse priors lack of the consideration for the brightness information about the blur kernel, which will affect the restoration effect of the blur kernel. Besides, previous methods seldom apply sparse priors to both spatial domain and transform domain information. We propose a novel blind text image deblurring model based on multi-scale fusion and sparse priors. Besides the sparse gradient prior on the latent clean text image, we add the sparse prior on the high-frequency wavelet coefficients of the latent text image, which will better constrain the solution space and obtain good clean images. The semi-quadratic splitting method is used to alternately optimize the blur kernel and the latent clean image. Meanwhile, we consider the influence of the brightness feature of the restored blur kernel. By multi-scale fusion technique on the basis of Laplacian weight and saliency weight, we fuse the computed blur kernels in three channels to improve the quality of blur kernel. The experimental results show that our algorithm has good results in the restoration of blur kernels and text images.", "authors": ["Zhe Li", "Ming Yang", "Libo Cheng", "Xiaoning Jia"], "year": 2023, "venue": "IEEE Access", "cited_by_count": 10, "type": "article", "concepts": ["Deblurring", "Computer science", "Artificial intelligence", "Image fusion", "Image (mathematics)"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W3189012752", "doi": "https://doi.org/10.1109/access.2021.3101319", "title": "A Model-Driven Deep Dehazing Approach by Learning Deep Priors", "abstract": "Photos taken in hazy weather are usually covered with white masks and lose important details. Haze removal is a fundamental task and a prerequisite to many other vision tasks. Single image dehazing is an ill-posed inverse problem that has attracted much attention in recent years. Generally, current single dehazing methods can be categorized into the traditional prior-based methods and the data-driven deep learning methods that respectively investigate haze-related image priors and deep architectures. In this paper, we propose a novel model-driven deep learning approach that combines the advantages of both kinds of methods. First, we build an energy model for single image dehazing with physical constraints in both color image space and haze-related feature space (implemented as dark channel space in this work), regularized by haze-related image priors. Then, we design an iterative optimization algorithm for solving the proposed dehazing energy model based on the half-quadratic splitting algorithm, and the priors are transformed to their corresponding proximal operators. Finally, inspired by the optimization algorithm, we design a deep dehazing neural network, dubbed as proximal dehaze-net, by learning the proximal operators for haze-related image priors using CNNs. Our network incorporates physical model constraints of hazes and haze-related prior learning into a novel deep architecture. Extensive experiments show that our method achieves promising performance for single image dehazing.", "authors": ["Dong Yang", "Jian Sun"], "year": 2021, "venue": "IEEE Access", "cited_by_count": 14, "type": "article", "concepts": ["Prior probability", "Computer science", "Deep learning", "Artificial intelligence", "Inpainting"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4382814212", "doi": "https://doi.org/10.1155/2023/4506331", "title": "Two‐Step Unsupervised Approach for Sand‐Dust Image Enhancement", "abstract": "In sand‐dust environments, light is scattered and absorbed, and sand‐dust images thus suffer from severe image degradation problems, such as color shifts, low contrast, and blurred details. To address these problems, we propose a two‐step unsupervised sand‐dust image enhancement algorithm. In the first step, a convenient and competent color correction method is put forward to solve the color shift problem. Considering the wavelength attenuation features of sand‐dust images, a linear stretching and blue channel compensation method is designed, and an adaptive color shift correction factor is developed to remove the color shift. In the second step, to enhance the clarity and details of the images, an unsupervised generative adversarial network is proposed, which does not require pairs of data for training. To reduce detail loss, the detail enhancement branch is designed, and the generator considers to more details through the constructed coarse‐grained and fine‐grained discriminators. The introduced multiscale perceptual loss promotes the image fidelity well. Experiments show that the proposed method achieves better color correction, enhances image details and clarity, has a better subjective effect, and outperforms existing sand‐dust image enhancement methods both quantitatively and qualitatively. Similarly, our method promotes the application capability of the target detection algorithm and also has a good enhancement effect on underwater images and haze images.", "authors": ["Guxue Gao", "Huicheng Lai", "Zhenhong Jia"], "year": 2023, "venue": "International Journal of Intelligent Systems", "cited_by_count": 9, "type": "article", "concepts": ["Computer science", "Color balance", "Artificial intelligence", "Image (mathematics)", "Color correction"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4212954470", "doi": "https://doi.org/10.20944/preprints202202.0159.v1", "title": "Generative Adversarial Unsupervised Image Restoration in Hybrid Degradation Scenes", "abstract": "In this paper, we propose an unsupervised blind restoration model for images in hybrid degradation scenes. The proposed model encodes the content information and degradation information of images and then uses the attention module to disentangle the two kinds of information. It can improve the ability of disentangled presentation learning for a generative adversarial network (GAN) to restore the images in hybrid degradation scenes, enhance the detailed features of restored image and remove the artifact combining the adversarial loss, cycle-consistency loss, and perception loss. The experimental results on the DIV2K dataset and medical images show that the proposed method outperforms existing unsupervised image restoration algorithms in terms of peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and subjective visual evaluation.", "authors": ["Fan Tang", "Xinyu Zhu", "Jinrong Hu", "Juhong Tie", "Jiliu Zhou", "Ying Fu"], "year": 2022, "venue": "Preprints.org", "cited_by_count": 6, "type": "preprint", "concepts": ["Artificial intelligence", "Computer science", "Image restoration", "Degradation (telecommunications)", "Generative grammar"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4379255977", "doi": "https://doi.org/10.48550/arxiv.2306.00306", "title": "Low-Light Image Enhancement with Wavelet-based Diffusion Models", "abstract": "Diffusion models have achieved promising results in image restoration tasks, yet suffer from time-consuming, excessive computational resource consumption, and unstable restoration. To address these issues, we propose a robust and efficient Diffusion-based Low-Light image enhancement approach, dubbed DiffLL. Specifically, we present a wavelet-based conditional diffusion model (WCDM) that leverages the generative power of diffusion models to produce results with satisfactory perceptual fidelity. Additionally, it also takes advantage of the strengths of wavelet transformation to greatly accelerate inference and reduce computational resource usage without sacrificing information. To avoid chaotic content and diversity, we perform both forward diffusion and denoising in the training phase of WCDM, enabling the model to achieve stable denoising and reduce randomness during inference. Moreover, we further design a high-frequency restoration module (HFRM) that utilizes the vertical and horizontal details of the image to complement the diagonal information for better fine-grained restoration. Extensive experiments on publicly available real-world benchmarks demonstrate that our method outperforms the existing state-of-the-art methods both quantitatively and visually, and it achieves remarkable improvements in efficiency compared to previous diffusion-based methods. In addition, we empirically show that the application for low-light face detection also reveals the latent practical values of our method. Code is available at https://github.com/JianghaiSCU/Diffusion-Low-Light.", "authors": ["Hai Jiang", "Ao Luo", "Songchen Han", "Haoqiang Fan", "Shuaicheng Liu"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 16, "type": "preprint", "concepts": ["Computer science", "Wavelet", "Artificial intelligence", "Inference", "Image restoration"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4409366360", "doi": "https://doi.org/10.1609/aaai.v39i8.32905", "title": "Debiased All-in-one Image Restoration with Task Uncertainty Regularization", "abstract": "All-in-one image restoration is a fundamental low-level vision task with significant real-world applications. The primary challenge lies in addressing diverse degradations within a single model. While current methods primarily exploit task prior information to guide the restoration models, they typically employ uniform multi-task learning, overlooking the heterogeneity in model optimization across different degradation tasks. To eliminate the bias, we propose a task-aware optimization strategy, that introduces adaptive task-specific regularization for multi-task image restoration learning. Specifically, our method dynamically weights and balances losses for different restoration tasks during training, encouraging the implementation of the most reasonable optimization route. In this way, we can achieve more robust and effective model training. Notably, our approach can serve as a plug-and-play strategy to enhance existing models without requiring modifications during inference. Extensive experiments in diverse all-in-one restoration settings demonstrate the superiority and generalization of our approach. For example, AirNet retrained with TUR achieves average improvements of 1.16 dB on three distinct tasks and 1.81 dB on five distinct all-in-one tasks. These results underscore TUR's effectiveness in advancing the SOTAs in all-in-one image restoration, paving the way for more robust and versatile image restoration.", "authors": ["Gang Wu", "Junjun Jiang", "Yijun Wang", "Kui Jiang", "Xianming Liu"], "year": 2025, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 5, "type": "article", "concepts": ["Regularization (linguistics)", "Image restoration", "Image (mathematics)", "Artificial intelligence", "Task (project management)"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4319302037", "doi": "https://doi.org/10.1007/s10462-023-10405-7", "title": "Deep learning: survey of environmental and camera impacts on internet of things images", "abstract": "Internet of Things (IoT) images are captivating growing attention because of their wide range of applications which requires visual analysis to drive automation. However, IoT images are predominantly captured from outdoor environments and thus are inherently impacted by the camera and environmental parameters which can adversely affect corresponding applications. Deep Learning (DL) has been widely adopted in the field of image processing and computer vision and can reduce the impact of these parameters on IoT images. Albeit, there are many DL-based techniques available in the current literature for analyzing and reducing the environmental and camera impacts on IoT images. However, to the best of our knowledge, no survey paper presents state-of-the-art DL-based approaches for this purpose. Motivated by this, for the first time, we present a Systematic Literature Review (SLR) of existing DL techniques available for analyzing and reducing environmental and camera lens impacts on IoT images. As part of this SLR, firstly, we reiterate and highlight the significance of IoT images in their respective applications. Secondly, we describe the DL techniques employed for assessing the environmental and camera lens distortion impacts on IoT images. Thirdly, we illustrate how DL can be effective in reducing the impact of environmental and camera lens distortion in IoT images. Finally, along with the critical reflection on the advantages and limitations of the techniques, we also present ways to address the research challenges of existing techniques and identify some further researches to advance the relevant research areas.", "authors": ["Roopdeep Kaur", "Gour Karmakar", "Feng Xia", "Muhammad Imran"], "year": 2023, "venue": "Artificial Intelligence Review", "cited_by_count": 13, "type": "article", "concepts": ["Computer science", "Internet of Things", "Artificial intelligence", "Field (mathematics)", "Automation"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W2901114637", "doi": "https://doi.org/10.48550/arxiv.1811.08575", "title": "Unsupervised Single Image Deraining with Self-supervised Constraints", "abstract": "Most existing single image deraining methods require learning supervised models from a large set of paired synthetic training data, which limits their generality, scalability and practicality in real-world multimedia applications. Besides, due to lack of labeled-supervised constraints, directly applying existing unsupervised frameworks to the image deraining task will suffer from low-quality recovery. Therefore, we propose an Unsupervised Deraining Generative Adversarial Network (UD-GAN) to tackle above problems by introducing self-supervised constraints from the intrinsic statistics of unpaired rainy and clean images. Specifically, we firstly design two collaboratively optimized modules, namely Rain Guidance Module (RGM) and Background Guidance Module (BGM), to take full advantage of rainy image characteristics: The RGM is designed to discriminate real rainy images from fake rainy images which are created based on outputs of the generator with BGM. Simultaneously, the BGM exploits a hierarchical Gaussian-Blur gradient error to ensure background consistency between rainy input and de-rained output. Secondly, a novel luminance-adjusting adversarial loss is integrated into the clean image discriminator considering the built-in luminance difference between real clean images and derained images. Comprehensive experiment results on various benchmarking datasets and different training settings show that UD-GAN outperforms existing image deraining methods in both quantitative and qualitative comparisons.", "authors": ["Xin Jin", "Zhibo Chen", "Jianxin Lin", "Zhikai Chen", "Wei Zhou"], "year": 2018, "venue": "arXiv (Cornell University)", "cited_by_count": 8, "type": "preprint", "concepts": ["Computer science", "Artificial intelligence", "Discriminator", "Image (mathematics)", "Pattern recognition (psychology)"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4388878512", "doi": "https://doi.org/10.1109/access.2023.3335618", "title": "Delving Deeper Into Image Dehazing: A Survey", "abstract": "Images captured under foggy or hazy weather conditions are affected by the scattering of atmospheric particles, resulting in decreased contrast and color variation, thereby limiting their practical applications. In recent years, deep learning methods showcase significant advancements in image dehazing. However, the complexity and degradation factors in hazy images challenge the generalization capacity of dehazing methods. This paper comprehensively reviews the recent developments in single-image dehazing techniques based on deep learning. From the perspectives of Convolutional Neural Networks (CNN) and Generative Adversarial Networks (GAN), different models are introduced and classified into four categories: Encoder-Decoder, Multi-Module, Multi-Branch, and Dual-Generative Adversarial Networks. The robustness and effectiveness of deep learning models are analyzed by comparing their performance and model complexity on public datasets. Additionally, limitations of current benchmark datasets and evaluation metrics are identified, and unresolved issues and future research directions are discussed. Our efforts in this paper will serve as a comprehensive reference for future research and call for further development in deep learning-based image dehazing.", "authors": ["Guohou Li", "Jia Li", "Gongchao Chen", "Zhibin Wang", "Songlin Jin", "Chang Ding", "Weidong Zhang"], "year": 2023, "venue": "IEEE Access", "cited_by_count": 9, "type": "article", "concepts": ["Computer science", "Robustness (evolution)", "Deep learning", "Artificial intelligence", "Benchmark (surveying)"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4221075701", "doi": "https://doi.org/10.3390/rs14061513", "title": "Unsupervised Remote Sensing Image Super-Resolution Guided by Visible Images", "abstract": "Remote sensing images are widely used in many applications. However, due to being limited by the sensors, it is difficult to obtain high-resolution (HR) images from remote sensing images. In this paper, we propose a novel unsupervised cross-domain super-resolution method devoted to reconstructing a low-resolution (LR) remote sensing image guided by an unpaired HR visible natural image. Therefore, an unsupervised visible image-guided remote sensing image super-resolution network (UVRSR) is built. The network is divided into two learnable branches: a visible image-guided branch (VIG) and a remote sensing image-guided branch (RIG). As HR visible images can provide rich textures and sufficient high-frequency information, the purpose of VIG is to treat them as targets and make full use of their advantages in reconstruction. Specially, we first use a CycleGAN to drag the LR visible natural images to the remote sensing domain; then, we apply an SR network to upscale these simulated remote sensing domain LR images. However, the domain gap between SR remote sensing images and HR visible targets is massive. To enforce domain consistency, we propose a novel domain-ruled discriminator in the reconstruction. Furthermore, inspired by the zero-shot super-resolution network (ZSSR) to explore the internal information of remote sensing images, we add a remote sensing domain inner study to train the SR network in RIG. Sufficient experimental works show UVRSR can achieve superior results with state-of-the-art unpaired and remote sensing SR methods on several challenging remote sensing image datasets.", "authors": ["Zili Zhang", "Yan Tian", "Jianxiang Li", "Yiping Xu"], "year": 2022, "venue": "Remote Sensing", "cited_by_count": 14, "type": "article", "concepts": ["Remote sensing", "Computer science", "Artificial intelligence", "Image (mathematics)", "Computer vision"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4366327759", "doi": "https://doi.org/10.48550/arxiv.2304.08291", "title": "Refusion: Enabling Large-Size Realistic Image Restoration with Latent-Space Diffusion Models", "abstract": "This work aims to improve the applicability of diffusion models in realistic image restoration. Specifically, we enhance the diffusion model in several aspects such as network architecture, noise level, denoising steps, training image size, and optimizer/scheduler. We show that tuning these hyperparameters allows us to achieve better performance on both distortion and perceptual scores. We also propose a U-Net based latent diffusion model which performs diffusion in a low-resolution latent space while preserving high-resolution information from the original input for the decoding process. Compared to the previous latent-diffusion model which trains a VAE-GAN to compress the image, our proposed U-Net compression strategy is significantly more stable and can recover highly accurate images without relying on adversarial optimization. Importantly, these modifications allow us to apply diffusion models to various image restoration tasks, including real-world shadow removal, HR non-homogeneous dehazing, stereo super-resolution, and bokeh effect transformation. By simply replacing the datasets and slightly changing the noise network, our model, named Refusion, is able to deal with large-size images (e.g., 6000 x 4000 x 3 in HR dehazing) and produces good results on all the above restoration problems. Our Refusion achieves the best perceptual performance in the NTIRE 2023 Image Shadow Removal Challenge and wins 2nd place overall.", "authors": ["Ziwei Luo", "Fredrik Gustafsson", "Zheng Zhao", "Jens Sjölund", "Thomas B. Schön"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 6, "type": "preprint", "concepts": ["Computer science", "Image restoration", "Artificial intelligence", "Distortion (music)", "Computer vision"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W3163628478", "doi": "https://doi.org/10.1371/journal.pone.0251337", "title": "An end-to-end sea fog removal network using multiple scattering model", "abstract": "An end-to-end sea fog removal network using multiple scattering model was proposed. In this network, the atmospheric multiple scattering model was re-formulated and used for sea fog removal. Compared with the atmospheric single scattering model, the atmospheric multiple scattering model could more comprehensively consider the effect of multiple scattering, which was important to the dense fog scenes, such as in ocean scene. Therefore, we used the atmospheric multiple scattering model to avoid image blurring. The model can directly generate the dehazing results, and unify the three parameters of the transmission map, the atmospheric light and the blur kernel into one formula. The latest smooth dilation and sub-pixel techniques were used in the network model. The latest techniques can avoid the gridding artifacts and the halo artifacts, the multi-scale sub-network was used to consider the features of multi-scale. In addition, multiple loss functions were used in end-to-end network. In the experimental results, the model was superior to the state-of-the-art models in terms of quantitatively and qualitatively.", "authors": ["Shunmin An", "Xixia Huang", "Zhangjing Zheng", "Linling Wang"], "year": 2021, "venue": "PLoS ONE", "cited_by_count": 8, "type": "article", "concepts": ["Scattering", "Halo", "Atmospheric model", "Computer science", "Diffuse sky radiation"], "search_query": "image denoising deblurring dehazing deraining deep learning"}

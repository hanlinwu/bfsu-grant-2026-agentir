{"openalex_id": "https://openalex.org/W4413114136", "doi": "https://doi.org/10.1109/tpami.2025.3598132", "title": "A Survey on All-in-One Image Restoration: Taxonomy, Evaluation and Future Trends", "abstract": "Image restoration (IR) seeks to recover high-quality images from degraded observations caused by a wide range of factors, including noise, blur, compression, and adverse weather. While traditional IR methods have made notable progress by targeting individual degradation types, their specialization often comes at the cost of generalization, leaving them ill-equipped to handle the multifaceted distortions encountered in real-world applications. In response to this challenge, the all-in-one image restoration (AiOIR) paradigm has recently emerged, offering a unified framework that adeptly addresses multiple degradation types.", "authors": ["Junjun Jiang", "Zhiyi Zuo", "Gang Wu", "Kui Jiang", "Xianming Liu"], "year": 2025, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "cited_by_count": 11, "type": "article", "concepts": ["Computer science", "Data science", "Taxonomy (biology)", "Categorization", "Image restoration"], "search_query": "all-in-one image restoration unified model degradation", "score": 10.0, "subtopic": "All-in-One Restoration", "rationale": "Comprehensive survey of all-in-one image restoration — perfectly aligned with the research topic, providing taxonomy, evaluation and future directions for AiOIR paradigm.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "First in-depth survey of AiOIR with taxonomy by architecture, learning paradigm, and innovations", "key_findings": "Systematizes AiOIR into unified framework, consolidates datasets and evaluation protocols, identifies key challenges and future directions", "limitations": "Survey paper, does not propose new method"}, "bib_key": "Jiang2025ASurvey"}
{"openalex_id": "https://openalex.org/W4404533930", "doi": "https://doi.org/10.1007/978-3-031-73202-7_25", "title": "DiffBIR: Toward Blind Image Restoration with Generative Diffusion Prior", "abstract": "", "authors": ["Xinqi Lin", "Jingwen He", "Ziyan Chen", "Zhaoyang Lyu", "Bo Dai", "Fanghua Yu", "Yu Qiao", "Wanli Ouyang", "Chao Dong"], "year": 2024, "venue": "Lecture notes in computer science", "cited_by_count": 136, "type": "book-chapter", "concepts": ["Computer science", "Generative grammar", "Image restoration", "Artificial intelligence", "Image (mathematics)"], "search_query": "diffusion model image restoration generation", "score": 9.0, "subtopic": "Diffusion Restoration", "rationale": "DiffBIR is a landmark blind image restoration system using generative diffusion priors; two-stage pipeline (degradation removal + diffusion regeneration) is highly relevant as foundational work for agent-based restoration pipelines.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Two-stage: restoration modules for degradation removal + IRControlNet leveraging latent diffusion for realistic detail generation; region-adaptive restoration guidance at inference", "key_findings": "SOTA on blind SR, face restoration, denoising; region-adaptive guidance allows fidelity-realness tradeoff", "limitations": "Two-stage pipeline may accumulate errors; computationally expensive"}, "bib_key": "Lin2024Toward"}
{"openalex_id": "https://openalex.org/W4386302521", "doi": "https://doi.org/10.48550/arxiv.2308.15070", "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior", "abstract": "We present DiffBIR, a general restoration pipeline that could handle different blind image restoration tasks in a unified framework. DiffBIR decouples blind image restoration problem into two stages: 1) degradation removal: removing image-independent content; 2) information regeneration: generating the lost image content...", "authors": ["Xinqi Lin", "Jingwen He", "Ziyan Chen", "Zhaoyang Lyu", "Ben Fei", "Bo Dai", "Wanli Ouyang", "Yu Qiao", "Chao Dong"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 46, "type": "preprint", "concepts": ["Image restoration", "Computer science", "Fidelity", "Pipeline (software)", "Generative model"], "search_query": "diffusion model image restoration generation", "score": 9.0, "subtopic": "Diffusion Restoration", "rationale": "DiffBIR arXiv preprint (same work as ECCV version above): unified blind restoration pipeline with degradation removal + IRControlNet diffusion regeneration. Perfect match for diffusion-based restoration.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Stage 1: restoration modules for degradation removal; Stage 2: IRControlNet with latent diffusion for detail generation; region-adaptive restoration guidance", "key_findings": "SOTA blind SR, face restoration, denoising on synthetic and real-world datasets", "limitations": "Two-stage cascade; computationally expensive at inference"}, "bib_key": "Lin2023Towards"}
{"openalex_id": "https://openalex.org/W4386043787", "doi": "https://doi.org/10.48550/arxiv.2308.09388", "title": "Diffusion Models for Image Restoration and Enhancement: A Comprehensive Survey", "abstract": "Image restoration (IR) has been an indispensable and challenging task in the low-level vision field, which strives to improve the subjective quality of images distorted by various forms of degradation. Recently, the diffusion model has achieved significant advancements in the visual generation of AIGC, thereby raising an intuitive question, 'whether diffusion model can boost image restoration'. To answer this, some pioneering studies attempt to integrate diffusion models into the image restoration task, resulting in superior performances than previous GAN-based methods...", "authors": ["Xin Li", "Yulin Ren", "Xin Jin", "Cuiling Lan", "Xingrui Wang", "Wenjun Zeng", "Xinchao Wang", "Zhibo Chen"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 41, "type": "preprint", "concepts": ["Computer science", "Inpainting", "Image restoration", "Deblurring", "Artificial intelligence"], "search_query": "diffusion model image restoration generation", "score": 9.0, "subtopic": "Diffusion Restoration", "rationale": "Comprehensive survey of diffusion models for image restoration; covers learning paradigms, conditional strategies, framework design; directly relevant as a background reference for diffusion-based restoration research.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Survey covering two workflows for diffusion-based IR; classifies methods for both standard and blind/real-world IR; compares SR, deblurring, inpainting methods", "key_findings": "Identifies 5 future directions: sampling efficiency, model compression, distortion simulation, distortion-invariant learning, framework design", "limitations": "Preprint survey; may not cover 2024 methods"}, "bib_key": "Li2023DiffusionModels"}
{"openalex_id": "https://openalex.org/W4381798022", "doi": "https://doi.org/10.48550/arxiv.2306.13090", "title": "PromptIR: Prompting for All-in-One Blind Image Restoration", "authors": ["Vaishnav Potlapalli", "Syed Waqas Zamir", "Salman Khan", "Fahad Shahbaz Khan"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 34, "type": "preprint", "concepts": ["Image restoration", "Computer science", "Degradation (telecommunications)", "Image (mathematics)", "Generalization"], "search_query": "prompt-based image restoration learning degradation", "score": 9, "subtopic": "All-in-One Restoration", "rationale": "PromptIR is a key all-in-one blind image restoration method using prompts to encode degradation-specific information. Directly cited in the research theme.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Prompt-based learning module encoding degradation info, dynamically guides restoration network; plug-and-play prompts", "key_findings": "State-of-the-art on denoising, deraining, dehazing with no prior knowledge of corruption type", "limitations": "Blind to composite/mixed degradations; prompt design is task-specific"}, "bib_key": "Potlapalli2023Prompting"}
{"openalex_id": "https://openalex.org/W4403792113", "doi": "https://doi.org/10.1145/3664647.3680762", "title": "Harmony in Diversity: Improving All-in-One Image Restoration via Multi-Task Collaboration", "abstract": "Deep learning-based all-in-one image restoration methods have garnered significant attention in recent years due to capable of addressing multiple degradation tasks.", "authors": ["Gang Wu", "Junjun Jiang", "Kui Jiang", "Xianming Liu"], "year": 2024, "venue": "", "cited_by_count": 14, "type": "article", "concepts": ["Harmony (color)", "Image restoration", "Computer science", "Diversity (politics)", "Task (project management)"], "search_query": "all-in-one image restoration unified model degradation", "score": 9.0, "subtopic": "All-in-One Restoration", "rationale": "Directly addresses all-in-one image restoration with multi-task collaboration optimization strategy — highly relevant to the research topic as it improves unified models for multiple degradation types.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Active-reweighting strategy (Art) for harmonizing multi-task optimization in all-in-one image restoration", "key_findings": "Art plug-and-play strategy achieves +1.16dB and +1.21dB PSNR improvements on AirNet and TransWeather respectively", "limitations": "Optimization strategy only, does not propose new architecture"}, "bib_key": "Wu2024HarmonyIn"}
{"openalex_id": "https://openalex.org/W4382173267", "doi": "https://doi.org/10.48550/arxiv.2306.13653", "title": "ProRes: Exploring Degradation-aware Visual Prompt for Universal Image Restoration", "abstract": "Image restoration aims to reconstruct degraded images, e.g., denoising or deblurring. Existing works focus on designing task-specific methods and there are inadequate attempts at universal methods. However, simply unifying multiple tasks into one universal architecture suffers from uncontrollable and undesired predictions. To address those issues, we explore prompt learning in universal architectures for image restoration tasks. In this paper, we present Degradation-aware Visual Prompts, which encode various types of image degradation, e.g., noise and blur, into unified visual prompts. These degradation-aware prompts provide control over image processing and allow weighted combinations for customized image restoration. We then leverage degradation-aware visual prompts to establish a controllable and universal model for image restoration, called ProRes, which is applicable to an extensive range of image restoration tasks. ProRes leverages the vanilla Vision Transformer (ViT) without any task-specific designs. Furthermore, the pre-trained ProRes can easily adapt to new tasks through efficient prompt tuning with only a few images. Without bells and whistles, ProRes achieves competitive performance compared to task-specific methods and experiments can demonstrate its ability for controllable restoration and adaptation for new tasks. The code and models will be released in \\url{https://github.com/leonmakise/ProRes}.", "authors": ["Jiaqi Ma", "Tianheng Cheng", "Guoli Wang", "Qian Zhang", "Xinggang Wang", "Lefei Zhang"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 14, "type": "preprint", "concepts": ["Deblurring", "Image restoration", "Computer science", "Artificial intelligence", "Leverage (statistics)"], "search_query": "image restoration transformer architecture efficient", "score": 9, "subtopic": "All-in-One Restoration", "rationale": "Degradation-aware visual prompts for universal image restoration; directly relevant to unified restoration with spatial awareness.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Visual prompt learning for degradation-aware universal restoration", "key_findings": "Prompts enable one model to handle diverse degradations", "limitations": "arXiv preprint, prompt design complexity"}, "bib_key": "Ma2023Exploring"}
{"openalex_id": "https://openalex.org/W4413147433", "doi": "https://doi.org/10.1109/cvpr52734.2025.01190", "title": "Complexity Experts are Task-Discriminative Learners for Any Image Restoration", "abstract": "Recent advancements in all-in-one image restoration models have revolutionized the ability to address diverse degradations through a unified framework. However, parameters tied to specific tasks often remain inactive for other tasks, making mixture-of-experts (MoE) architectures a natural extension.", "authors": ["Eduard Zamfir", "Zongwei Wu", "Nancy Mehta", "Yuedong Tan", "Danda Pani Paudel", "Yulun Zhang", "Radu Timofte"], "year": 2025, "venue": "", "cited_by_count": 11, "type": "article", "concepts": ["Discriminative model", "Computer science", "Task (project management)", "Artificial intelligence", "Image restoration"], "search_query": "all-in-one image restoration unified model degradation", "score": 9.0, "subtopic": "All-in-One Restoration", "rationale": "MoE-based all-in-one image restoration (MoCE-IR) with complexity-aware expert allocation — directly relevant to unified multi-degradation restoration with efficient inference.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Mixture of complexity experts with task-biased allocation for all-in-one restoration", "key_findings": "MoCE-IR allows bypassing irrelevant experts at inference while achieving SOTA performance", "limitations": "MoE complexity may not scale well to very large numbers of degradation types"}, "bib_key": "Zamfir2025ComplexityExper"}
{"openalex_id": "https://openalex.org/W4413157794", "doi": "https://doi.org/10.1109/cvpr52734.2025.00705", "title": "Vision-Language Gradient Descent-driven All-in-One Deep Unfolding Networks", "abstract": "Dynamic image degradations, including noise, blur and lighting inconsistencies, pose significant challenges in image restoration, often due to sensor limitations or adverse environmental conditions. Existing Deep Unfolding Networks (DUNs) offer stable restoration performance but require manual selection of degradation matrices for each degradation type, limiting their adaptability across diverse scenarios. To address this issue, we propose the Vision-Language-guided Unfolding Network (VLU-Net), a unified DUN framework for handling multiple degradation types simultaneously.", "authors": ["Haijin Zeng", "Xiangming Wang", "Yongyong Chen", "Jingyong Su", "Jie Liu"], "year": 2025, "venue": "", "cited_by_count": 6, "type": "article", "concepts": ["Computer science", "Gradient descent", "Artificial intelligence", "Descent (aeronautics)", "Stochastic gradient descent"], "search_query": "all-in-one image restoration unified model degradation", "score": 9.0, "subtopic": "All-in-One Restoration", "rationale": "VLM-guided all-in-one deep unfolding network — directly combines vision-language model with unified restoration framework, addressing multiple degradations simultaneously, highly relevant.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "VLM (refined on degraded image-text pairs) aligns features with degradation descriptions to auto-select transforms in DUN framework", "key_findings": "VLU-Net outperforms one-by-one and all-in-one methods by 3.74dB on SOTS dehazing and 1.70dB on Rain100L deraining", "limitations": "Requires paired image-text training data for VLM refinement"}, "bib_key": "Zeng2025Gradient"}
{"openalex_id": "https://openalex.org/W4389599809", "doi": "https://doi.org/10.48550/arxiv.2312.05038", "title": "Prompt-In-Prompt Learning for Universal Image Restoration", "abstract": "Image restoration, which aims to retrieve and enhance degraded images, is fundamental across a wide range of applications. While conventional deep learning approaches have notably improved the image quality across various tasks, they still suffer from (i) the high storage cost needed for various task-specific models and (ii) the lack of interactivity and flexibility, hindering their wider application. Drawing inspiration from the pronounced success of prompts in both linguistic and visual domains, we propose novel Prompt-In-Prompt learning for universal image restoration, named PIP. First, we present two novel prompts, a degradation-aware prompt to encode high-level degradation knowledge and a basic restoration prompt to provide essential low-level information. Second, we devise a novel prompt-to-prompt interaction module to fuse these two prompts into a universal restoration prompt. Third, we introduce a selective prompt-to-feature interaction module to modulate the degradation-related feature. By doing so, the resultant PIP works as a plug-and-play module to enhance existing restoration models for universal image restoration. Extensive experimental results demonstrate the superior performance of PIP on multiple restoration tasks, including image denoising, deraining, dehazing, deblurring, and low-light enhancement. Remarkably, PIP is interpretable, flexible, efficient, and easy-to-use, showing promising potential for real-world applications. The code is available at https://github.com/longzilicart/pip_universal.", "authors": ["Zilong Li", "Yiming Lei", "Chenglong Ma", "Junping Zhang", "Hongming Shan"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 6, "type": "preprint", "concepts": ["Deblurring", "Computer science", "Image restoration", "Flexibility (engineering)", "Feature (linguistics)"], "search_query": "image denoising deblurring dehazing deraining deep learning", "score": 9, "subtopic": "All-in-One Restoration", "rationale": "Prompt-in-prompt learning for universal image restoration; directly relevant to unified prompt-based restoration approaches.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Hierarchical prompt-in-prompt framework for universal restoration", "key_findings": "Flexible prompt-based approach handling diverse degradations", "limitations": "arXiv preprint"}, "bib_key": "Li2023Learning"}
{"openalex_id": "https://openalex.org/W4409581375", "doi": "https://doi.org/10.1109/tpami.2025.3562211", "title": "Degradation-Aware Residual-Conditioned Optimal Transport for Unified Image Restoration", "abstract": "Unified, or more formally, all-in-one image restoration has emerged as a practical and promising low-level vision task for real-world applications. In this context, the key issue lies in how to deal with different types of degraded images simultaneously.", "authors": ["Xiaole Tang", "Xiang Gu", "Xiaoyi He", "Xin Hu", "Jian Sun"], "year": 2025, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "cited_by_count": 5, "type": "article", "concepts": ["Degradation (telecommunications)", "Residual", "Image restoration", "Computer science", "Artificial intelligence"], "search_query": "all-in-one image restoration unified model degradation", "score": 9.0, "subtopic": "All-in-One Restoration", "rationale": "DA-RCOT models all-in-one image restoration as optimal transport problem with degradation-specific residual cues — directly relevant to unified restoration for multiple degradations including real-world mixed degradations.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Degradation-aware residual-conditioned optimal transport (DA-RCOT) with two-pass map for all-in-one restoration", "key_findings": "Favorable performance across 5 degradations with superior adaptability to real-world mixed degradations", "limitations": "Optimal transport formulation is computationally expensive"}, "bib_key": "Tang2025"}
{"openalex_id": "https://openalex.org/W4410294577", "doi": "https://doi.org/10.1109/tip.2025.3567205", "title": "Learning Dynamic Prompts for All-in-One Image Restoration", "abstract": "All-in-one image restoration, which seeks to handle multiple types of degradation within a unified model, has become a prominent research topic in computer vision.", "authors": ["Gang Wu", "Junjun Jiang", "Kui Jiang", "Xianming Liu", "Liqiang Nie"], "year": 2025, "venue": "IEEE Transactions on Image Processing", "cited_by_count": 5, "type": "article", "concepts": ["Image restoration", "Computer science", "Computer vision", "Artificial intelligence", "Image processing"], "search_query": "all-in-one image restoration unified model degradation", "score": 9.0, "subtopic": "All-in-One Restoration", "rationale": "Dynamic prompt learning for all-in-one image restoration — directly addresses unified restoration with adaptive degradation-specific prompts, highly relevant to the research topic.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "DPPD: Degradation Prototype Assignment + Prompt Distribution Learning for dynamic adaptive prompts in unified restoration", "key_findings": "Dynamic prompts from distributions outperform fixed prompts; discriminative degradation representations improve multi-task performance", "limitations": "Prompt-based approach may not generalize to unseen degradation types"}, "bib_key": "Wu2025LearningDynamic"}
{"openalex_id": "https://openalex.org/W4406110715", "doi": "https://doi.org/10.1109/access.2025.3526168", "title": "Always Clear Days: Degradation Type and Severity Aware All-in-One Adverse Weather Removal", "abstract": "All-in-one adverse weather removal is an emerging topic on image restoration, which aims to restore multiple weather degradations in a unified model, and the challenges are twofold.", "authors": ["Y.-T Chen", "Soo-Chang Pei"], "year": 2025, "venue": "IEEE Access", "cited_by_count": 5, "type": "article", "concepts": ["Computer science", "Adverse weather", "Leverage (statistics)", "Domain adaptation", "Normalization (sociology)"], "search_query": "all-in-one image restoration unified model degradation", "score": 9.0, "subtopic": "All-in-One Restoration", "rationale": "Degradation type and severity aware all-in-one adverse weather removal — directly relevant to unified restoration with spatially-aware degradation analysis (severity and type).", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "UtilityIR: Marginal Quality Ranking Loss + Contrastive Loss for weather type/severity extraction, MHCA and LG-AdaIN for spatially-varying restoration", "key_findings": "Outperforms SOTA with fewer parameters; handles previously unseen combined degradations", "limitations": "Limited to adverse weather degradations"}, "bib_key": "Chen2025AlwaysClear"}
{"openalex_id": "https://openalex.org/W4406039833", "doi": "https://doi.org/10.1007/s10489-024-06226-y", "title": "RamIR: Reasoning and action prompting with Mamba for all-in-one image restoration", "abstract": "", "authors": ["A. H. Tang", "Yan Wu", "Yuwei Zhang"], "year": 2025, "venue": "Applied Intelligence", "cited_by_count": 3, "type": "article", "concepts": ["Computer science", "Benchmark (surveying)", "Transformer", "Block (permutation group theory)", "Artificial intelligence"], "search_query": "all-in-one image restoration unified model degradation", "score": 9.0, "subtopic": "All-in-One Restoration", "rationale": "Reasoning and action prompting with Mamba for all-in-one image restoration — highly relevant combining reasoning/prompting with state-space model architecture for unified restoration.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Mamba-based all-in-one restoration model with reasoning and action prompting for degradation-aware processing", "key_findings": "Mamba architecture with reasoning prompts achieves competitive performance on all-in-one benchmarks", "limitations": "Limited abstract available; Mamba may have limitations on very high-resolution images"}, "bib_key": "Tang2025Reasoning"}
{"openalex_id": "https://openalex.org/W4406347607", "doi": "https://doi.org/10.1016/j.engappai.2025.110017", "title": "Collaborative Semantic Contrastive for All-in-one Image Restoration", "abstract": "", "authors": ["Bin Hu", "Sai Yang", "Fan Liu", "Weiping Ding"], "year": 2025, "venue": "Engineering Applications of Artificial Intelligence", "cited_by_count": 3, "type": "article", "concepts": ["Computer science", "Image (mathematics)", "Natural language processing", "Artificial intelligence", "Information retrieval"], "search_query": "all-in-one image restoration unified model degradation", "score": 9.0, "subtopic": "All-in-One Restoration", "rationale": "Collaborative semantic contrastive learning for all-in-one image restoration — directly relevant to unified restoration framework with contrastive training for degradation discrimination.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Semantic contrastive learning to improve feature discrimination across degradation types in all-in-one restoration", "key_findings": "Collaborative contrastive training improves degradation-specific feature learning for unified model", "limitations": "Limited abstract available"}, "bib_key": "Hu2025CollaborativeSe"}
{"openalex_id": "https://openalex.org/W4415970876", "doi": "https://doi.org/10.1109/tcsvt.2025.3629686", "title": "Diff-Restorer: Unleashing Visual Prompts for Diffusion-based Universal Image Restoration", "abstract": "Image restoration aims to recover high-quality images from degraded observations, yet real-world degradations are complex, coupled, and difficult to model.", "authors": ["Yuhong Zhang", "Hengsheng Zhang", "Xinning Chai", "Zhengxue Cheng", "Rong Xie", "Li Song", "Wenjun Zhang"], "year": 2025, "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "cited_by_count": 3, "type": "article", "concepts": [], "search_query": "all-in-one image restoration unified model degradation", "score": 9.0, "subtopic": "Diffusion Restoration", "rationale": "Diff-Restorer combines diffusion model with VLM (CLIP) visual prompts for universal image restoration across single, real-world and mixed degradations — directly combines multiple relevant topics.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Diffusion-based universal restoration using CLIP to extract decoupled content and degradation embeddings as visual prompts", "key_findings": "Outperforms SOTA on single, real-world, and mixed degradation tasks for generality, realism, and fidelity", "limitations": "CLIP-based prompts may struggle with novel degradation types not seen during CLIP training"}, "bib_key": "Zhang2025Unleashing"}
{"openalex_id": "https://openalex.org/W4413146948", "doi": "https://doi.org/10.1109/cvpr52734.2025.02623", "title": "Degradation-Aware Feature Perturbation for All-in-One Image Restoration", "abstract": "All-in-one image restoration aims to recover clear images from various degradation types and levels with a unified model.", "authors": ["Xiangpeng Tian", "Xiangyu Liao", "Xiao Liu", "Meng Li", "Chao Ren"], "year": 2025, "venue": "", "cited_by_count": 2, "type": "article", "concepts": ["Image restoration", "Degradation (telecommunications)", "Perturbation (astronomy)", "Computer science", "Feature (linguistics)"], "search_query": "all-in-one image restoration unified model degradation", "score": 9.0, "subtopic": "All-in-One Restoration", "rationale": "Degradation-aware feature perturbation for all-in-one restoration (DFPIR) — directly relevant to unified restoration with task interference mitigation via channel-wise and attention-wise perturbations.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "DFPIR: Degradation-Guided Perturbation Block with channel shuffling and attention masking to align features across degradation types", "key_findings": "SOTA on denoising, dehazing, deraining, motion deblurring, and low-light enhancement in all-in-one setting", "limitations": "Perturbation operations add computational overhead"}, "bib_key": "Tian2025Feature"}
{"openalex_id": "https://openalex.org/W4393946425", "doi": "https://doi.org/10.48550/arxiv.2404.02154", "title": "Dynamic Pre-training: Towards Efficient and Scalable All-in-One Image Restoration", "abstract": "All-in-one image restoration tackles different types of degradations with a unified model instead of having task-specific, non-generic models for each degradation.", "authors": ["Akshay Dudhane", "Omkar Thawakar", "Syed Waqas Zamir", "Salman Khan", "Fahad Shahbaz Khan", "Ming-Hsuan Yang"], "year": 2024, "venue": "arXiv (Cornell University)", "cited_by_count": 2, "type": "preprint", "concepts": ["Training (meteorology)", "Scalability", "Computer science", "Image (mathematics)", "Image restoration"], "search_query": "all-in-one image restoration unified model degradation", "score": 9.0, "subtopic": "All-in-One Restoration", "rationale": "DyNet: dynamic pre-training for efficient scalable all-in-one image restoration — directly relevant to unified restoration with lightweight deployment through weight-sharing dynamic networks.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "DyNet: encoder-decoder with weights-sharing for multiple variants; dynamic pre-training on Million-IRD dataset (2M images) trains all variants jointly", "key_findings": "31.34% GFlops reduction and 56.75% parameter reduction vs baseline while achieving SOTA on all-in-one denoising, deraining, dehazing", "limitations": "Limited to 3 degradations in experiments; large dataset collection required"}, "bib_key": "Dudhane2024Dynamic"}
{"openalex_id": "https://openalex.org/W7126221274", "doi": "https://doi.org/10.48550/arxiv.2601.21592", "title": "Unifying Heterogeneous Degradations: Uncertainty-Aware Diffusion Bridge Model for All-in-One Image Restoration", "abstract": "All-in-One Image Restoration (AiOIR) faces the fundamental challenge in reconciling conflicting optimization objectives across heterogeneous degradations.", "authors": ["Luwei Tu", "Jiawei Wu", "Xing Luo", "Zhi Jin"], "year": 2026, "venue": "arXiv (Cornell University)", "cited_by_count": 0, "type": "preprint", "concepts": ["Computer science", "Schedule", "Mathematical optimization", "Path (computing)", "Image restoration"], "search_query": "all-in-one image restoration unified model degradation", "score": 9.0, "subtopic": "Diffusion Restoration", "rationale": "Uncertainty-aware diffusion bridge for all-in-one image restoration — directly relevant combining diffusion-based approach with unified multi-degradation framework.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "UDBM: relaxed diffusion bridge with pixel-wise uncertainty, noise schedule and path schedule for all-in-one restoration in a single step", "key_findings": "SOTA performance across diverse restoration tasks within single inference step via entropy-regularized transport", "limitations": "Very recent preprint, not yet peer-reviewed"}, "bib_key": "Tu2026UnifyingHeterog"}
{"openalex_id": "https://openalex.org/W4404687984", "doi": "https://doi.org/10.1109/tip.2024.3501855", "title": "MWFormer: Multi-Weather Image Restoration Using Degradation-Aware Transformers", "abstract": "Restoring images captured under adverse weather conditions is a fundamental task for many computer vision applications. However, most existing weather restoration approaches are only capable of handling a specific type of degradation, which is often insufficient in real-world scenarios, such as rainy-snowy or rainy-hazy weather. Towards being able to address these situations, we propose a multi-weather Transformer, or MWFormer for short, which is a holistic vision Transformer that aims to solve multiple weather-induced degradations using a single, unified architecture. MWFormer uses hyper-networks and feature-wise linear modulation blocks to restore images degraded by various weather types using the same set of learned parameters. We first employ contrastive learning to train an auxiliary network that extracts content-independent, distortion-aware feature embeddings that efficiently represent predicted weather types, of which more than one may occur. Guided by these weather-informed predictions, the image restoration Transformer adaptively modulates its parameters to conduct both local and global feature processing, in response to multiple possible weather. Moreover, MWFormer allows for a novel way of tuning, during application, to either a single type of weather restoration or to hybrid weather restoration without any retraining, offering greater controllability than existing methods. Our experimental results on multi-weather restoration benchmarks show that MWFormer achieves significant performance improvements compared to existing state-of-the-art methods, without requiring much computational cost. Moreover, we demonstrate that our methodology of using hyper-networks can be integrated into various network architectures to further boost their performance. The code is available at: https://github.com/taco-group/MWFormer.", "authors": ["Ruoxi Zhu", "Zhengzhong Tu", "Jiaming Liu", "Alan C. Bovik", "Yibo Fan"], "year": 2024, "venue": "IEEE Transactions on Image Processing", "cited_by_count": 28, "type": "article", "concepts": ["Image restoration", "Computer science", "Image processing", "Degradation (telecommunications)", "Computer vision"], "search_query": "image restoration transformer architecture efficient", "score": 8.5, "subtopic": "All-in-One Restoration", "rationale": "Multi-weather degradation-aware transformer for unified restoration; highly relevant to all-in-one and spatial-aware processing.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Degradation-aware transformer handling multiple weather conditions in one model", "key_findings": "Unified multi-weather restoration with degradation awareness", "limitations": "Limited to weather-type degradations"}, "bib_key": "Zhu2024"}
{"openalex_id": "https://openalex.org/W4409366360", "doi": "https://doi.org/10.1609/aaai.v39i8.32905", "title": "Debiased All-in-one Image Restoration with Task Uncertainty Regularization", "abstract": "All-in-one image restoration is a fundamental low-level vision task with significant real-world applications. The primary challenge lies in addressing diverse degradations within a single model. While current methods primarily exploit task prior information to guide the restoration models, they typically employ uniform multi-task learning, overlooking the heterogeneity in model optimization across different degradation tasks. To eliminate the bias, we propose a task-aware optimization strategy, that introduces adaptive task-specific regularization for multi-task image restoration learning. Specifically, our method dynamically weights and balances losses for different restoration tasks during training, encouraging the implementation of the most reasonable optimization route. In this way, we can achieve more robust and effective model training. Notably, our approach can serve as a plug-and-play strategy to enhance existing models without requiring modifications during inference. Extensive experiments in diverse all-in-one restoration settings demonstrate the superiority and generalization of our approach. For example, AirNet retrained with TUR achieves average improvements of 1.16 dB on three distinct tasks and 1.81 dB on five distinct all-in-one tasks. These results underscore TUR's effectiveness in advancing the SOTAs in all-in-one image restoration, paving the way for more robust and versatile image restoration.", "authors": ["Gang Wu", "Junjun Jiang", "Yijun Wang", "Kui Jiang", "Xianming Liu"], "year": 2025, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 5, "type": "article", "concepts": ["Regularization (linguistics)", "Image restoration", "Image (mathematics)", "Artificial intelligence", "Task (project management)"], "search_query": "image denoising deblurring dehazing deraining deep learning", "score": 8.5, "subtopic": "All-in-One Restoration", "rationale": "Debiased all-in-one restoration with task uncertainty regularization; directly relevant to unified multi-task restoration.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Task uncertainty regularization to address bias in all-in-one restoration", "key_findings": "Debiasing improves balanced performance across degradation types", "limitations": "Regularization adds training complexity"}, "bib_key": "Wu2025Debiased"}
{"openalex_id": "https://openalex.org/W4390874575", "doi": "https://doi.org/10.1109/iccv51070.2023.00371", "title": "Segment Anything", "abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation...", "authors": ["Alexander M. Kirillov", "Eric Mintun", "Nikhila Ravi", "Hanzi Mao", "Chloe Rolland", "Laura Gustafson", "Tete Xiao", "Spencer Whitehead", "Alexander C. Berg", "Wan-Yen Lo", "Piotr Dollar", "Ross Girshick"], "year": 2023, "venue": "", "cited_by_count": 7782, "type": "article", "concepts": ["Computer science", "Segmentation", "Task (project management)", "Artificial intelligence", "Shot (pellet)"], "search_query": "segment anything model SAM zero-shot segmentation", "score": 8.0, "subtopic": "Foundation Models Vision", "rationale": "SAM is a foundational component in the spatially-aware image restoration pipeline — used for region-level segmentation before degradation analysis.", "alignment": {"task": "medium", "method": "high", "modality": "high"}, "extraction": {"design": "Promptable segmentation model trained on SA-1B (1B masks, 11M images) with point/box/mask prompts.", "key_findings": "Zero-shot performance competitive with or superior to supervised models across diverse segmentation tasks.", "limitations": "Segmentation only — does not perform restoration or degradation analysis."}, "bib_key": "Kirillov2023SegmentAnything"}
{"openalex_id": "https://openalex.org/W4225672218", "doi": "https://doi.org/10.1109/cvpr52688.2022.00564", "title": "Restormer: Efficient Transformer for High-Resolution Image Restoration", "abstract": "Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shortcomings of CNNs (i.e., limited receptive field and inadaptability to input content), its computational complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to most image restoration tasks involving high-resolution images. In this work, we propose an efficient Transformer model by making several key designs in the building blocks (multi-head attention and feed-forward network) such that it can capture long-range pixel interactions, while still remaining applicable to large images. Our model, named Restoration Transformer (Restormer), achieves state-of-the-art results on several image restoration tasks, including image deraining, single-image motion deblurring, defocus deblurring (single-image and dual-pixel data), and image denoising (Gaussian grayscale/color denoising, and real image denoising). The source code and pre-trained models are available at https://github.com/swz30/Restormer.", "authors": ["Syed Waqas Zamir", "Aditya Arora", "Salman Khan", "Munawar Hayat", "Fahad Shahbaz Khan", "Ming–Hsuan Yang"], "year": 2022, "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "cited_by_count": 3123, "type": "article", "concepts": ["Deblurring", "Computer science", "Artificial intelligence", "Image restoration", "Computer vision"], "search_query": "image restoration transformer architecture efficient", "score": 8, "subtopic": "Transformer Restoration", "rationale": "Foundational transformer architecture for image restoration; key baseline for transformer-based methods.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Multi-Dconv head transposed attention and gated-Dconv feed-forward network", "key_findings": "SOTA on denoising, deblurring, deraining at CVPR 2022", "limitations": "Single-task focus per model instance"}, "bib_key": "Zamir2022Efficient"}
{"openalex_id": "https://openalex.org/W3212516020", "doi": "https://doi.org/10.1145/3528233.3530757", "title": "Palette: Image-to-Image Diffusion Models", "abstract": "This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration.", "authors": ["Chitwan Saharia", "William Chan", "Huiwen Chang", "Chris Lee", "Jonathan Ho", "Tim Salimans", "David J. Fleet", "Mohammad Norouzi"], "year": 2022, "venue": "", "cited_by_count": 1396, "type": "preprint", "concepts": ["Computer science", "Inpainting", "Artificial intelligence", "Image (mathematics)", "Task (project management)"], "search_query": "all-in-one image restoration unified model degradation", "score": 8.0, "subtopic": "Diffusion Restoration", "rationale": "Unified diffusion model for multiple image restoration/translation tasks (inpainting, JPEG restoration, colorization) — highly relevant as both a diffusion-based approach and a unified multi-task restoration framework.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Conditional diffusion model unified across colorization, inpainting, uncropping, and JPEG restoration without task-specific tuning", "key_findings": "Generalist diffusion model matches or exceeds GAN/regression baselines across all tasks; L2 vs L1 affects diversity", "limitations": "Not specifically designed for all-in-one multi-degradation handling; each task still trained separately"}, "bib_key": "Saharia2022"}
{"openalex_id": "https://openalex.org/W4390872095", "doi": "https://doi.org/10.1109/iccv51070.2023.01130", "title": "Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model", "abstract": "In this paper, we rethink the low-light image enhancement task and propose a physically explainable and generative diffusion model for low-light image enhancement, termed as Diff-Retinex. We aim to integrate the advantages of the physical model and the generative network. Furthermore, we hope to supplement and even deduce the information missing in the low-light image through the generative network. Therefore, Diff-Retinex formulates the lowlight image enhancement problem into Retinex decomposition and conditional image generation...", "authors": ["Xunpeng Yi", "Han Xu", "Hao Zhang", "Linfeng Tang", "Jiayi Ma"], "year": 2023, "venue": "", "cited_by_count": 187, "type": "article", "concepts": ["Color constancy", "Artificial intelligence", "Generative model", "Computer science", "Computer vision"], "search_query": "diffusion model image restoration generation", "score": 8.0, "subtopic": "Diffusion Restoration", "rationale": "Diff-Retinex integrates Retinex decomposition with generative diffusion model for low-light enhancement; directly addresses image restoration using diffusion priors with physical models.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Retinex Transformer decomposition network (TDN) + multi-path generative diffusion to reconstruct normal-light Retinex distributions", "key_findings": "Effective low-light enhancement with generative diffusion; handles dark illumination, noise, color deviation simultaneously", "limitations": "Specific to low-light; may not generalize to other degradation types"}, "bib_key": "Yi2023Rethinking"}
{"openalex_id": "https://openalex.org/W4386066096", "doi": "https://doi.org/10.1109/cvpr52729.2023.01350", "title": "ShadowDiffusion: When Degradation Prior Meets Diffusion Model for Shadow Removal", "abstract": "Recent deep learning methods have achieved promising results in image shadow removal. However, their restored images still suffer from unsatisfactory boundary artifacts, due to the lack of degradation prior embedding and the deficiency in modeling capacity. Our work addresses these issues by proposing a unified diffusion framework that integrates both the image and degradation priors for highly effective shadow removal...", "authors": ["Lanqing Guo", "Chong Wang", "Wenhan Yang", "Siyu Huang", "Yufei Wang", "Hanspeter Pfister", "Bihan Wen"], "year": 2023, "venue": "", "cited_by_count": 108, "type": "article", "concepts": ["Shadow (psychology)", "Computer science", "Artificial intelligence", "Degradation (telecommunications)", "Image (mathematics)"], "search_query": "diffusion model image restoration generation", "score": 8.0, "subtopic": "Diffusion Restoration", "rationale": "ShadowDiffusion integrates degradation prior + diffusion for shadow removal; progressively refines shadow mask as auxiliary task; strong baseline for spatially-localized degradation removal with diffusion.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Unrolling diffusion model with shadow degradation model as prior; progressive shadow mask refinement as auxiliary diffusion task", "key_findings": "Significant PSNR improvement (+3dB on SRD vs. SOTA); better boundary artifact handling", "limitations": "Specific to shadow removal; shadow mask estimation may fail in complex scenes"}, "bib_key": "Guo2023When"}
{"openalex_id": "https://openalex.org/W4393159973", "doi": "https://doi.org/10.1609/aaai.v38i8.28746", "title": "ResDiff: Combining CNN and Diffusion Model for Image Super-resolution", "abstract": "Adapting the Diffusion Probabilistic Model (DPM) for direct image super-resolution is wasteful, given that a simple Convolutional Neural Network (CNN) can recover the main low-frequency content. Therefore, we present ResDiff, a novel Diffusion Probabilistic Model based on Residual structure for Single Image Super-Resolution (SISR)...", "authors": ["Shuyao Shang", "Zhengyang Shan", "Guangxing Liu", "Lunqian Wang", "Xinghua Wang", "Zekai Zhang", "Jinglin Zhang"], "year": 2024, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 104, "type": "article", "concepts": ["Image (mathematics)", "Diffusion", "Superresolution", "Computer science", "Resolution (logic)"], "search_query": "diffusion model image restoration generation", "score": 8.0, "subtopic": "Diffusion Restoration", "rationale": "ResDiff combines CNN (low-frequency) + DPM (residual high-frequency) for SR; efficient hybrid approach directly relevant to diffusion-based image restoration.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "CNN restores low-frequency; DPM predicts residual in HR-CNN space; frequency-domain loss for CNN; frequency-guided diffusion for DPM", "key_findings": "Faster convergence, superior quality, diverse samples vs. pure diffusion SR methods", "limitations": "Specific to SR task; two-component design requires careful balancing"}, "bib_key": "Shang2024Combining"}
{"openalex_id": "https://openalex.org/W4402716100", "doi": "https://doi.org/10.1109/cvpr52733.2024.00268", "title": "Residual Denoising Diffusion Models", "abstract": "We propose residual denoising diffusion models (RDDM), a novel dual diffusion process that decouples the traditional single denoising diffusion process into residual diffusion and noise diffusion. This dual diffusion framework expands the denoising-based diffusion models, initially uninterpretable for image restoration, into a unified and interpretable model for both image generation and restoration by introducing residuals...", "authors": ["Jiawei Liu", "Qiang Wang", "Huijie Fan", "Yinong Wang", "Yandong Tang", "Liangqiong Qu"], "year": 2024, "venue": "", "cited_by_count": 77, "type": "article", "concepts": ["Residual", "Diffusion", "Noise reduction", "Computer science", "Artificial intelligence"], "search_query": "diffusion model image restoration generation", "score": 8.0, "subtopic": "Diffusion Restoration", "rationale": "RDDM decouples diffusion into residual+noise components to unify image generation and restoration; direct contribution to diffusion-based restoration with UNet + L1 loss showing strong performance.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Dual diffusion: residual diffusion (target-to-degraded) + noise diffusion (random perturbation); generic UNet trained with L1 loss and batch size 1", "key_findings": "Competitive with SOTA restoration methods using simple UNet; partially path-independent generation for better reverse process understanding", "limitations": "Unified framework may sacrifice task-specific performance; sampling still multi-step"}, "bib_key": "Liu2024ResidualDenoisi"}
{"openalex_id": "https://openalex.org/W4402704606", "doi": "https://doi.org/10.1109/cvpr52733.2024.02408", "title": "Q-Instruct: Improving Low-Level Visual Abilities for Multi-Modality Foundation Models", "abstract": "Multi-modality large language models (MLLMs)...", "authors": ["Haoning Wu", "Zicheng Zhang"], "year": 2024, "venue": "", "cited_by_count": 62, "type": "article", "concepts": ["Modality (human-computer interaction)", "Foundation (evidence)", "Computer science", "Artificial intelligence", "Human-computer interaction"], "search_query": "visual language model image quality assessment", "score": 8.0, "subtopic": "Image Quality Assessment", "rationale": "Q-Instruct creates VLM instruction-following data for low-level IQA — directly supports VLM-based quality assessment in agent-based restoration pipelines.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Q-Pathway: 58K human feedbacks on 18,973 images; GPT-converted to 200K Q-Instruct instruction-response pairs for MLLM low-level visual training.", "key_findings": "Q-Instruct consistently improves low-level visual capabilities (IQA, quality description) across multiple base models.", "limitations": "IQA/description training data contribution; no restoration component."}, "bib_key": "Wu2024Improving"}
{"openalex_id": "https://openalex.org/W4403947072", "doi": "https://doi.org/10.1007/978-3-031-73247-8_5", "title": "Pixel-Aware Stable Diffusion for Realistic Image Super-Resolution and Personalized Stylization", "abstract": "Diffusion models have demonstrated impressive performance in various image generation, editing, enhancement and translation tasks. In particular, the pre-trained text-to-image stable diffusion models provide a potential solution to the challenging realistic image super-resolution (Real-ISR) and image stylization problems with their strong generative priors...", "authors": ["Tao Yang", "Rongyuan Wu", "Peiran Ren", "Xuansong Xie", "Lei Zhang"], "year": 2024, "venue": "Lecture notes in computer science", "cited_by_count": 58, "type": "book-chapter", "concepts": ["Computer science", "Pixel", "Computer vision", "Computer graphics (images)", "Artificial intelligence"], "search_query": "diffusion model image restoration generation", "score": 8.0, "subtopic": "Diffusion Restoration", "rationale": "PASD leverages stable diffusion priors for realistic SR and stylization; pixel-aware cross-attention for local structure preservation + degradation removal module; directly relevant to diffusion-based SR/restoration.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Pixel-aware cross-attention for local structure + degradation removal module + adjustable noise schedule; stylization via base model replacement", "key_findings": "Robust Real-ISR and personalized stylization without pairwise training data", "limitations": "Dependent on stable diffusion base model quality; limited to image domain"}, "bib_key": "Yang2024Stable"}
{"openalex_id": "https://openalex.org/W4396782894", "doi": "https://doi.org/10.1109/tcsvt.2024.3398810", "title": "Prompt-Based Ingredient-Oriented All-in-One Image Restoration", "authors": ["Hu Gao", "Jing Yang", "Ying Zhang", "Ning Wang", "Jingfan Yang", "Depeng Dang"], "year": 2024, "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "cited_by_count": 26, "type": "article", "concepts": ["Ingredient", "Image restoration", "Computer science", "Image processing", "Computer vision"], "search_query": "prompt-based image restoration learning degradation", "score": 8, "subtopic": "All-in-One Restoration", "rationale": "All-in-one image restoration method using prompt-based ingredient-oriented learning with CNN-Transformer hybrid. Published in TCSVT.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "CAPTNet: encoder with degradation-specific prompts guiding decoder, CNN+Transformer, multi-head rearranged attention with prompts", "key_findings": "Competitive performance on multiple degradation tasks", "limitations": "Ingredient-oriented approach may not generalize to unseen degradation types"}, "bib_key": "Gao2024"}
{"openalex_id": "https://openalex.org/W4409723823", "doi": "https://doi.org/10.1109/tpami.2025.3563612", "title": "Diff-Retinex++: Retinex-Driven Reinforced Diffusion Model for Low-Light Image Enhancement", "abstract": "This paper proposes a Retinex-driven reinforced diffusion model for low-light image enhancement, termed Diff-Retinex++, to address various degradations caused by low light. Our main approach integrates the diffusion model with Retinex-driven restoration to achieve physically-inspired generative enhancement, making it a pioneering effort...", "authors": ["Xunpeng Yi", "Han Xu", "Hao Zhang", "Linfeng Tang", "Jiayi Ma"], "year": 2025, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "cited_by_count": 25, "type": "article", "concepts": ["Color constancy", "Artificial intelligence", "Image enhancement", "Computer vision", "Computer science"], "search_query": "diffusion model image restoration generation", "score": 8.0, "subtopic": "Diffusion Restoration", "rationale": "Diff-Retinex++ extends Diff-Retinex with MoE and knowledge distillation; Retinex-driven diffusion for low-light with dual DDM+RMoE stages; published in TPAMI.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "DDM (denoising diffusion) + RMoE (Retinex-driven MoE with knowledge distillation and attention); iterative DDM+RMoE for reinforced enhancement", "key_findings": "State-of-the-art low-light enhancement with physical interpretability; improved fidelity and vividness over Diff-Retinex", "limitations": "Complex two-stage design; focused on low-light only"}, "bib_key": "Yi2025"}
{"openalex_id": "https://openalex.org/W4402776053", "doi": "https://doi.org/10.1109/cvpr52733.2024.00244", "title": "Learning Diffusion Texture Priors for Image Restoration", "abstract": "Diffusion Models have shown remarkable performance in image generation tasks, which are capable of generating diverse and realistic image content. When adopting diffusion models for image restoration, the crucial challenge lies in how to preserve high-level image fidelity in the randomness diffusion process and generate accurate background structures and realistic texture details. In this paper, we propose a general framework and develop a Diffusion Texture Prior Model (DTPM) for image restoration tasks...", "authors": ["Ye Tian", "Sixiang Chen", "Wenhao Chai", "Zhaohu Xing", "Jing Qin", "Lin Ge", "Lei Zhu"], "year": 2024, "venue": "", "cited_by_count": 21, "type": "article", "concepts": ["Prior probability", "Artificial intelligence", "Image restoration", "Texture (cosmology)", "Computer science"], "search_query": "diffusion model image restoration generation", "score": 8.0, "subtopic": "Diffusion Restoration", "rationale": "DTPM explicitly models high-quality texture via diffusion for restoration tasks; general framework with conditional adapters for downstream tasks; directly relevant to diffusion priors in restoration.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "DTPM pretrained on 55K HQ images for texture; conditional guidance adapters + initial predictor for fast downstream adaptation", "key_findings": "General diffusion texture prior reduces randomness vs. traditional diffusion; effective for multiple restoration tasks", "limitations": "Requires two-phase training; initial predictor quality limits downstream performance"}, "bib_key": "Tian2024LearningDiffusi"}
{"openalex_id": "https://openalex.org/W4387076386", "doi": "https://doi.org/10.48550/arxiv.2309.14181", "title": "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision", "abstract": "The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models...", "authors": ["Haoning Wu", "Zicheng Zhang", "Erli Zhang"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 20, "type": "preprint", "concepts": ["Perception", "Computer science", "Benchmark (surveying)", "Construct (python library)", "Correctness"], "search_query": "visual language model image quality assessment", "score": 8.0, "subtopic": "Image Quality Assessment", "rationale": "Q-Bench holistically evaluates MLLM low-level vision abilities (perception, description, quality assessment) — core to VLM-based quality feedback in agent restoration pipelines.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Q-Bench: three evaluation dimensions (low-level perception QA, low-level description, IQA score alignment) on LLVisionQA, LLDescribe, and 7 IQA datasets for 15+ MLLMs.", "key_findings": "MLLMs have preliminary but unstable low-level visual skills; clear areas for enhancement identified.", "limitations": "Evaluation benchmark; no restoration component."}, "bib_key": "Wu2023A"}
{"openalex_id": "https://openalex.org/W4401726215", "doi": "https://doi.org/10.1109/tpami.2024.3445770", "title": "Q-Bench+: A Benchmark for Multi-Modal Foundation Models on Low-Level Vision From Single Images to Pairs", "abstract": "The rapid development of Multi-modality Large Language Models (MLLMs) has navigated a paradigm shift in computer vision...", "authors": ["Zicheng Zhang", "Haoning Wu", "Erli Zhang", "Guangtao Zhai", "Weisi Lin"], "year": 2024, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "cited_by_count": 17, "type": "article", "concepts": ["Modal", "Artificial intelligence", "Benchmark (surveying)", "Computer science", "Foundation (evidence)"], "search_query": "visual language model image quality assessment", "score": 8.0, "subtopic": "Image Quality Assessment", "rationale": "Q-Bench+ extends MLLM low-level vision evaluation to image pairs — directly relevant to VLM-based IQA component comparing restored vs degraded images.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Extends Q-Bench to pairwise: LLVisionQA+ (2990 single + 1999 pairs), LLDescribe+ (499 single + 450 pairs), IQA score evaluation on 7 datasets with 24 MLLMs.", "key_findings": "Only GPT-4V shows higher pairwise accuracy vs single-image (like humans); identifies clear capability gaps.", "limitations": "Benchmark only; no restoration integration."}, "bib_key": "Zhang2024A"}
{"openalex_id": "https://openalex.org/W4379089715", "doi": "https://doi.org/10.48550/arxiv.2305.20049", "title": "A Unified Conditional Framework for Diffusion-based Image Restoration", "abstract": "Diffusion Probabilistic Models (DPMs) have recently shown remarkable performance in image generation tasks, which are capable of generating highly realistic images. When adopting DPMs for image restoration tasks, the crucial aspect lies in how to integrate the conditional information to guide the DPMs to generate accurate and natural output, which has been largely overlooked in existing works. In this paper, we present a unified conditional framework based on diffusion models for image restoration...", "authors": ["Yi Zhang", "Xiaoyu Shi", "Dasong Li", "Xiaogang Wang", "Jian Wang", "Hongsheng Li"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 11, "type": "preprint", "concepts": ["Computer science", "Deblurring", "Leverage (statistics)", "Image restoration", "JPEG"], "search_query": "diffusion model image restoration generation", "score": 8.0, "subtopic": "Diffusion Restoration", "rationale": "Unified conditional diffusion framework for IR with spatially-adaptive generation conditioning; integrates initial prediction guidance into diffusion blocks; handles extreme low-light, deblurring, JPEG restoration.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Lightweight UNet for initial guidance + diffusion model for residual; spatial guidance integration into every diffusion block; inter-step patch-splitting for high-resolution", "key_findings": "Significant perceptual quality improvements on low-light, deblurring, JPEG restoration; generalizes across tasks", "limitations": "Preprint; tested on only 3 tasks; patch-splitting may cause edge artifacts"}, "bib_key": "Zhang2023AUnified"}
{"openalex_id": "https://openalex.org/W4402851724", "doi": "https://doi.org/10.1016/j.knosys.2024.112543", "title": "RDM-IR: Task-adaptive deep unfolding network for All-In-One image restoration", "abstract": "", "authors": ["Yuanshuo Cheng", "Mingwen Shao", "Yecong Wan", "Chao Wang"], "year": 2024, "venue": "Knowledge-Based Systems", "cited_by_count": 5, "type": "article", "concepts": ["RDM", "Task (project management)", "Image restoration", "Image (mathematics)", "Computer science"], "search_query": "all-in-one image restoration unified model degradation", "score": 8.0, "subtopic": "All-in-One Restoration", "rationale": "Task-adaptive deep unfolding network for all-in-one image restoration — directly relevant to unified image restoration with task-adaptive mechanism.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Deep unfolding network with task-adaptive modules for all-in-one restoration", "key_findings": "RDM-IR adapts to different degradation tasks via task-specific parameters in unfolding framework", "limitations": "Limited abstract available; deep unfolding has interpretability but limited flexibility"}, "bib_key": "Cheng2024"}
{"openalex_id": "https://openalex.org/W4405488387", "doi": "https://doi.org/10.1109/tcsvt.2024.3519352", "title": "IPT-ILR: Image Pyramid Transformer Coupled With Information Loss Regularization for All-in-One Image Restoration", "abstract": "All-in-one image restoration has recently developed to be a new research trend in the low-level computer vision field, aiming to tackle multiple image degradation types simultaneously in a unified model.", "authors": ["Sai Yang", "Bin Hu", "Fan Liu", "Xiaoxin Wu", "Weiping Ding", "Jun Zhou"], "year": 2024, "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "cited_by_count": 4, "type": "article", "concepts": ["Image restoration", "Artificial intelligence", "Computer vision", "Computer science", "Regularization (linguistics)"], "search_query": "all-in-one image restoration unified model degradation", "score": 8.0, "subtopic": "All-in-One Restoration", "rationale": "Image Pyramid Transformer with Information Loss Regularization for all-in-one restoration — directly relevant as Transformer-based unified restoration model.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Multi-scale image pyramid transformer with ILR weighting strategy for 6-task all-in-one restoration", "key_findings": "Cross-scale reference and task-weighted regularization improve performance across denoising, deblurring, dehazing, deraining, desnowing, low-light enhancement", "limitations": "Multi-scale processing increases computational cost"}, "bib_key": "Yang2024Image"}
{"openalex_id": "https://openalex.org/W4407677384", "doi": "https://doi.org/10.1016/j.engappai.2025.110267", "title": "Adaptive prompt guided unified image restoration with latent diffusion model", "authors": ["Xiang Lv", "Mingwen Shao", "Yecong Wan", "Yuanjian Qiao", "Changzhong Wang"], "year": 2025, "venue": "Engineering Applications of Artificial Intelligence", "cited_by_count": 4, "type": "article", "concepts": ["Computer science", "Diffusion", "Image (mathematics)", "Image restoration", "Artificial intelligence"], "search_query": "prompt-based image restoration learning degradation", "score": 8, "subtopic": "Diffusion Restoration", "rationale": "Adaptive prompt-guided unified image restoration using latent diffusion model. Combines diffusion-based restoration with adaptive prompts for multiple degradation types.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Adaptive prompts guide latent diffusion model for unified multi-degradation image restoration", "key_findings": "Effective unified restoration using diffusion priors with adaptive prompt guidance", "limitations": "Abstract unavailable; diffusion inference typically slow"}, "bib_key": "Lv2025AdaptivePrompt"}
{"openalex_id": "https://openalex.org/W4411047697", "doi": "https://doi.org/10.1016/j.patcog.2025.111875", "title": "AdaPrompt-IR: Adaptive learning to perceive degradation semantic and prompting for all-in-one image restoration", "authors": ["Wei Sun", "Qianzhou Wang", "Yaqi Wang", "Zhiqiang Hou", "Qingsen Yan", "Shuicheng Yan"], "year": 2025, "venue": "Pattern Recognition", "cited_by_count": 3, "type": "article", "concepts": ["Degradation (telecommunications)", "Computer science", "Artificial intelligence", "Image (mathematics)", "Image restoration"], "search_query": "prompt-based image restoration learning degradation", "score": 8, "subtopic": "All-in-One Restoration", "rationale": "Adaptive prompt-based all-in-one image restoration that perceives degradation semantics for dynamic guidance. Very relevant to unified restoration with semantic awareness.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Adaptive degradation semantic perception + prompt generation for all-in-one restoration", "key_findings": "Semantically adaptive prompts improve restoration across diverse degradation types", "limitations": "Abstract unavailable; details limited"}, "bib_key": "Sun2025Adaptive"}
{"openalex_id": "https://openalex.org/W4415535463", "doi": "https://doi.org/10.1145/3746027.3758196", "title": "WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration", "abstract": "Existing all-in-one image restoration approaches, which aim to handle multiple weather degradations within a single framework, are predominantly trained and evaluated using mixed single-weather synthetic datasets.", "authors": ["Qiyuan Guan", "Qianfeng Yang", "Xiang Chen", "Tianyu Song", "Guiyue Jin", "Jiyu Jin"], "year": 2025, "venue": "", "cited_by_count": 2, "type": "article", "concepts": [], "search_query": "all-in-one image restoration unified model degradation", "score": 8.0, "subtopic": "Real-World Degradation", "rationale": "Real-world benchmark dataset for all-in-one adverse weather restoration — directly relevant to evaluation of unified restoration methods on real-world degradations.", "alignment": {"task": "high", "method": "medium", "modality": "high"}, "extraction": {"design": "WeatherBench: large-scale real-world dataset with aligned image pairs for rain, snow, haze across diverse scenes", "key_findings": "Reveals domain gap between synthetic and real datasets; benchmarks task-specific, task-general, and all-in-one methods", "limitations": "Limited to adverse weather degradations; no indoor/artificial degradations"}, "bib_key": "Guan2025A"}
{"openalex_id": "https://openalex.org/W4412488893", "doi": "https://doi.org/10.1109/tpami.2025.3589606", "title": "Re-Boosting Self-Collaboration Parallel Prompt GAN for Unsupervised Image Restoration", "abstract": "Deep learning methods have demonstrated state-of-the-art performance in image restoration, especially when trained on large-scale paired datasets. However, acquiring paired data in real-world scenarios poses a significant challenge. Unsupervised restoration approaches based on generative adversarial networks (GANs) offer a promising solution without requiring paired datasets. Yet, these GAN-based approaches struggle to surpass the performance of conventional unsupervised GAN-based frameworks without significantly modifying model structures or increasing the computational complexity. To address these issues, we propose a self-collaboration (SC) strategy for existing restoration models. This strategy utilizes information from the previous stage as feedback to guide subsequent stages, achieving significant performance improvement without increasing the framework's inference complexity. The SC strategy comprises a prompt learning (PL) module and a restorer ($Res$Res). It iteratively replaces the previous less powerful fixed restorer $\\overline{Res}$Res¯ in the PL module with a more powerful $Res$Res. The enhanced PL module generates better pseudo-degraded/clean image pairs, leading to a more powerful $Res$Res for the next iteration. Our SC can significantly improve the $Res$Res 's performance by over 1.5 dB without adding extra parameters or computational complexity during inference. Meanwhile, existing self-ensemble (SE) and our SC strategies enhance the performance of pre-trained restorers from different perspectives. As SE increases computational complexity during inference, we propose a re-boosting module to the SC (Reb-SC) to improve the SC strategy further by incorporating SE into SC without increasing inference time. This approach further enhances the restorer's performance by approximately 0.3 dB. Additionally, we present a baseline framework that includes parallel generative adversarial branches with complementary \"self-synthesis\" and \"unpaired-synthesis\" constraints, ensuring the effectiveness of the training framework. Extensive experimental results on restoration tasks demonstrate that the proposed model performs favorably against existing state-of-the-art unsupervised restoration methods.", "authors": ["Lin Xin", "Yuyan Zhou", "Jingtong Yue", "Chao Ren", "Kelvin C. K. Chan", "Qi Lu", "Ming-Hsuan Yang"], "year": 2025, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "cited_by_count": 2, "type": "article", "concepts": ["Boosting (machine learning)", "Inference", "Computer science", "Computational complexity theory", "Artificial intelligence"], "search_query": "prompt-based image restoration learning degradation", "score": 8, "subtopic": "All-in-One Restoration", "rationale": "Self-collaboration parallel prompt GAN for unsupervised image restoration; prompt-based multi-task restoration approach.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Parallel prompt GAN with self-collaboration for unsupervised restoration", "key_findings": "Achieves competitive results without paired training data", "limitations": "GAN-based may have stability issues"}, "bib_key": "Xin2025"}
{"openalex_id": "https://openalex.org/W4408345706", "doi": "https://doi.org/10.1109/icassp49660.2025.10887587", "title": "Soft Knowledge Distillation with Multi-Dimensional Cross-Net Attention for Image Restoration Models Compression", "abstract": "Transformer-based encoder-decoder models have achieved remarkable success in image-to-image transfer tasks, particularly in image restoration. However, their high computational complexity—manifested in elevated FLOPs and parameter counts—limits their application in real-world scenarios. Existing knowledge distillation methods in image restoration typically employ lightweight student models that directly mimic the intermediate features and reconstruction results of the teacher, overlooking the implicit attention relationships between them. To address this, we propose a Soft Knowledge Distillation (SKD) strategy that incorporates a Multi-dimensional Cross-net Attention (MCA) mechanism for compressing image restoration models...", "authors": ["Yongheng Zhang", "Danfeng Yan"], "year": 2025, "venue": "", "cited_by_count": 2, "type": "article", "concepts": ["Distillation", "Computer science", "Image (mathematics)", "Compression (physics)", "Image restoration"], "search_query": "knowledge distillation model compression image restoration", "score": 8.0, "subtopic": "Knowledge Distillation", "rationale": "Directly proposes KD strategy for compressing Transformer-based image restoration models, closely aligned with lightweight deployment research direction.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "SKD with Multi-dimensional Cross-net Attention (MCA) bridging student-teacher interactions in channel and spatial dimensions; Gaussian kernel feature distance; contrastive learning loss", "key_findings": "Significant FLOPs and parameter reduction while maintaining strong restoration performance on deraining, deblurring, denoising", "limitations": "Limited to three tasks; no comparison with trajectory distillation methods"}, "bib_key": "Zhang2025SoftKnowledge"}
{"openalex_id": "https://openalex.org/W4408352428", "doi": "https://doi.org/10.1109/icassp49660.2025.10889105", "title": "Knowledge Distillation for Image Restoration: Simultaneous Learning from Degraded and Clean Images", "abstract": "Model compression through knowledge distillation has seen extensive application in classification and segmentation tasks. However, its potential in image-to-image translation, particularly in image restoration, remains underexplored. To address this gap, we propose a Simultaneous Learning Knowledge Distillation (SLKD) framework tailored for model compression in image restoration tasks. SLKD employs a dual-teacher, single-student architecture with two distinct learning strategies: Degradation Removal Learning (DRL) and Image Reconstruction Learning (IRL), simultaneously...", "authors": ["Yongheng Zhang", "Danfeng Yan"], "year": 2025, "venue": "", "cited_by_count": 2, "type": "article", "concepts": ["Distillation", "Computer science", "Image restoration", "Image (mathematics)", "Artificial intelligence"], "search_query": "knowledge distillation model compression image restoration", "score": 8.0, "subtopic": "Knowledge Distillation", "rationale": "Directly proposes a dual-teacher KD framework for image restoration, achieving 80%+ FLOPs/parameter reduction. Highly relevant to lightweight deployment of restoration pipelines.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Dual-teacher single-student SLKD: Teacher A guides degradation removal (BRISQUE extractor), Teacher B guides image reconstruction (PIQE extractor)", "key_findings": "Exceeds 80% reduction in FLOPs/params across 5 datasets and 3 tasks while maintaining strong restoration performance", "limitations": "Evaluated on three tasks only; BRISQUE/PIQE extractors may not generalize to all degradations"}, "bib_key": "Zhang2025KnowledgeDistil"}
{"openalex_id": "https://openalex.org/W4413156075", "doi": "https://doi.org/10.1109/cvpr52734.2025.01191", "title": "Visual-Instructed Degradation Diffusion for All-in-One Image Restoration", "abstract": "Image restoration tasks like deblurring, denoising, and dehazing usually need distinct models for each degradation type, restricting their generalization in real-world scenarios with mixed or unknown degradations. In this work, we propose Defusion, a novel all-in-one image restoration framework that utilizes visual instruction-guided degradation diffusion. Unlike existing methods that rely on task-specific models or ambiguous text-based priors, Defusion constructs explicit visual instructions that align with the visual degradation patterns.", "authors": ["Wenbo Luo", "Haina Qin", "Zewen Chen", "Libin Wang", "Dandan Zheng", "Yuming Li", "Yufan Liu", "Bing Li", "Weiming Hu"], "year": 2025, "venue": "", "cited_by_count": 2, "type": "article", "concepts": ["Degradation (telecommunications)", "Image restoration", "Computer science", "Diffusion", "Computer vision"], "search_query": "real world image degradation mixed complex restoration", "score": 8.0, "subtopic": "All-in-One Restoration", "rationale": "Visual-instruction guided diffusion for all-in-one restoration with mixed/unknown degradations; directly addresses core themes of visual instruction, diffusion-based all-in-one restoration.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Defusion: visual instructions constructed by applying degradations to standardized visual elements; diffusion model operates in degradation space", "key_findings": "Outperforms SOTA across diverse restoration tasks including complex/real-world degradations", "limitations": "Visual instruction construction may not cover all real-world degradation types"}, "bib_key": "Luo2025Degradation"}
{"openalex_id": "https://openalex.org/W4417248878", "doi": "https://doi.org/10.1109/tpami.2025.3642852", "title": "Beyond Degradation Redundancy: Contrastive Prompt Learning for All-in-One Image Restoration", "authors": ["Gang Wu", "Junjun Jiang", "Kui Jiang", "Xianming Liu", "Liqiang Nie"], "year": 2025, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "cited_by_count": 1, "type": "article", "concepts": [], "search_query": "all-in-one image restoration unified model degradation", "score": 8, "subtopic": "All-in-One Restoration", "rationale": "Directly addresses all-in-one image restoration with contrastive prompt learning to reduce degradation redundancy, published in TPAMI. Highly relevant to the unified model track.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "CPL framework: Sparse Prompt Module (SPM) + Contrastive Prompt Regularization (CPR) to strengthen task boundaries", "key_findings": "Achieves state-of-the-art on 5 benchmarks in multi-task and composite degradation settings", "limitations": "Focuses on known degradation types; not agent-based"}, "bib_key": "Wu2025BeyondDegradati"}
{"openalex_id": "https://openalex.org/W4409367756", "doi": "https://doi.org/10.1609/aaai.v39i5.32587", "title": "UP-Restorer: When Unrolling Meets Prompts for Unified Image Restoration", "authors": ["Minghao Liu", "Wenhan Yang", "Jinyi Luo", "Jiaying Liu"], "year": 2025, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 1, "type": "article", "concepts": ["Image (mathematics)", "Computer science", "Psychology", "Artificial intelligence"], "search_query": "prompt-based image restoration learning degradation", "score": 8, "subtopic": "All-in-One Restoration", "rationale": "Unrolling optimization meets prompt learning for all-in-one image restoration. Novel physics-inspired approach combining MAP optimization unrolling with diffusion-based prompt guidance.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Unrolling MAP optimization into score-based diffusion model with dynamically generated prompts for transmission/atmospheric/noise maps", "key_findings": "Significant improvements across multiple restoration tasks; physics-motivated design", "limitations": "Complex architecture combining optimization unrolling with diffusion"}, "bib_key": "Liu2025When"}
{"openalex_id": "https://openalex.org/W4411445611", "doi": "https://doi.org/10.2139/ssrn.5311960", "title": "Multi-Dimension Visual Prompt Enhanced Image Restoration Network Via Mamba-Transformer Aggregation", "abstract": "", "authors": ["Aiwen Jiang", "Hourong Chen", "Jihua Ye", "Mingwen Wang", "Bo Liu"], "year": 2025, "venue": "SSRN Electronic Journal", "cited_by_count": 1, "type": "preprint", "concepts": ["Transformer", "Dimension (graph theory)", "Image (mathematics)", "Computer science", "Mathematics"], "search_query": "prompt-based image restoration learning degradation", "score": 8, "subtopic": "All-in-One Restoration", "rationale": "Prompt-based image restoration using Mamba-Transformer aggregation, directly relevant to unified/prompt-based restoration.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Mamba-Transformer aggregation with multi-dimension visual prompts", "key_findings": "Enhanced image restoration via visual prompts", "limitations": "SSRN preprint, limited venue prestige"}, "bib_key": "Jiang2025Visual"}
{"openalex_id": "https://openalex.org/W4413013541", "doi": "https://doi.org/10.1038/s41598-025-14581-0", "title": "Real-world super-resolution with VLM-based degradation prior learning", "abstract": "", "authors": ["Xiaxu Chen", "De-kang Mao", "Jun Ke"], "year": 2025, "venue": "Scientific Reports", "cited_by_count": 1, "type": "article", "concepts": ["Degradation (telecommunications)", "Computer science", "Resolution (logic)", "Artificial intelligence", "Telecommunications"], "search_query": "real world image degradation mixed complex restoration", "score": 8.0, "subtopic": "Real-World Degradation", "rationale": "VLM-based degradation prior learning for real-world SR; directly aligns with VLM use for degradation analysis in restoration pipeline.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "VLM used to learn degradation priors for real-world super-resolution", "key_findings": "N/A (no abstract)", "limitations": "SR task only; no abstract details available"}, "bib_key": "Chen2025"}
{"openalex_id": "https://openalex.org/W4413776565", "doi": "https://doi.org/10.2139/ssrn.5407564", "title": "Image Restoration Model Compression via Mamba-oriented Heterogeneous Knowledge Distillation", "abstract": "", "authors": ["Sai Yang", "Bin Hu", "Xiaoxin Wu", "Fan Liu", "Jun Zhou"], "year": 2025, "venue": "SSRN Electronic Journal", "cited_by_count": 0, "type": "preprint", "concepts": ["Image (mathematics)", "Compression (physics)", "Distillation", "Computer science", "Artificial intelligence"], "search_query": "knowledge distillation model compression image restoration", "score": 8.0, "subtopic": "Knowledge Distillation", "rationale": "Proposes Mamba-based heterogeneous KD for image restoration model compression, directly relevant to lightweight deployment of restoration models.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Mamba-oriented heterogeneous knowledge distillation for compressing image restoration models", "key_findings": "Effective model compression while preserving restoration quality using Mamba architecture", "limitations": "Preprint with no citations yet; details limited in abstract"}, "bib_key": "Yang2025ImageRestoratio"}
{"openalex_id": "https://openalex.org/W4312812783", "doi": "https://doi.org/10.1109/cvpr52688.2022.01716", "title": "Uformer: A General U-Shaped Transformer for Image Restoration", "abstract": "In this paper, we present Uformer, an effective and efficient Transformer-based architecture for image restoration, in which we build a hierarchical encoder-decoder network using the Transformer block. In Uformer, there are two core designs. First, we introduce a novel locally-enhanced window (LeWin) Transformer block, which performs non-overlapping window-based self-attention instead of global self-attention. It significantly reduces the computational complexity on high resolution feature map while capturing local context. Second, we propose a learnable multi-scale restoration modulator in the form of a multi-scale spatial bias to adjust features in multiple layers of the Uformer decoder. Our modulator demonstrates superior capability for restoring details for various image restoration tasks while introducing marginal extra parameters and computational cost. Powered by these two designs, Uformer enjoys a high capability for capturing both local and global dependencies for image restoration. To evaluate our approach, extensive experiments are conducted on several image restoration tasks, including image denoising, motion deblurring, defocus deblurring and deraining. Without bells and whistles, our Uformer achieves superior or comparable performance compared with the state-of-the-art algorithms. The code and models are available at https://github.com/ZhendongWang6/Uformer.", "authors": ["Zhendong Wang", "Xiaodong Cun", "Jianmin Bao", "Wengang Zhou", "Jianzhuang Liu", "Houqiang Li"], "year": 2022, "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "cited_by_count": 1866, "type": "article", "concepts": ["Deblurring", "Image restoration", "Computer science", "Transformer", "Encoder"], "search_query": "image restoration transformer architecture efficient", "score": 7.5, "subtopic": "Transformer Restoration", "rationale": "U-shaped transformer for image restoration with window-based self-attention; important baseline architecture.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Hierarchical encoder-decoder with learnable multi-scale modulator", "key_findings": "Effective U-shaped transformer design for multiple restoration tasks", "limitations": "Window-based attention limits global context"}, "bib_key": "Wang2022A"}
{"openalex_id": "https://openalex.org/W4393150006", "doi": "https://doi.org/10.1609/aaai.v38i2.27907", "title": "Omni-Kernel Network for Image Restoration", "abstract": "Image restoration aims to reconstruct a high-quality image from a degraded low-quality observation. Recently, Transformer models have achieved promising performance on image restoration tasks due to their powerful ability to model long-range dependencies. However, the quadratically growing complexity with respect to the input size makes them inapplicable to practical applications. In this paper, we develop an efficient convolutional network for image restoration by enhancing multi-scale representation learning. To this end, we propose an omni-kernel module that consists of three branches, i.e., global, large, and local branches, to learn global-to-local feature representations efficiently. Specifically, the global branch achieves a global perceptive field via the dual-domain channel attention and frequency-gated mechanism. Furthermore, to provide multi-grained receptive fields, the large branch is formulated via different shapes of depth-wise convolutions with unusually large kernel sizes. Moreover, we complement local information using a point-wise depth-wise convolution. Finally, the proposed network, dubbed OKNet, is established by inserting the omni-kernel module into the bottleneck position for efficiency. Extensive experiments demonstrate that our network achieves state-of-the-art performance on 11 benchmark datasets for three representative image restoration tasks, including image dehazing, image desnowing, and image defocus deblurring. The code is available at https://github.com/c-yn/OKNet.", "authors": ["Yuning Cui", "Wenqi Ren", "Alois Knoll"], "year": 2024, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 183, "type": "article", "concepts": ["Image restoration", "Kernel (algebra)", "Computer science", "Artificial intelligence", "Image (mathematics)"], "search_query": "image restoration transformer architecture efficient", "score": 7.5, "subtopic": "Transformer Restoration", "rationale": "Multi-scale omni-kernel network for image restoration with global-to-local processing; relevant spatial-aware design.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Omni-kernel module with global, large, and local branches for multi-scale features", "key_findings": "SOTA on dehazing, desnowing, and defocus deblurring on 11 benchmarks", "limitations": "CNN-based, no transformer component despite competing with them"}, "bib_key": "Cui2024Network"}
{"openalex_id": "https://openalex.org/W4406457474", "doi": "https://doi.org/10.1109/tcsvt.2025.3530090", "title": "VmambaIR: Visual State Space Model for Image Restoration", "abstract": "Image restoration is a critical task in low-level computer vision, aiming to restore high-quality images from degraded inputs. Various models, such as convolutional neural networks (CNNs), generative adversarial networks (GANs), transformers, and diffusion models (DMs), have been employed to address this problem with significant impact. However, CNNs have limitations in capturing long-range dependencies. DMs require large prior models and computationally intensive denoising steps. Transformers have powerful modeling capabilities but face challenges due to quadratic complexity with input image size. To tackle these challenges, we propose VmambaIR, one of the first works to introduce State Space Models (SSMs) with linear complexity into comprehensive image restoration tasks. Specifically, we utilize a Unet architecture to stack our proposed Omni Selective Scan (OSS) blocks, consisting of an OSS module and an Efficient Feed-Forward Network (EFFN). Our proposed omni selective scan mechanism overcomes the unidirectional modeling limitation of SSMs by efficiently modeling image information flows in all six directions to better exploit surrounding restoration information. Furthermore, we conducted a comprehensive evaluation of our VmambaIR across multiple image restoration tasks, including image deraining, single image super-resolution, and real-world image super-resolution. Extensive experimental results demonstrate that our proposed VmambaIR achieves state-of-the-art (SOTA) performance with much fewer computational resources and parameters. Our research highlights the potential of state space models as promising alternatives to the transformer and CNN architectures in serving as foundational frameworks for next-generation low-level visual tasks.", "authors": ["Yuan Shi", "Bin Xia", "Xiaoyu Jin", "Xing Wang", "Tianyu Zhao", "Xin Xia", "Xuefeng Xiao", "Wenming Yang"], "year": 2025, "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "cited_by_count": 93, "type": "article", "concepts": ["Image restoration", "Computer vision", "Computer science", "Artificial intelligence", "Image (mathematics)"], "search_query": "image restoration transformer architecture efficient", "score": 7.5, "subtopic": "Transformer Restoration", "rationale": "Mamba-based state space model for image restoration; emerging architecture for efficient restoration.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Visual state space model combining Mamba with image restoration", "key_findings": "Competitive with transformers at lower computational cost", "limitations": "Relatively new paradigm, less established"}, "bib_key": "Shi2025Visual"}
{"openalex_id": "https://openalex.org/W4393153879", "doi": "https://doi.org/10.1609/aaai.v38i6.28412", "title": "Learning from History: Task-agnostic Model Contrastive Learning for Image Restoration", "abstract": "Contrastive learning has emerged as a prevailing paradigm for high-level vision tasks, which, by introducing properly negative samples, has also been exploited for low-level vision tasks to achieve a compact optimization space to account for their ill-posed nature. However, existing methods rely on manually predefined and task-oriented negatives, which often exhibit pronounced task-specific biases. To address this challenge, our paper introduces an innovative method termed 'learning from history', which dynamically generates negative samples from the target model itself. Our approach, named Model Contrastive Learning for Image Restoration (MCLIR), rejuvenates latency models as negative models, making it compatible with diverse image restoration tasks. We propose the Self-Prior guided Negative loss (SPN) to enable it. This approach significantly enhances existing models when retrained with the proposed model contrastive paradigm. The results show significant improvements in image restoration across various tasks and architectures. For example, models retrained with SPN outperform the original FFANet and DehazeFormer by 3.41 and 0.57 dB on the RESIDE indoor dataset for image dehazing. Similarly, they achieve notable improvements of 0.47 dB on SPA-Data over IDT for image deraining and 0.12 dB on Manga109 for a 4x scale super-resolution over lightweight SwinIR, respectively. Code and retrained models are available at https://github.com/Aitical/MCLIR.", "authors": ["Gang Wu", "Junjun Jiang", "Kui Jiang", "Xianming Liu"], "year": 2024, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 11, "type": "article", "concepts": ["Task (project management)", "Computer science", "Artificial intelligence", "Image (mathematics)", "Image restoration"], "search_query": "image denoising deblurring dehazing deraining deep learning", "score": 7.5, "subtopic": "All-in-One Restoration", "rationale": "Task-agnostic contrastive learning for image restoration; relevant to unified restoration training strategies.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Contrastive learning framework that is task-agnostic for restoration", "key_findings": "Historical model contrastive learning improves multi-task restoration", "limitations": "Contrastive approach may increase training complexity"}, "bib_key": "Wu2024LearningFrom"}
{"openalex_id": "https://openalex.org/W4392884081", "doi": "https://doi.org/10.1016/j.jvcir.2024.104117", "title": "GoLDFormer: A global–local deformable window transformer for efficient image restoration", "abstract": "", "authors": ["Quan Chen", "Bolun Zheng", "Chenggang Yan", "Zunjie Zhu", "Tingyu Wang", "Greg Slabaugh", "Shanxin Yuan"], "year": 2024, "venue": "Journal of Visual Communication and Image Representation", "cited_by_count": 4, "type": "article", "concepts": ["Transformer", "Computer science", "Computation", "Window (computing)", "Computer vision"], "search_query": "image restoration transformer architecture efficient", "score": 7.5, "subtopic": "Transformer Restoration", "rationale": "Global-local deformable window transformer for image restoration; relevant spatial-aware architecture.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Deformable window transformer with global-local attention for restoration", "key_findings": "Efficient global-local context extraction via deformable windows", "limitations": "Limited details without abstract"}, "bib_key": "Chen2024A"}
{"openalex_id": "https://openalex.org/W4392642845", "doi": "https://doi.org/10.1109/access.2024.3375360", "title": "Decomformer: Decompose Self-Attention of Transformer for Efficient Image Restoration", "abstract": "A transformer architecture achieves outstanding performance in computer vision tasks based on the ability to capture long-range dependencies. However, a quadratic increase in complexity with respect to spatial resolution makes it impractical to apply for image restoration tasks. In this paper, we propose a Decomformer that efficiently captures global relationship by decomposing self-attention into linear combination of vectors and coefficients to reduce the heavy computational cost. This approximation not only reduces the complexity linearly, but also preserves the globality of the vanilla self-attention properly. Moreover, we apply a linear simple gate to represent the complex self-attention mechanism as the proposed decomposition directly. To show the effectiveness of our approach, we apply it to image restoration tasks including denoising, deblurring and deraining. The proposed decomposing scheme for self-attention in the Transformer achieves better or comparable results with state-of-the-arts as well as much more efficiency than most of previous approaches.", "authors": ["Eunho Lee", "Youngbae Hwang"], "year": 2024, "venue": "IEEE Access", "cited_by_count": 3, "type": "article", "concepts": ["Deblurring", "Computer science", "Image restoration", "Transformer", "Globality"], "search_query": "image restoration transformer architecture efficient", "score": 7.5, "subtopic": "Transformer Restoration", "rationale": "Efficient transformer via self-attention decomposition for image restoration; relevant architecture optimization.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Decomposed self-attention reducing quadratic complexity for restoration", "key_findings": "Efficient attention mechanism maintaining restoration quality", "limitations": "Published in IEEE Access, lower venue impact"}, "bib_key": "Lee2024Decompose"}
{"openalex_id": "https://openalex.org/W4404275081", "doi": "https://doi.org/10.2139/ssrn.5018208", "title": "Joint Multi-Dimensional Dynamic Attention and Transformer for General Image Restoration", "abstract": "", "authors": ["Huan Zhang", "Xu Zhang", "Nian Cai", "Jianglei Di", "Yun Zhang"], "year": 2024, "venue": "SSRN Electronic Journal", "cited_by_count": 2, "type": "preprint", "concepts": ["Joint (building)", "Transformer", "Image restoration", "Computer science", "Computer vision"], "search_query": "image restoration transformer architecture efficient", "score": 7.5, "subtopic": "Transformer Restoration", "rationale": "Multi-dimensional dynamic attention transformer for general image restoration; relevant unified architecture.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Joint multi-dimensional dynamic attention with transformer for general restoration", "key_findings": "Dynamic attention adapts to different restoration tasks", "limitations": "SSRN preprint"}, "bib_key": "Zhang2024Joint"}
{"openalex_id": "https://openalex.org/W2754213847", "doi": "https://doi.org/10.1109/tip.2018.2831899", "title": "NIMA: Neural Image Assessment", "abstract": "Automatically learned quality assessment for images has recently become a hot topic due to its usefulness in a wide variety of applications such as evaluating image capture pipelines, storage techniques and sharing media. Despite the subjective nature of this problem, most existing methods only predict the mean opinion score provided by datasets such as AVA and TID2013. Our approach differs from others in that we predict the distribution of human opinion scores using a convolutional neural network...", "authors": ["Hossein Talebi", "Peyman Milanfar"], "year": 2018, "venue": "IEEE Transactions on Image Processing", "cited_by_count": 876, "type": "article", "concepts": [], "search_query": "image quality assessment no-reference blind deep learning", "score": 7.0, "subtopic": "Image Quality Assessment", "rationale": "NIMA is a landmark NR-IQA method predicting human opinion score distributions using CNNs, directly relevant as an IQA component for guiding or evaluating image restoration pipelines.", "alignment": {"task": "high", "method": "medium", "modality": "high"}, "extraction": {"design": "CNN predicts distribution of human opinion scores; no-reference, semantic/perceptual-aware", "key_findings": "High correlation with human perception; applicable to photo editing pipeline optimization", "limitations": "Trained on aesthetic datasets (AVA, TID2013); may not generalize to restoration-specific quality"}, "bib_key": "Talebi2018Neural"}
{"openalex_id": "https://openalex.org/W4386075642", "doi": "https://doi.org/10.1109/cvpr52729.2023.00570", "title": "Efficient Frequency Domain-based Transformers for High-Quality Image Deblurring", "abstract": "We present an effective and efficient method that explores the properties of Transformers in the frequency domain for high-quality image deblurring. Our method is motivated by the convolution theorem that the correlation or convolution of two signals in the spatial domain is equivalent to an element-wise product of them in the frequency domain. This inspires us to develop an efficient frequency domain-based self-attention solver (FSAS) to estimate the scaled dot-product attention by an element-wise product operation instead of the matrix multiplication in the spatial domain. In addition, we note that simply using the naive feed-forward network (FFN) in Transformers does not generate good deblurred results. To overcome this problem, we propose a simple yet effective discriminative frequency domain-based FFN (DFFN), where we introduce a gated mechanism in the FFN based on the Joint Photographic Experts Group (JPEG) compression algorithm to discriminatively determine which low- and high-frequency information of the features should be preserved for latent clear image restoration. We formulate the proposed FSAS and DFFN into an asymmetrical network based on an encoder and decoder architecture, where the FSAS is only used in the decoder module for better image deblurring. Experimental results show that the proposed method performs favorably against the state-of-the-art approaches.", "authors": ["Lingshun Kong", "Jiangxin Dong", "Jianjun Ge", "Mingqiang Li", "Jinshan Pan"], "year": 2023, "venue": "", "cited_by_count": 263, "type": "article", "concepts": ["Deblurring", "Computer science", "Frequency domain", "Transformer", "Image quality"], "search_query": "image restoration transformer architecture efficient", "score": 7, "subtopic": "Transformer Restoration", "rationale": "Frequency domain transformer for image deblurring; relevant efficient transformer design for restoration.", "alignment": {"task": "medium", "method": "high", "modality": "high"}, "extraction": {"design": "Frequency domain transformer leveraging convolution theorem", "key_findings": "Efficient deblurring via frequency domain processing", "limitations": "Focused on deblurring only"}, "bib_key": "Kong2023EfficientFreque"}
{"openalex_id": "https://openalex.org/W4385667669", "doi": "https://doi.org/10.1007/s11263-023-01843-5", "title": "Pyramid Attention Network for Image Restoration", "abstract": "", "authors": ["Yiqun Mei", "Yuchen Fan", "Yulun Zhang", "Jiahui Yu", "Yuqian Zhou", "Ding Liu", "Yun Fu", "Thomas S. Huang", "Humphrey Shi"], "year": 2023, "venue": "International Journal of Computer Vision", "cited_by_count": 112, "type": "article", "concepts": ["Artificial intelligence", "Computer science", "Pyramid (geometry)", "Computer vision", "Image restoration"], "search_query": "image denoising deblurring dehazing deraining deep learning", "score": 7, "subtopic": "Transformer Restoration", "rationale": "Pyramid attention network for image restoration; relevant multi-scale attention architecture.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Multi-scale pyramid attention for image restoration", "key_findings": "Effective multi-scale feature aggregation for restoration", "limitations": "Older architecture design"}, "bib_key": "Mei2023PyramidAttentio"}
{"openalex_id": "https://openalex.org/W4321497943", "doi": "https://doi.org/10.3390/s23052385", "title": "Vision Transformers in Image Restoration: A Survey", "abstract": "The Vision Transformer (ViT) architecture has been remarkably successful in image restoration. For a while, Convolutional Neural Networks (CNN) predominated in most computer vision tasks. Now, both CNN and ViT are efficient approaches that demonstrate powerful capabilities to restore a better version of an image given in a low-quality format. In this study, the efficiency of ViT in image restoration is studied extensively. The ViT architectures are classified for every task of image restoration. Seven image restoration tasks are considered: Image Super-Resolution, Image Denoising, General Image Enhancement, JPEG Compression Artifact Reduction, Image Deblurring, Removing Adverse Weather Conditions, and Image Dehazing. The outcomes, the advantages, the limitations, and the possible areas for future research are detailed. Overall, it is noted that incorporating ViT in the new architectures for image restoration is becoming a rule. This is due to some advantages compared to CNN, such as better efficiency, especially when more data are fed to the network, robustness in feature extraction, and a better feature learning approach that sees better the variances and characteristics of the input. Nevertheless, some drawbacks exist, such as the need for more data to show the benefits of ViT over CNN, the increased computational cost due to the complexity of the self-attention block, a more challenging training process, and the lack of interpretability. These drawbacks represent the future research direction that should be targeted to increase the efficiency of ViT in the image restoration domain.", "authors": ["Anas M. Ali", "Bilel Benjdira", "Anis Koubâa", "Walid El‐Shafai", "Zahid Khan", "Wadii Boulila"], "year": 2023, "venue": "Sensors", "cited_by_count": 109, "type": "article", "concepts": ["Image restoration", "Deblurring", "Computer science", "Artificial intelligence", "Convolutional neural network"], "search_query": "image restoration transformer architecture efficient", "score": 7, "subtopic": "Transformer Restoration", "rationale": "Comprehensive survey of vision transformers in image restoration; useful overview of the field.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Survey covering ViT architectures for restoration tasks", "key_findings": "Systematic review of transformer-based restoration methods", "limitations": "Survey paper, no novel methods proposed"}, "bib_key": "Ali2023VisionTransform"}
{"openalex_id": "https://openalex.org/W4391272787", "doi": "https://doi.org/10.48550/arxiv.2401.14159", "title": "Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks", "abstract": "We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM)...", "authors": ["Tianhe Ren", "Shilong Liu", "Ailing Zeng"], "year": 2024, "venue": "arXiv (Cornell University)", "cited_by_count": 88, "type": "preprint", "concepts": ["Pipeline (software)", "Computer science", "Segmentation", "Benchmark (surveying)", "Artificial intelligence"], "search_query": "segment anything model SAM zero-shot segmentation", "score": 7.0, "subtopic": "Foundation Models Vision", "rationale": "Grounded SAM combines open-world detection + SAM segmentation via text prompts — directly relevant to spatial-aware restoration pipeline that detects and segments degraded regions by text description.", "alignment": {"task": "medium", "method": "high", "modality": "high"}, "extraction": {"design": "Pipeline combining Grounding DINO (open-set detection) + SAM for text-prompted detection and segmentation; extended with BLIP, Stable Diffusion, etc.", "key_findings": "Achieves 48.7 mAP on SegInW zero-shot benchmark; enables automatic annotation, controllable editing.", "limitations": "Not applied to restoration; requires text prompts for localisation."}, "bib_key": "Ren2024Grounded"}
{"openalex_id": "https://openalex.org/W4382449856", "doi": "https://doi.org/10.1609/aaai.v37i1.25111", "title": "Hybrid CNN-Transformer Feature Fusion for Single Image Deraining", "abstract": "Since rain streaks exhibit diverse geometric appearances and irregular overlapped phenomena, these complex characteristics challenge the design of an effective single image deraining model...", "authors": ["Xiang Chen", "Jinshan Pan", "Jiyang Lu", "Zhentao Fan", "Hao Li"], "year": 2023, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 66, "type": "article", "concepts": ["Computer science", "Transformer", "Artificial intelligence", "Feature (linguistics)", "Source code"], "search_query": "spatially varying degradation image restoration local", "score": 7.0, "subtopic": "Transformer Restoration", "rationale": "Hybrid CNN-Transformer for deraining with degradation-aware MoE emphasizing spatially-varying rain distribution features; relevant to both Transformer restoration and spatial-aware processing.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "HCT-FFN: CNN-stage with DaMoE modules for spatially-varying rain features + Transformer BaViT for global texture; interactive fusion between stages", "key_findings": "Effective rain removal with spatial adaptivity; competitive on benchmarks", "limitations": "Focused on deraining; does not generalize to multi-degradation all-in-one setting"}, "bib_key": "Chen2023Hybrid"}
{"openalex_id": "https://openalex.org/W4323022514", "doi": "https://doi.org/10.1109/access.2023.3250616", "title": "A Comprehensive Review of Deep Learning-Based Real-World Image Restoration", "abstract": "Real-world imagery does not always exhibit good visibility and clean content, but often suffers from various kinds of degradations (e.g., noise, blur, rain drops, fog, color distortion, etc.), which severely affect vision-driven tasks (e.g., image classification, target recognition, and tracking, etc.). Thus, restoring the true scene from such degraded images is of significance. In recent years, a large body of deep learning-based image processing works has been exploited due to the advances in deep neural networks. This paper aims to make a comprehensive review of real-world image restoration algorithms and beyond. More specifically, this review provides overviews of critical benchmark datasets, image quality assessment methods, and four major categories of deep learning-based image restoration methods, i.e., based on convolutional neural network (CNN), generative adversarial network (GAN), Transformer, and multi-layer perceptron (MLP). The paper highlights the latest developments and advances in each category of network architecture to provide an up-to-date overview. Moreover, the representative state-of-the-art image restoration methods are compared visually and numerically. Finally, for real-world image restoration, the current situations are objectively assessed, challenges are discussed, and future directions and trends are presented.", "authors": ["Lujun Zhai", "Yonghui Wang", "Suxia Cui", "Yu Zhou"], "year": 2023, "venue": "IEEE Access", "cited_by_count": 62, "type": "review", "concepts": ["Computer science", "Image restoration", "Artificial intelligence", "Convolutional neural network", "Deep learning"], "search_query": "image denoising deblurring dehazing deraining deep learning", "score": 7, "subtopic": "Real-World Degradation", "rationale": "Comprehensive review of DL-based real-world image restoration covering multiple degradation types.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Survey of deep learning methods for real-world degradations", "key_findings": "Systematic categorization of restoration approaches and degradation types", "limitations": "Review paper, no novel methods"}, "bib_key": "Zhai2023AComprehensive"}
{"openalex_id": "https://openalex.org/W4408235503", "doi": "https://doi.org/10.1109/tcsvt.2025.3549351", "title": "Efficient Image Enhancement With a Diffusion-Based Frequency Prior", "abstract": "Due to the lack of appropriate priors, generating the content of dark regions remains a challenge in low-light image enhancement tasks. Currently, diffusion models employ robust image generation capabilities for enhancing low-light images. However, diffusion models require multiple iterations at the image feature level to generate details and content, which limits the speed...", "authors": ["Qingsen Yan", "Tao Hu", "Peng Wu", "Duwei Dai", "Shuhang Gu", "Wei Dong", "Yanning Zhang"], "year": 2025, "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "cited_by_count": 56, "type": "article", "concepts": ["Image enhancement", "Computer science", "Image (mathematics)", "Artificial intelligence", "Computer vision"], "search_query": "diffusion model image restoration generation", "score": 7.0, "subtopic": "Diffusion Restoration", "rationale": "FPIE uses frequency domain constraints to learn compact priors for accelerating diffusion-based low-light enhancement; efficient diffusion approach for restoration.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Frequency prior generation network + image restoration network; joint training with frequency domain constraints; wavelet-based transformer for detail generation", "key_findings": "Accelerated inference via compact frequency priors; SOTA on low-light benchmarks with real-world generalization", "limitations": "Focused on low-light; frequency prior specificity may limit generalization"}, "bib_key": "Yan2025EfficientImage"}
{"openalex_id": "https://openalex.org/W3193517049", "doi": "https://doi.org/10.1109/iccv48922.2021.01008", "title": "Learning Conditional Knowledge Distillation for Degraded-Reference Image Quality Assessment", "abstract": "An important scenario for image quality assessment (IQA) is to evaluate image restoration (IR) algorithms...", "authors": ["Heliang Zheng", "Huan Yang", "Jianlong Fu", "Zheng-Jun Zha", "Jiebo Luo"], "year": 2021, "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "cited_by_count": 52, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Metric (unit)", "Image quality", "Quality (philosophy)"], "search_query": "image quality assessment no-reference blind deep learning", "score": 7.0, "subtopic": "Image Quality Assessment", "rationale": "DR-IQA explicitly for evaluating image restoration algorithms using degraded references — directly relevant to quality evaluation in restoration pipelines where clean references are unavailable.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "DR-IQA: knowledge distillation to transfer knowledge from pristine references to degraded-reference IQA for blind image restoration evaluation.", "key_findings": "DR-IQA achieves near full-reference IQA performance using only degraded images as references; provides differentiable metric for GAN-based restoration.", "limitations": "Requires degraded reference; not fully blind; evaluated on specific restoration tasks."}, "bib_key": "Zheng2021LearningConditi"}
{"openalex_id": "https://openalex.org/W4404654752", "doi": "https://doi.org/10.1007/978-3-031-72995-9_22", "title": "When Fast Fourier Transform Meets Transformer for Image Restoration", "abstract": "", "authors": ["Xingyu Jiang", "Xiuhui Zhang", "Ning Gao", "Yue Deng"], "year": 2024, "venue": "Lecture notes in computer science", "cited_by_count": 36, "type": "book-chapter", "concepts": ["Computer science", "Fourier transform", "Image restoration", "Transformer", "Computer vision"], "search_query": "image restoration transformer architecture efficient", "score": 7, "subtopic": "Transformer Restoration", "rationale": "Combining FFT with transformer for image restoration; relevant efficient architecture design.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "FFT-Transformer hybrid architecture for image restoration", "key_findings": "Frequency and spatial domain fusion improves restoration", "limitations": "Limited details without abstract"}, "bib_key": "Jiang2024WhenFast"}
{"openalex_id": "https://openalex.org/W4404536559", "doi": "https://doi.org/10.1007/978-3-031-72904-1_9", "title": "A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment", "abstract": "", "authors": ["Tianhe Wu", "Kede Ma", "Jie Liang", "Yujiu Yang", "Lei Zhang"], "year": 2024, "venue": "Lecture notes in computer science", "cited_by_count": 24, "type": "book-chapter", "concepts": ["Computer science", "Quality (philosophy)", "Natural language processing", "Image quality", "Artificial intelligence"], "search_query": "visual language model image quality assessment", "score": 7.0, "subtopic": "Image Quality Assessment", "rationale": "Comprehensive MLLM study for IQA — highly relevant to VLM-driven IQA in agent-based restoration pipelines.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Systematic evaluation of multiple MLLMs on diverse IQA tasks and datasets.", "key_findings": "MLLMs show promising low-level IQA capabilities; CLIP-based models particularly effective.", "limitations": "Evaluation study; no restoration integration."}, "bib_key": "Wu2024AComprehensive"}
{"openalex_id": "https://openalex.org/W3147474004", "doi": "https://doi.org/10.1109/tmm.2021.3068561", "title": "Motion Blur Removal With Quality Assessment Guidance", "abstract": "Non-uniform blind motion deblurring is a challenging yet fundamental task in the computer vision field...", "authors": ["Jichun Li", "Bo Yan", "Qing Lin", "Ang Li", "Chenxi Ma"], "year": 2021, "venue": "IEEE Transactions on Multimedia", "cited_by_count": 17, "type": "article", "concepts": ["Deblurring", "Computer science", "Artificial intelligence", "Residual", "Metric (unit)"], "search_query": "image quality assessment no-reference blind deep learning", "score": 7.0, "subtopic": "Image Quality Assessment", "rationale": "IQA-guided deblurring — directly relevant to using quality assessment to guide and improve restoration, bridging IQA and restoration themes.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Deep DEBLUR-IQA: non-reference IQA network for deblurred images guides deblurring network optimization; efficient RDB/LRB architecture.", "key_findings": "IQA-guided enhancement improves subjective deblurring quality while maintaining good PSNR; 50x faster than multi-scale CNN methods.", "limitations": "Motion deblurring only; IQA network needs training separately."}, "bib_key": "Li2021MotionBlur"}
{"openalex_id": "https://openalex.org/W4390873198", "doi": "https://doi.org/10.1109/iccv51070.2023.00672", "title": "Towards Authentic Face Restoration with Iterative Diffusion Models and Beyond", "abstract": "An authentic face restoration system is becoming increasingly demanding in many computer vision applications, e.g., image enhancement, video communication, and taking portrait. Most of the advanced face restoration models can recover high-quality faces from low-quality ones but usually fail to faithfully generate realistic and high-frequency details that are favored by users...", "authors": ["Yang Zhao", "Tingbo Hou", "Yu-Chuan Su", "Xuhui Jia", "Yandong Li", "Matthias Grundmann"], "year": 2023, "venue": "", "cited_by_count": 17, "type": "article", "concepts": ["Computer science", "Image restoration", "Face (sociological concept)", "Noise reduction", "Artificial intelligence"], "search_query": "diffusion model image restoration generation", "score": 7.0, "subtopic": "Diffusion Restoration", "rationale": "IDM iterative diffusion for face restoration with intrinsic+extrinsic iterative enhancement; demonstrates diffusion for domain-specific blind restoration.", "alignment": {"task": "high", "method": "high", "modality": "medium"}, "extraction": {"design": "Iterative DDM with intrinsic refinement + extrinsic enhancement; authentic restoration criterion defined; data cleaned by restoration improves generation", "key_findings": "Superior blind face restoration; authentic data helps image generation (FFHQ/ImageNet)", "limitations": "Domain-specific to faces; may not generalize to other degradation types"}, "bib_key": "Zhao2023TowardsAuthenti"}
{"openalex_id": "https://openalex.org/W4385804911", "doi": "https://doi.org/10.1109/cvprw59228.2023.00123", "title": "Unlimited-Size Diffusion Restoration", "abstract": "Recently, using diffusion models for zero-shot image restoration (IR) has become a new hot paradigm. This type of method only needs to use the pre-trained off-the-shelf diffusion models, without any finetuning, and can directly handle various IR tasks...", "authors": ["Yinhuai Wang", "Jiwen Yu", "Runyi Yu", "Jian Zhang"], "year": 2023, "venue": "", "cited_by_count": 15, "type": "article", "concepts": ["Computer science", "Diffusion", "Image restoration", "Image (mathematics)", "Range (aeronautics)"], "search_query": "diffusion model image restoration generation", "score": 7.0, "subtopic": "Diffusion Restoration", "rationale": "Zero-shot diffusion restoration for arbitrary-size images; Mask-Shift + Hierarchical Restoration methods for coherent large-image restoration; relevant to practical deployment.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Mask-Shift Restoration for local incoherence + Hierarchical Restoration for out-of-domain issues; parameter-free, works with pretrained diffusion models", "key_findings": "Enables zero-shot IR at unlimited image size; applicable to both restoration and unlimited-size generation", "limitations": "Relies on quality of pretrained diffusion models; may have coherency issues at large scales"}, "bib_key": "Wang2023Diffusion"}
{"openalex_id": "https://openalex.org/W4401506925", "doi": "https://doi.org/10.1109/tcsvt.2024.3441713", "title": "Image Intrinsic Components Guided Conditional Diffusion Model for Low-Light Image Enhancement", "abstract": "Through formulating the image restoration as a generation problem, the conditional diffusion model has been applied to low-light image enhancement (LIE) to restore the details in dark regions. However, in the previous diffusion model based LIE methods, the conditions used for guiding generation are degraded images...", "authors": ["Sicong Kang", "Shuaibo Gao", "Wenhui Wu", "Xu Wang", "Shuoyao Wang", "Guoping Qiu"], "year": 2024, "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "cited_by_count": 15, "type": "article", "concepts": ["Image enhancement", "Image (mathematics)", "Diffusion", "Image processing", "Computer science"], "search_query": "diffusion model image restoration generation", "score": 7.0, "subtopic": "Diffusion Restoration", "rationale": "ICCDiff uses Retinex intrinsic components (reflectance + illumination) as non-degraded conditions for diffusion-based low-light enhancement; addresses condition quality issue in diffusion-based restoration.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Retinex decomposition for reflectance/illumination extraction; component-dependent feature extraction; adaptive feature fusion into diffusion model (not rigid concatenation)", "key_findings": "Better local detail restoration and brightness enhancement vs. previous diffusion LIE methods", "limitations": "Retinex decomposition may fail in extreme low-light; focused on LIE only"}, "bib_key": "Kang2024ImageIntrinsic"}
{"openalex_id": "https://openalex.org/W4404844036", "doi": "https://doi.org/10.1016/j.patcog.2024.111223", "title": "FrePrompter: Frequency self-prompt for all-in-one image restoration", "authors": ["Zhijian Wu", "Wenhui Liu", "Jingchao Wang", "Jun Li", "Dingjiang Huang"], "year": 2024, "venue": "Pattern Recognition", "cited_by_count": 14, "type": "article", "concepts": ["Image restoration", "Image (mathematics)", "Artificial intelligence", "Computer vision", "Computer science"], "search_query": "prompt-based image restoration learning degradation", "score": 7, "subtopic": "All-in-One Restoration", "rationale": "All-in-one image restoration using frequency-domain self-prompting. Combines frequency analysis with prompt learning for unified restoration.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Frequency-based self-prompt generation for all-in-one restoration without explicit degradation labels", "key_findings": "Achieves good performance across multiple degradation types using frequency prompts", "limitations": "Abstract unavailable; limited details on composite degradation handling"}, "bib_key": "Wu2024Frequency"}
{"openalex_id": "https://openalex.org/W4401990587", "doi": "https://doi.org/10.1109/icmew63481.2024.10645451", "title": "Q-Boost: On Visual Quality Assessment Ability of Low-Level Multi-Modality Foundation Models", "abstract": "Recent advancements in Multi-modality Large Language Models (MLLMs) have demonstrated remarkable capabilities in complex high-level vision tasks...", "authors": ["Zicheng Zhang", "Haoning Wu"], "year": 2024, "venue": "", "cited_by_count": 13, "type": "article", "concepts": ["Modality (human-computer interaction)", "Foundation (evidence)", "Computer science", "Quality (philosophy)", "Artificial intelligence"], "search_query": "visual language model image quality assessment", "score": 7.0, "subtopic": "Image Quality Assessment", "rationale": "Q-Boost enhances MLLM IQA/VQA with triadic tone + multi-prompt ensemble — relevant to VLM-based quality feedback in restoration pipelines.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Q-Boost strategy: triadic tone integration (positive/neutral/negative prompts) + multi-prompt ensemble for MLLM IQA/VQA.", "key_findings": "Outstanding zero-shot performance on IQA/VQA tasks with Q-Boost strategy.", "limitations": "Quality assessment prompting strategy; no restoration integration."}, "bib_key": "Zhang2024On"}
{"openalex_id": "https://openalex.org/W4393972605", "doi": "https://doi.org/10.1109/tip.2024.3383776", "title": "DriftRec: Adapting Diffusion Models to Blind JPEG Restoration", "abstract": "In this work, we utilize the high-fidelity generation abilities of diffusion models to solve blind JPEG restoration at high compression levels. We propose an elegant modification of the forward stochastic differential equation of diffusion models to adapt them to this restoration task and name our method DriftRec...", "authors": ["Simon Welker", "Henry N. Chapman", "Timo Gerkmann"], "year": 2024, "venue": "IEEE Transactions on Image Processing", "cited_by_count": 13, "type": "article", "concepts": ["Computer science", "Image restoration", "JPEG", "Artificial intelligence", "Computer vision"], "search_query": "diffusion model image restoration generation", "score": 7.0, "subtopic": "Diffusion Restoration", "rationale": "DriftRec modifies diffusion SDE forward process to exploit clean/corrupted image proximity for JPEG restoration; elegant theoretical contribution for diffusion-based blind restoration.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Modified forward SDE exploiting proximity between clean and corrupted images; low added noise; few sampling steps; no knowledge of corruption operation needed", "key_findings": "Avoids blurry outputs; generalizes to double JPEG and online images without retraining", "limitations": "Specific to JPEG; SDE modification may not generalize to all degradation types"}, "bib_key": "Welker2024Adapting"}
{"openalex_id": "https://openalex.org/W4302013160", "doi": "https://doi.org/10.48550/arxiv.2210.01069", "title": "Dual-former: Hybrid Self-attention Transformer for Efficient Image Restoration", "abstract": "Recently, image restoration transformers have achieved comparable performance with previous state-of-the-art CNNs. However, how to efficiently leverage such architectures remains an open problem. In this work, we present Dual-former whose critical insight is to combine the powerful global modeling ability of self-attention modules and the local modeling ability of convolutions in an overall architecture. With convolution-based Local Feature Extraction modules equipped in the encoder and the decoder, we only adopt a novel Hybrid Transformer Block in the latent layer to model the long-distance dependence in spatial dimensions and handle the uneven distribution between channels. Such a design eliminates the substantial computational complexity in previous image restoration transformers and achieves superior performance on multiple image restoration tasks. Experiments demonstrate that Dual-former achieves a 1.91dB gain over the state-of-the-art MAXIM method on the Indoor dataset for single image dehazing while consuming only 4.2% GFLOPs as MAXIM. For single image deraining, it exceeds the SOTA method by 0.1dB PSNR on the average results of five datasets with only 21.5% GFLOPs. Dual-former also substantially surpasses the latest desnowing method on various datasets, with fewer parameters.", "authors": ["Sixiang Chen", "Ye Tian", "Yun Liu", "Erkang Chen"], "year": 2022, "venue": "arXiv (Cornell University)", "cited_by_count": 11, "type": "preprint", "concepts": ["FLOPS", "Computer science", "Transformer", "Encoder", "Leverage (statistics)"], "search_query": "image restoration transformer architecture efficient", "score": 7, "subtopic": "Transformer Restoration", "rationale": "Efficient hybrid self-attention transformer for image restoration; relevant architecture design.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Dual self-attention combining local and global attention efficiently", "key_findings": "Reduced computational cost while maintaining restoration quality", "limitations": "arXiv preprint"}, "bib_key": "Chen2022Hybrid"}
{"openalex_id": "https://openalex.org/W4372266706", "doi": "https://doi.org/10.1109/icassp49357.2023.10095242", "title": "Sandformer: CNN and Transformer under Gated Fusion for Sand Dust Image Restoration", "abstract": "Although Convolutional Neural Networks (CNN) have made good progress in image restoration, the intrinsic equivalence and locality of convolutions still constrain further improvements in image quality. Recent vision transformer and selfattention have achieved promising results on various computer vision tasks. However, directly utilizing Transformer for image restoration is a challenging task. In this paper, we introduce an effective hybrid architecture for sand image restoration tasks, which leverages local features from CNN and long-range dependencies captured by transformer to improve the results further. We propose an efficient hybrid structure for sand dust image restoration to solve the feature inconsistency issue between Transformer and CNN. The framework complements each representation by modulating features from the CNN-based and Transformer-based branches rather than simply adding or concatenating features. Experiments demonstrate that SandFormer achieves significant performance improvements in synthetic and real dust scenes compared to previous sand image restoration methods.", "authors": ["Jun Shi", "Bingcai Wei", "Gang Zhou", "Liye Zhang"], "year": 2023, "venue": "", "cited_by_count": 10, "type": "article", "concepts": ["Transformer", "Computer science", "Image restoration", "Convolutional neural network", "Locality"], "search_query": "image restoration transformer architecture efficient", "score": 7, "subtopic": "Transformer Restoration", "rationale": "CNN-Transformer fusion for sand dust image restoration; relevant hybrid architecture for restoration.", "alignment": {"task": "medium", "method": "high", "modality": "high"}, "extraction": {"design": "Gated fusion of CNN and Transformer branches for sand dust restoration", "key_findings": "Effective fusion strategy for degraded sand dust images", "limitations": "Single degradation type focus"}, "bib_key": "Shi2023Cnn"}
{"openalex_id": "https://openalex.org/W4402916064", "doi": "https://doi.org/10.1109/cvprw63382.2024.00658", "title": "Photo-Realistic Image Restoration in the Wild with Controlled Vision-Language Models", "abstract": "Though diffusion models have been successfully applied to various image restoration (IR) tasks, their performance is sensitive to the choice of training datasets. Typically, diffusion models trained in specific datasets fail to recover images that have out-of-distribution degradations. To address this problem, this work leverages a capable vision-language model and a synthetic degradation pipeline to learn image restoration in the wild (wild IR)...", "authors": ["Ziwei Luo", "Fredrik Gustafsson", "Zheng Zhao", "Jens Sjolund", "Thomas B. Schon"], "year": 2024, "venue": "", "cited_by_count": 10, "type": "article", "concepts": ["Computer science", "Computer vision", "Image restoration", "Artificial intelligence", "Image (mathematics)"], "search_query": "diffusion model image restoration generation", "score": 7.0, "subtopic": "Real-World Degradation", "rationale": "CLIP+diffusion (IR-SDE) for wild image restoration with synthetic degradation pipeline; VLM guides restoration for out-of-distribution degradations; relevant to VLM-guided restoration and real-world degradation.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Degradation-aware CLIP model + IR-SDE diffusion; synthetic degradation pipeline (blur, resize, noise, JPEG); posterior sampling for fast generation", "key_findings": "Improved out-of-distribution IR; posterior sampling accelerates generation; unified IR with multiple degradations", "limitations": "Sensitive to synthetic degradation pipeline quality; VLM guidance limited to content features"}, "bib_key": "Luo2024Image"}
{"openalex_id": "https://openalex.org/W4402716178", "doi": "https://doi.org/10.1109/cvpr52733.2024.00248", "title": "Restoration by Generation with Constrained Priors", "abstract": "The inherent generative power of denoising diffusion models makes them well-suited for image restoration tasks where the objective is to find the optimal high-quality image within the generative space that closely resembles the input image. We propose a method to adapt a pretrained diffusion model for image restoration by simply adding noise to the input image to be restored and then denoise...", "authors": ["Zheng Ding", "Xuaner Zhang", "Zhuowen Tu", "Zhihao Xia"], "year": 2024, "venue": "", "cited_by_count": 10, "type": "article", "concepts": ["Prior probability", "Computer science", "Mathematical optimization", "Artificial intelligence", "Mathematics"], "search_query": "diffusion model image restoration generation", "score": 7.0, "subtopic": "Diffusion Restoration", "rationale": "Restoration by generation with constrained space using anchor images; demonstrates personalized restoration with diffusion; relevant to diffusion-based blind restoration with generalization.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Pretrained diffusion finetuned with anchor images to constrain generative space; noise-then-denoise restoration; personalized restoration via personal album as anchors", "key_findings": "Preserves identity and high-frequency details better than prior work; practical personalized restoration application", "limitations": "Requires anchor images for best performance; finetuning cost per user"}, "bib_key": "Ding2024RestorationBy"}
{"openalex_id": "https://openalex.org/W4402727905", "doi": "https://doi.org/10.1109/cvpr52733.2024.00280", "title": "Boosting Image Restoration via Priors from Pre-Trained Models", "abstract": "Pre-trained models with large-scale training data, such as CLIP and Stable Diffusion, have demonstrated remarkable performance in various high-level computer vision tasks such as image understanding and generation from language descriptions. Yet, their potential for low-level tasks such as image restoration remains relatively unexplored. In this paper, we explore such models to enhance image restoration. As off-the-shelf features (OSF) from pre-trained models do not directly serve image restoration, we propose to learn an additional lightweight module called Pre-Train-Guided Refinement Module (PTG-RM) to refine restoration results of a target restoration network with OSF. PTG-RM consists of two components, Pre-Train-Guided Spatial-Varying Enhancement (PTG-SVE), and Pre-Train-Guided Channel-Spatial Attention (PTG-CSA). PTG-SVE enables optimal short- and long-range neural operations, while PTG-CSA enhances channel-spatial attention for restoration-related learning. Extensive experiments demonstrate that PTG-RM, with its compact size (<1M parameters), effectively enhances restoration performance of various models across different tasks, including low-light enhancement, deraining, deblurring, and denoising.", "authors": ["Xiaogang Xu", "Shu Kong", "Tao Hu", "Zhe Liu", "Hujun Bao"], "year": 2024, "venue": "", "cited_by_count": 10, "type": "article", "concepts": ["Boosting (machine learning)", "Prior probability", "Artificial intelligence", "Computer science", "Image restoration"], "search_query": "diffusion model image restoration generation", "score": 7.0, "subtopic": "Foundation Models Vision", "rationale": "Uses CLIP/Stable Diffusion priors for image restoration across multiple tasks; highly relevant to foundation model usage in restoration pipelines.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Lightweight PTG-RM module using CLIP/SD off-the-shelf features; spatial-varying enhancement + channel-spatial attention", "key_findings": "PTG-RM (<1M params) consistently boosts multiple restoration tasks", "limitations": "Dependent on pre-trained model quality; limited to enhancement rather than standalone restoration"}, "bib_key": "Xu2024BoostingImage"}
{"openalex_id": "https://openalex.org/W4361208495", "doi": "https://doi.org/10.3389/fpls.2023.1154176", "title": "All-in-one aerial image enhancement network for forest scenes", "abstract": "Drone monitoring plays an irreplaceable and significant role in forest firefighting due to its characteristics of wide-range observation and real-time messaging. However, aerial images are often susceptible to different degradation problems before performing high-level visual tasks including but not limited to smoke detection, fire classification, and regional localization. Recently, the majority of image enhancement methods are centered around particular types of degradation, necessitating the memory unit to accommodate different models for distinct scenarios in practical applications. Furthermore, such a paradigm requires wasted computational and storage resources to determine the type of degradation, making it difficult to meet the real-time and lightweight requirements of real-world scenarios. In this paper, we propose an All-in-one Image Enhancement Network (AIENet) that can restore various degraded images in one network. Specifically, we design a new multi-scale receptive field image enhancement block, which can better reconstruct high-resolution details of target regions of different sizes. In particular, this plug-and-play module enables it to be embedded in any learning-based model. And it has better flexibility and generalization in practical applications. This paper takes three challenging image enhancement tasks encountered in drone monitoring as examples, whereby we conduct task-specific and all-in-one image enhancement experiments on a synthetic forest dataset. The results show that the proposed AIENet outperforms the state-of-the-art image enhancement algorithms quantitatively and qualitatively. Furthermore, extra experiments on high-level vision detection also show the promising performance of our method compared with some recent baselines.", "authors": ["Zhaoqi Chen", "Chuansheng Wang", "Fuquan Zhang", "Ling Zhang", "Antoni Grau", "Edmundo Guerra"], "year": 2023, "venue": "Frontiers in Plant Science", "cited_by_count": 9, "type": "article", "concepts": ["Computer science", "Drone", "Aerial image", "Block (permutation group theory)", "Artificial intelligence"], "search_query": "image denoising deblurring dehazing deraining deep learning", "score": 7, "subtopic": "All-in-One Restoration", "rationale": "All-in-one enhancement for aerial forest images handling multiple degradations; relevant unified approach.", "alignment": {"task": "medium", "method": "medium", "modality": "high"}, "extraction": {"design": "Unified network for aerial image enhancement handling haze, rain, snow, low-light", "key_findings": "Multi-degradation handling in one model for drone imagery", "limitations": "Domain-specific to forest/aerial scenes"}, "bib_key": "Chen2023Aerial"}
{"openalex_id": "https://openalex.org/W4391417547", "doi": "https://doi.org/10.48550/arxiv.2401.15235", "title": "CascadedGaze: Efficiency in Global Context Extraction for Image Restoration", "abstract": "Image restoration tasks traditionally rely on convolutional neural networks. However, given the local nature of the convolutional operator, they struggle to capture global information. The promise of attention mechanisms in Transformers is to circumvent this problem, but it comes at the cost of intensive computational overhead. Many recent studies in image restoration have focused on solving the challenge of balancing performance and computational cost via Transformer variants. In this paper, we present CascadedGaze Network (CGNet), an encoder-decoder architecture that employs Global Context Extractor (GCE), a novel and efficient way to capture global information for image restoration. The GCE module leverages small kernels across convolutional layers to learn global dependencies, without requiring self-attention. Extensive experimental results show that our computationally efficient approach performs competitively to a range of state-of-the-art methods on synthetic image denoising and single image deblurring tasks, and pushes the performance boundary further on the real image denoising task.", "authors": ["Amirhosein Ghasemabadi", "Mohammad Salameh", "Muhammad Kamran Janjua", "Chunhua Zhou", "Fengyu Sun", "Di Niu"], "year": 2024, "venue": "arXiv (Cornell University)", "cited_by_count": 8, "type": "preprint", "concepts": ["Image restoration", "Context (archaeology)", "Extraction (chemistry)", "Image (mathematics)", "Computer science"], "search_query": "image restoration transformer architecture efficient", "score": 7, "subtopic": "Transformer Restoration", "rationale": "Efficient global context extraction for image restoration; relevant architecture design.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Cascaded gaze mechanism for efficient global context in restoration", "key_findings": "Improved efficiency in capturing global information", "limitations": "arXiv preprint"}, "bib_key": "Ghasemabadi2024Efficiency"}
{"openalex_id": "https://openalex.org/W4377971563", "doi": "https://doi.org/10.2139/ssrn.4458244", "title": "Towards an Effective and Efficient Transformer for Rain-by-Snow Weather Removal", "abstract": "", "authors": ["Tao Gao", "Yuanbo Wen", "Kaihao Zhang", "Peng Cheng", "Ting Chen"], "year": 2023, "venue": "SSRN Electronic Journal", "cited_by_count": 8, "type": "preprint", "concepts": ["Snow", "Rain and snow mixed", "Environmental science", "Meteorology", "Transformer"], "search_query": "image restoration transformer architecture efficient", "score": 7, "subtopic": "Transformer Restoration", "rationale": "Transformer for mixed rain-snow removal; relevant multi-weather restoration approach.", "alignment": {"task": "medium", "method": "high", "modality": "high"}, "extraction": {"design": "Efficient transformer for combined rain and snow removal", "key_findings": "Handles mixed weather degradation", "limitations": "SSRN preprint, specific to rain-snow mix"}, "bib_key": "Gao2023TowardsAn"}
{"openalex_id": "https://openalex.org/W4415178442", "doi": "https://doi.org/10.1109/tip.2025.3618377", "title": "UniUIR: Considering Underwater Image Restoration as an All-in-One Learner", "abstract": "Existing underwater image restoration (UIR) methods generally only handle color distortion or jointly address color and haze issues, but they often overlook the more complex degradations that can occur in underwater scenes. To address this limitation, we propose a Universal Underwater Image Restoration method, termed as UniUIR, considering the complex scenario of real-world underwater mixed distortions as an all-in-one manner.", "authors": ["Xu Zhang", "Huan Zhang", "Guoli Wang", "Qian Zhang", "Lefei Zhang", "Bo Du"], "year": 2025, "venue": "IEEE Transactions on Image Processing", "cited_by_count": 8, "type": "article", "concepts": [], "search_query": "real world image degradation mixed complex restoration", "score": 7.0, "subtopic": "All-in-One Restoration", "rationale": "All-in-one underwater image restoration with Mamba MoE, spatial-frequency prior generator, and depth information for region-level degradation; highly relevant to core themes.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "MMoEM (Mamba MoE) for degradation-specific experts + spatial-frequency prior generator + depth information for region-dependent distortions", "key_findings": "Strong generalization on underwater mixed degradation; SOTA on qualitative/quantitative metrics", "limitations": "Underwater domain specifically"}, "bib_key": "Zhang2025Considering"}
{"openalex_id": "https://openalex.org/W4321594307", "doi": "https://doi.org/10.48550/arxiv.2302.09554", "title": "Mixed Hierarchy Network for Image Restoration", "abstract": "Image restoration is a long-standing low-level vision problem, e.g., deblurring and deraining. In the process of image restoration, it is necessary to consider not only the spatial details and contextual information of restoration to ensure the quality, but also the system complexity. Although many methods have been able to guarantee the quality of image restoration, the system complexity of the state-of-the-art (SOTA) methods is increasing as well. Motivated by this, we present a mixed hierarchy network that can balance these competing goals. Our main proposal is a mixed hierarchy architecture, that progressively recovers contextual information and spatial details from degraded images while we design intra-blocks to reduce system complexity. Specifically, our model first learns the contextual information using encoder-decoder architectures, and then combines them with high-resolution branches that preserve spatial detail. In order to reduce the system complexity of this architecture for convenient analysis and comparison, we replace or remove the nonlinear activation function with multiplication and use a simple network structure. In addition, we replace spatial convolution with global self-attention for the middle block of encoder-decoder. The resulting tightly interlinked hierarchy architecture, named as MHNet, delivers strong performance gains on several image restoration tasks, including image deraining, and deblurring.", "authors": ["Hu Gao", "Depeng Dang"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 7, "type": "preprint", "concepts": ["Deblurring", "Computer science", "Image restoration", "Hierarchy", "Image (mathematics)"], "search_query": "image denoising deblurring dehazing deraining deep learning", "score": 7, "subtopic": "Transformer Restoration", "rationale": "Mixed hierarchy network combining spatial details and contextual information for image restoration.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Mixed hierarchy architecture balancing spatial detail and global context", "key_findings": "Effective multi-scale hierarchy for deblurring and deraining", "limitations": "arXiv preprint"}, "bib_key": "Gao2023MixedHierarchy"}
{"openalex_id": "https://openalex.org/W4404176632", "doi": "https://doi.org/10.1007/978-3-031-72855-6_11", "title": "UniProcessor: A Text-Induced Unified Low-Level Image Processor", "authors": ["Huiyu Duan", "Xiongkuo Min", "Sijing Wu", "Wei Shen", "Guangtao Zhai"], "year": 2024, "venue": "Lecture notes in computer science", "cited_by_count": 6, "type": "book-chapter", "concepts": ["Uniprocessor system", "Computer science", "Image (mathematics)", "Artificial intelligence", "Parallel computing"], "search_query": "prompt-based image restoration learning degradation", "score": 7, "subtopic": "All-in-One Restoration", "rationale": "Text-induced unified low-level image processor; uses text/language prompts to guide unified image processing, relevant to the VLM-guided restoration theme.", "alignment": {"task": "high", "method": "medium", "modality": "high"}, "extraction": {"design": "Text prompts induce unified low-level image processing across multiple tasks", "key_findings": "Unified text-guided approach handles multiple low-level vision tasks", "limitations": "Abstract unavailable; details limited"}, "bib_key": "Duan2024A"}
{"openalex_id": "https://openalex.org/W4402915827", "doi": "https://doi.org/10.1109/tcsvt.2024.3469190", "title": "Multi-Weather Restoration: An Efficient Prompt-Guided Convolution Architecture", "authors": ["Chengyang Li", "Fangwei Sun", "Heng Zhou", "Yongqiang Xie", "Zhongbo Li", "Li Zhu"], "year": 2024, "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "cited_by_count": 6, "type": "article", "concepts": ["Architecture", "Convolution (computer science)", "Computer science", "Artificial intelligence", "Geography"], "search_query": "prompt-based image restoration learning degradation", "score": 7, "subtopic": "All-in-One Restoration", "rationale": "Efficient prompt-guided convolution architecture for multi-weather restoration. Addresses multiple weather degradations with prompt generation module.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "MW-ConvNet: U-shaped CNN with prompt generation module at encoder terminus, wavelet pooling for frequency separation", "key_findings": "0.12s/image inference; outperforms transformer/diffusion methods in speed with competitive quality", "limitations": "Pure CNN, lacks global modeling ability of transformers"}, "bib_key": "Li2024"}
{"openalex_id": "https://openalex.org/W4212954470", "doi": "https://doi.org/10.20944/preprints202202.0159.v1", "title": "Generative Adversarial Unsupervised Image Restoration in Hybrid Degradation Scenes", "abstract": "In this paper, we propose an unsupervised blind restoration model for images in hybrid degradation scenes. The proposed model encodes the content information and degradation information of images and then uses the attention module to disentangle the two kinds of information. It can improve the ability of disentangled presentation learning for a generative adversarial network (GAN) to restore the images in hybrid degradation scenes, enhance the detailed features of restored image and remove the artifact combining the adversarial loss, cycle-consistency loss, and perception loss. The experimental results on the DIV2K dataset and medical images show that the proposed method outperforms existing unsupervised image restoration algorithms in terms of peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and subjective visual evaluation.", "authors": ["Fan Tang", "Xinyu Zhu", "Jinrong Hu", "Juhong Tie", "Jiliu Zhou", "Ying Fu"], "year": 2022, "venue": "Preprints.org", "cited_by_count": 6, "type": "preprint", "concepts": ["Artificial intelligence", "Computer science", "Image restoration", "Degradation (telecommunications)", "Generative grammar"], "search_query": "image denoising deblurring dehazing deraining deep learning", "score": 7, "subtopic": "All-in-One Restoration", "rationale": "Unsupervised restoration for hybrid degradation scenes; relevant to multi-degradation handling.", "alignment": {"task": "high", "method": "medium", "modality": "high"}, "extraction": {"design": "GAN-based unsupervised model encoding content and degradation separately", "key_findings": "Handles hybrid degradation without paired data", "limitations": "Preprints.org venue, limited validation"}, "bib_key": "Tang2022GenerativeAdver"}
{"openalex_id": "https://openalex.org/W4366327759", "doi": "https://doi.org/10.48550/arxiv.2304.08291", "title": "Refusion: Enabling Large-Size Realistic Image Restoration with Latent-Space Diffusion Models", "abstract": "This work aims to improve the applicability of diffusion models in realistic image restoration. Specifically, we enhance the diffusion model in several aspects such as network architecture, noise level, denoising steps, training image size, and optimizer/scheduler. We show that tuning these hyperparameters allows us to achieve better performance on both distortion and perceptual scores. We also propose a U-Net based latent diffusion model which performs diffusion in a low-resolution latent space while preserving high-resolution information from the original input for the decoding process. Compared to the previous latent-diffusion model which trains a VAE-GAN to compress the image, our proposed U-Net compression strategy is significantly more stable and can recover highly accurate images without relying on adversarial optimization. Importantly, these modifications allow us to apply diffusion models to various image restoration tasks, including real-world shadow removal, HR non-homogeneous dehazing, stereo super-resolution, and bokeh effect transformation. By simply replacing the datasets and slightly changing the noise network, our model, named Refusion, is able to deal with large-size images (e.g., 6000 x 4000 x 3 in HR dehazing) and produces good results on all the above restoration problems. Our Refusion achieves the best perceptual performance in the NTIRE 2023 Image Shadow Removal Challenge and wins 2nd place overall.", "authors": ["Ziwei Luo", "Fredrik Gustafsson", "Zheng Zhao", "Jens Sjölund", "Thomas B. Schön"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 6, "type": "preprint", "concepts": ["Computer science", "Image restoration", "Artificial intelligence", "Distortion (music)", "Computer vision"], "search_query": "image denoising deblurring dehazing deraining deep learning", "score": 7, "subtopic": "Diffusion Restoration", "rationale": "Diffusion model for realistic large-size image restoration; relevant diffusion-based restoration approach.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "Latent-space diffusion model enhanced for realistic large-size restoration", "key_findings": "Improved applicability of diffusion models for realistic restoration", "limitations": "arXiv preprint, slow inference"}, "bib_key": "Luo2023Enabling"}
{"openalex_id": "https://openalex.org/W4406092414", "doi": "https://doi.org/10.1016/j.inffus.2025.102930", "title": "Ada4DIR: An adaptive model-driven all-in-one image restoration network for remote sensing images", "authors": ["Ziyang Lihe", "Qiangqiang Yuan", "Jiang He", "Xianyu Jin", "Yi Xiao", "Yuzeng Chen", "Huanfeng Shen", "Liangpei Zhang"], "year": 2025, "venue": "Information Fusion", "cited_by_count": 5, "type": "article", "concepts": ["Computer science", "Image (mathematics)", "Artificial intelligence", "Image restoration", "Computer vision"], "search_query": "prompt-based image restoration learning degradation", "score": 7, "subtopic": "All-in-One Restoration", "rationale": "All-in-one image restoration network for remote sensing, model-driven adaptive approach. Extends unified restoration to remote sensing domain.", "alignment": {"task": "high", "method": "medium", "modality": "medium"}, "extraction": {"design": "Adaptive model-driven all-in-one restoration tailored for remote sensing image degradations", "key_findings": "Effective multi-degradation restoration for remote sensing images", "limitations": "Domain-specific (remote sensing); abstract unavailable"}, "bib_key": "Lihe2025An"}
{"openalex_id": "https://openalex.org/W4407638147", "doi": "https://doi.org/10.1109/tits.2025.3538485", "title": "Enhancing Perception for Autonomous Vehicles: A Multi-Scale Feature Modulation Network for Image Restoration", "abstract": "Accurate environmental perception is essential for the effective operation of autonomous vehicles. However, visual images captured in dynamic environments or adverse weather conditions often suffer from various degradations. Image restoration focuses on reconstructing clear and sharp images by eliminating undesired degradations from corrupted inputs. These degradations typically vary in size and severity, making it crucial to employ robust multi-scale representation learning techniques...", "authors": ["Yuning Cui", "Jianyong Zhu", "Alois Knoll"], "year": 2025, "venue": "IEEE Transactions on Intelligent Transportation Systems", "cited_by_count": 5, "type": "article", "concepts": ["Perception", "Scale (ratio)", "Feature (linguistics)", "Computer science", "Artificial intelligence"], "search_query": "spatially varying degradation image restoration local", "score": 7.0, "subtopic": "Real-World Degradation", "rationale": "Multi-scale feature modulation network for image restoration addressing spatially-varying degradations in frequency and spatial domains; handles multiple degradation types for autonomous driving.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "MSFM: multi-scale frequency attention module + multi-scale spatial modulation module; operates in both frequency and spatial domains", "key_findings": "SOTA performance on 12 datasets across dehazing, defocus/motion deblurring, desnowing", "limitations": "Applied to autonomous vehicle domain; no multi-agent or LLM components"}, "bib_key": "Cui2025EnhancingPercep"}
{"openalex_id": "https://openalex.org/W4405974083", "doi": "https://doi.org/10.1109/tcsi.2024.3519532", "title": "A Unified Accelerator for All-in-One Image Restoration Based on Prompt Degradation Learning", "authors": ["Siyu Zhang", "Qiwei Dong", "Wendong Mao", "Zhongfeng Wang"], "year": 2025, "venue": "IEEE Transactions on Circuits and Systems I Regular Papers", "cited_by_count": 3, "type": "article", "concepts": ["Degradation (telecommunications)", "Image restoration", "Computer science", "Image (mathematics)", "Artificial intelligence"], "search_query": "prompt-based image restoration learning degradation", "score": 7, "subtopic": "All-in-One Restoration", "rationale": "Algorithm-hardware co-design for all-in-one image restoration with prompt-based degradation learning. Addresses unified multi-task restoration including hardware efficiency.", "alignment": {"task": "high", "method": "medium", "modality": "high"}, "extraction": {"design": "ERFM model + integer approximation + head-stationary dataflow on TSMC 28nm CMOS", "key_findings": "3.3x throughput and 3.7x energy efficiency improvements over vision transformer accelerators", "limitations": "Hardware-specific; prompt learning approach less novel than pure algorithmic contributions"}, "bib_key": "Zhang2025AUnified"}
{"openalex_id": "https://openalex.org/W4404635286", "doi": "https://doi.org/10.1016/j.neucom.2024.128955", "title": "Multi-modal degradation feature learning for unified image restoration based on contrastive learning", "authors": ["Lei Chen", "Qibing Xiong", "Wei Zhang", "Xiaoli Liang", "Zhihua Gan", "Lianqing Li", "Xin He"], "year": 2024, "venue": "Neurocomputing", "cited_by_count": 3, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Feature (linguistics)", "Modal", "Degradation (telecommunications)"], "search_query": "prompt-based image restoration learning degradation", "score": 7, "subtopic": "All-in-One Restoration", "rationale": "Multi-modal (visual + text) degradation feature learning for unified image restoration using contrastive learning. Leverages vision-language models for restoration guidance.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "MultiContentNet learns multi-modal features (visual degradation + textual context), used as prompts for restoration network via cross-attention", "key_findings": "State-of-the-art on 6 blind restoration tasks; VLM features improve cross-database restoration", "limitations": "Two-stage approach with separate feature learning adds complexity"}, "bib_key": "Chen2024Degradation"}
{"openalex_id": "https://openalex.org/W4392972181", "doi": "https://doi.org/10.48550/arxiv.2403.11157", "title": "Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model", "authors": ["Dian Zheng", "Xiaoming Wu", "Shuzhou Yang", "Jian Zhang", "Jian-Fang Hu", "Wei-Shi Zheng"], "year": 2024, "venue": "arXiv (Cornell University)", "cited_by_count": 3, "type": "preprint", "concepts": ["Hourglass", "Image restoration", "Image (mathematics)", "Computer vision", "Computer science"], "search_query": "prompt-based image restoration learning degradation", "score": 7, "subtopic": "Diffusion Restoration", "rationale": "Universal image restoration using selective hourglass mapping with diffusion model; shares information across tasks with Shared Distribution Term (SDT).", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "DiffUIR: selective strong condition guidance + SDT to map different degradation distributions to a shared one; reverse maps back with task-specific guidance", "key_findings": "SOTA on 5 restoration tasks, 22 benchmarks; lightweight (0.89M) achieves strong performance", "limitations": "Preprint; diffusion inference cost"}, "bib_key": "Zheng2024SelectiveHourgl"}
{"openalex_id": "https://openalex.org/W4404338208", "doi": "https://doi.org/10.48550/arxiv.2410.18666", "title": "DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation", "authors": ["Yuang Ai", "Xiaoqiang Zhou", "Huaibo Huang", "Xiaotian Han", "Zhengyu Chen", "Quanzeng You", "Hongxia Yang"], "year": 2024, "venue": "arXiv (Cornell University)", "cited_by_count": 2, "type": "preprint", "concepts": ["Computer science", "Image (mathematics)", "Privacy protection", "Internet privacy", "Data science"], "search_query": "prompt-based image restoration learning degradation", "score": 7, "subtopic": "Real-World Degradation", "rationale": "DiT-based real-world image restoration using MLLM perceptual capabilities and Mixture of Adaptive Modulator for diverse real-world degradations. Combines MLLMs with restoration.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "GenIR data pipeline + DreamClear DiT model with MoAM (token-wise degradation priors to integrate restoration experts)", "key_findings": "Superior real-world restoration; privacy-safe 1M image dataset; handles diverse degradations via MoAM", "limitations": "Large model scale; data curation pipeline is complex"}, "bib_key": "Ai2024"}
{"openalex_id": "https://openalex.org/W4409262098", "doi": "https://doi.org/10.1109/wacv61041.2025.00069", "title": "All-in-One Image Compression and Restoration", "authors": ["H. Zeng", "Jiacheng Li", "Ziqiang Zheng", "Zhiwei Xiong"], "year": 2025, "venue": "", "cited_by_count": 1, "type": "article", "concepts": ["Image restoration", "Image compression", "Computer science", "Computer vision", "Compression (physics)"], "search_query": "all-in-one image restoration unified model degradation", "score": 7, "subtopic": "All-in-One Restoration", "rationale": "Unified framework for all-in-one image compression and restoration handling various degradations. Directly relevant to unified restoration models.", "alignment": {"task": "high", "method": "medium", "modality": "high"}, "extraction": {"design": "Content information aggregation + degradation representation aggregation in a joint compression-restoration framework", "key_findings": "Superior RD performance on various degraded inputs, strong generalization to real-world scenarios", "limitations": "Couples compression with restoration; not applicable to pure restoration pipelines"}, "bib_key": "Zeng2025Image"}
{"openalex_id": "https://openalex.org/W4403556433", "doi": "https://doi.org/10.48550/arxiv.2409.03455", "title": "Data-free Distillation with Degradation-prompt Diffusion for Multi-weather Image Restoration", "authors": ["Pei Wang", "Xiaotong Luo", "Yuan Xie", "Yanyun Qu"], "year": 2024, "venue": "arXiv (Cornell University)", "cited_by_count": 1, "type": "preprint", "concepts": ["Degradation (telecommunications)", "Distillation", "Environmental science", "Diffusion", "Image (mathematics)"], "search_query": "prompt-based image restoration learning degradation", "score": 7, "subtopic": "Knowledge Distillation", "rationale": "Data-free knowledge distillation for multi-weather image restoration using degradation-prompt diffusion. Directly addresses knowledge distillation for image restoration deployment.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "D4IR: degradation-aware prompt adapter + diffusion model to synthesize domain-related images for data-free student distillation", "key_findings": "Achieves comparable performance to distillation with original data; superior to unsupervised methods", "limitations": "Preprint; domain shift between generated and original data remains a challenge"}, "bib_key": "Wang2024Distillation"}
{"openalex_id": "https://openalex.org/W7117495353", "doi": "https://doi.org/10.1109/tgrs.2025.3649014", "title": "INP-Net: Implicit Neural Prompting Network for Remote Sensing Image Dehazing", "abstract": "Restoring high-quality images from hazy observations is crucial for visual perception and downstream detection in remote sensing applications. Recent deep learning-based methods have achieved remarkable progress in dehazing, however, they often suffer from either unreliable prompting guidance or inadequate global modeling. To bridge these gaps, we propose INP-Net, an Implicit Neural Prompting Network for remote sensing image dehazing. INP-Net introduces an implicit neural prompting mechanism that exploits learnable implicit neural representations (INR) in the YCbCr color space as degradation-insensitive prompts, which are dynamically injected into the RGB decoding pipeline. In addition, we design the Adaptive Sampling and Prior-enhanced (ASP) Transformer Block to enrich global feature diversity. The ASP comprises two core components: Adaptive Soft Sampling Self-Attention (ASSA), which performs probabilistic token selection to eliminate channel redundancy and enhance discriminative representations; and the Prior-Modulated Mixed-Scale Feed-Forward Network (PMFN), which leverages haze prior knowledge as a multi-scale modulator to guide local feature restoration. Extensive experiments on multiple benchmark datasets demonstrate that the proposed INP-Net surpasses state-of-the-art methods both quantitatively and qualitatively. Our method achieves PSNR gains of 0.47 dB and 0.75 dB over state-of-the-art methods on the challenging StateHaze1K-thick and NH-Haze datasets, respectively.", "authors": ["Shan Liang", "Tao Gao", "Ting Chen", "Yuanbo Wen", "Qianxi Zhang", "Xiao Wang"], "year": 2025, "venue": "IEEE Transactions on Geoscience and Remote Sensing", "cited_by_count": 1, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Redundancy (engineering)", "Computer vision", "Decoding methods"], "search_query": "prompt-based image restoration learning degradation", "score": 7, "subtopic": "All-in-One Restoration", "rationale": "Implicit neural prompting for remote sensing dehazing; prompt-based restoration relevant to unified approaches.", "alignment": {"task": "medium", "method": "high", "modality": "medium"}, "extraction": {"design": "Implicit neural prompting network for dehazing remote sensing images", "key_findings": "Reduces redundancy via implicit prompts", "limitations": "Domain-specific to remote sensing"}, "bib_key": "Liang2025Implicit"}
{"openalex_id": "https://openalex.org/W4408441573", "doi": "https://doi.org/10.3390/drones9030209", "title": "Simultaneous Learning Knowledge Distillation for Image Restoration: Efficient Model Compression for Drones", "abstract": "Deploying high-performance image restoration models on drones is critical for applications like autonomous navigation, surveillance, and environmental monitoring. However, the computational and memory limitations of drones pose significant challenges to utilizing complex image restoration models in real-world scenarios. To address this issue, we propose the Simultaneous Learning Knowledge Distillation (SLKD) framework, specifically designed to compress image restoration models for resource-constrained drones...", "authors": ["Yongheng Zhang"], "year": 2025, "venue": "Drones", "cited_by_count": 0, "type": "article", "concepts": ["Drone", "Distillation", "Artificial intelligence", "Image (mathematics)", "Computer science"], "search_query": "knowledge distillation model compression image restoration", "score": 7.0, "subtopic": "Knowledge Distillation", "rationale": "Extended SLKD framework for drone deployment; demonstrates 85%+ FLOPs reduction with minor PSNR loss. Directly relevant to lightweight restoration for resource-constrained devices.", "alignment": {"task": "high", "method": "high", "modality": "high"}, "extraction": {"design": "SLKD dual-teacher framework applied to drone image restoration (deraining, deblurring, dehazing)", "key_findings": "85.4% FLOPs reduction, 85.8% parameter reduction, only 2.6% PSNR drop on 5 benchmarks", "limitations": "Extension of prior SLKD work; same limitations; no citations yet"}, "bib_key": "Zhang2025SimultaneousLea"}
{"openalex_id": "https://openalex.org/W4382240242", "doi": "https://doi.org/10.1609/aaai.v37i3.25364", "title": "Ultra-High-Definition Low-Light Image Enhancement: A Benchmark and Transformer-Based Method", "abstract": "As the quality of optical sensors improves, there is a need for processing large-scale images. In particular, the ability of devices to capture ultra-high definition (UHD) images and video places new demands on the image processing pipeline. In this paper, we consider the task of low-light image enhancement (LLIE) and introduce a large-scale database consisting of images at 4K and 8K resolution. We conduct systematic benchmarking studies and provide a comparison of current LLIE algorithms. As a second contribution, we introduce LLFormer, a transformer-based low-light enhancement method. The core components of LLFormer are the axis-based multi-head self-attention and cross-layer attention fusion block, which significantly reduces the linear complexity. Extensive experiments on the new dataset and existing public datasets show that LLFormer outperforms state-of-the-art methods. We also show that employing existing LLIE methods trained on our benchmark as a pre-processing step significantly improves the performance of downstream tasks, e.g., face detection in low-light conditions. The source code and pre-trained models are available at https://github.com/TaoWangzj/LLFormer.", "authors": ["Tao Wang", "Kaihao Zhang", "Tianrun Shen", "Wenhan Luo", "Björn Stenger", "Tong Lu"], "year": 2023, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 359, "type": "article", "concepts": ["Computer science", "Benchmarking", "Benchmark (surveying)", "Artificial intelligence", "Transformer"], "search_query": "image restoration transformer architecture efficient", "score": 6.5, "subtopic": "Transformer Restoration", "rationale": "Transformer for UHD low-light enhancement; relevant efficient restoration at high resolution.", "alignment": {"task": "medium", "method": "high", "modality": "high"}, "extraction": {"design": "Transformer-based method for ultra-high-definition low-light enhancement", "key_findings": "Scalable transformer design for large-scale image restoration", "limitations": "Single task focus on low-light"}, "bib_key": "Wang2023"}
{"openalex_id": "https://openalex.org/W4283820866", "doi": "https://doi.org/10.1609/aaai.v36i2.20033", "title": "Close the Loop: A Unified Bottom-Up and Top-Down Paradigm for Joint Image Deraining and Segmentation", "abstract": "In this work, we focus on a very practical problem: image segmentation under rain conditions. Image deraining is a classic low-level restoration task, while image segmentation is a typical high-level understanding task. Most of the existing methods intuitively employ the bottom-up paradigm by taking deraining as a preprocessing step for subsequent segmentation. However, our statistical analysis indicates that not only deraining would benefit segmentation (bottom-up), but also segmentation would further improve deraining performance (top-down) in turn. This motivates us to solve the rainy image segmentation task within a novel top-down and bottom-up unified paradigm, in which two sub-tasks are alternatively performed and collaborated with each other. Specifically, the bottom-up procedure yields both clearer images and rain-robust features from both image and feature domains, so as to ease the segmentation ambiguity caused by rain streaks. The top-down procedure adopts semantics to adaptively guide the restoration for different contents via a novel multi-path semantic attentive module (SAM). Thus the deraining and segmentation could boost the performance of each other cooperatively and progressively. Extensive experiments and ablations demonstrate that the proposed method outperforms the state-of-the-art on rainy image segmentation.", "authors": ["Yi Li", "Yi Chang", "Chang‐Feng Yu", "Luxin Yan"], "year": 2022, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 28, "type": "article", "concepts": ["Segmentation", "Computer science", "Artificial intelligence", "Task (project management)", "Scale-space segmentation"], "search_query": "image denoising deblurring dehazing deraining deep learning", "score": 6.5, "subtopic": "Real-World Degradation", "rationale": "Joint deraining and segmentation with bottom-up top-down paradigm; relevant to task-aware restoration.", "alignment": {"task": "medium", "method": "medium", "modality": "high"}, "extraction": {"design": "Unified bottom-up and top-down framework linking low-level deraining with high-level segmentation", "key_findings": "Joint optimization improves both deraining and segmentation", "limitations": "Limited to rain degradation and segmentation task"}, "bib_key": "Li2022CloseThe"}
{"openalex_id": "https://openalex.org/W4379255977", "doi": "https://doi.org/10.48550/arxiv.2306.00306", "title": "Low-Light Image Enhancement with Wavelet-based Diffusion Models", "abstract": "Diffusion models have achieved promising results in image restoration tasks, yet suffer from time-consuming, excessive computational resource consumption, and unstable restoration. To address these issues, we propose a robust and efficient Diffusion-based Low-Light image enhancement approach, dubbed DiffLL. Specifically, we present a wavelet-based conditional diffusion model (WCDM) that leverages the generative power of diffusion models to produce results with satisfactory perceptual fidelity. Additionally, it also takes advantage of the strengths of wavelet transformation to greatly accelerate inference and reduce computational resource usage without sacrificing information. To avoid chaotic content and diversity, we perform both forward diffusion and denoising in the training phase of WCDM, enabling the model to achieve stable denoising and reduce randomness during inference. Moreover, we further design a high-frequency restoration module (HFRM) that utilizes the vertical and horizontal details of the image to complement the diagonal information for better fine-grained restoration. Extensive experiments on publicly available real-world benchmarks demonstrate that our method outperforms the existing state-of-the-art methods both quantitatively and visually, and it achieves remarkable improvements in efficiency compared to previous diffusion-based methods. In addition, we empirically show that the application for low-light face detection also reveals the latent practical values of our method. Code is available at https://github.com/JianghaiSCU/Diffusion-Low-Light.", "authors": ["Hai Jiang", "Ao Luo", "Songchen Han", "Haoqiang Fan", "Shuaicheng Liu"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 16, "type": "preprint", "concepts": ["Computer science", "Wavelet", "Artificial intelligence", "Inference", "Image restoration"], "search_query": "image denoising deblurring dehazing deraining deep learning", "score": 6.5, "subtopic": "Diffusion Restoration", "rationale": "Wavelet-based diffusion model for low-light enhancement; relevant diffusion-based restoration approach.", "alignment": {"task": "medium", "method": "high", "modality": "high"}, "extraction": {"design": "Wavelet-based diffusion model for efficient low-light enhancement", "key_findings": "Reduced diffusion inference time via wavelet decomposition", "limitations": "Single task focus, arXiv preprint"}, "bib_key": "Jiang2023Image"}
{"openalex_id": "https://openalex.org/W4391667544", "doi": "https://doi.org/10.48550/arxiv.2402.04139", "title": "U-shaped Vision Mamba for Single Image Dehazing", "abstract": "Currently, Transformer is the most popular architecture for image dehazing, but due to its large computational complexity, its ability to handle long-range dependency is limited on resource-constrained devices. To tackle this challenge, we introduce the U-shaped Vision Mamba (UVM-Net), an efficient single-image dehazing network. Inspired by the State Space Sequence Models (SSMs), a new deep sequence model known for its power to handle long sequences, we design a Bi-SSM block that integrates the local feature extraction ability of the convolutional layer with the ability of the SSM to capture long-range dependencies. Extensive experimental results demonstrate the effectiveness of our method. Our method provides a more highly efficient idea of long-range dependency modeling for image dehazing as well as other image restoration tasks. The URL of the code is \\url{https://github.com/zzr-idam/UVM-Net}. Our method takes only \\textbf{0.009} seconds to infer a $325 \\times 325$ resolution image (100FPS) without I/O handling time.", "authors": ["Zhuoran Zheng", "Chen Wu"], "year": 2024, "venue": "arXiv (Cornell University)", "cited_by_count": 15, "type": "preprint", "concepts": ["Computer vision", "Image (mathematics)", "Artificial intelligence", "Computer science", "Geology"], "search_query": "image restoration transformer architecture efficient", "score": 6.5, "subtopic": "Transformer Restoration", "rationale": "Mamba-based U-shaped network for dehazing; relevant emerging architecture for restoration.", "alignment": {"task": "medium", "method": "high", "modality": "high"}, "extraction": {"design": "U-shaped Mamba architecture for efficient dehazing", "key_findings": "Linear complexity alternative to transformers for dehazing", "limitations": "Single task (dehazing), arXiv preprint"}, "bib_key": "Zheng2024Vision"}
{"openalex_id": "https://openalex.org/W2949349317", "doi": "https://doi.org/10.48550/arxiv.1609.07769", "title": "Deep Joint Rain Detection and Removal from a Single Image", "abstract": "In this paper, we address a rain removal problem from a single image, even in the presence of heavy rain and rain streak accumulation. Our core ideas lie in the new rain image models and a novel deep learning architecture. We first modify an existing model comprising a rain streak layer and a background layer, by adding a binary map that locates rain streak regions. Second, we create a new model consisting of a component representing rain streak accumulation (where individual streaks cannot be seen, and thus visually similar to mist or fog), and another component representing various shapes and directions of overlapping rain streaks, which usually happen in heavy rain. Based on the first model, we develop a multi-task deep learning architecture that learns the binary rain streak map, the appearance of rain streaks, and the clean background, which is our ultimate output. The additional binary map is critically beneficial, since its loss function can provide additional strong information to the network. To handle rain streak accumulation (again, a phenomenon visually similar to mist or fog) and various shapes and directions of overlapping rain streaks, we propose a recurrent rain detection and removal network that removes rain streaks and clears up the rain accumulation iteratively and progressively. In each recurrence of our method, a new contextualized dilated network is developed to exploit regional contextual information and outputs better representation for rain detection. The evaluation on real images, particularly on heavy rain, shows the effectiveness of our novel models and architecture, outperforming the state-of-the-art methods significantly. Our codes and data sets will be publicly available.", "authors": ["Wenhan Yang", "Robby T. Tan", "Jiashi Feng", "Jiaying Liu", "Zongming Guo", "Shuicheng Yan"], "year": 2016, "venue": "arXiv (Cornell University)", "cited_by_count": 10, "type": "preprint", "concepts": ["Streak", "Computer science", "Mist", "Binary number", "Image (mathematics)"], "search_query": "image denoising deblurring dehazing deraining deep learning", "score": 6.5, "subtopic": "Spatial-Aware Processing", "rationale": "Joint rain detection and removal with spatial awareness of rain regions; relevant spatial-aware restoration.", "alignment": {"task": "medium", "method": "medium", "modality": "high"}, "extraction": {"design": "Joint rain detection and removal with rain density classification", "key_findings": "Spatial rain detection improves targeted removal", "limitations": "Limited to rain degradation"}, "bib_key": "Yang2016DeepJoint"}
{"openalex_id": "https://openalex.org/W4406030314", "doi": "https://doi.org/10.1007/s11760-024-03798-7", "title": "Multi-scale representation for image deraining with state space model", "abstract": "", "authors": ["Yufeng Li", "Chuanlong Xie", "Hongming Chen"], "year": 2025, "venue": "Signal Image and Video Processing", "cited_by_count": 3, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Feature (linguistics)", "Convolutional neural network", "Block (permutation group theory)"], "search_query": "image restoration transformer architecture efficient", "score": 6.5, "subtopic": "Transformer Restoration", "rationale": "State space model for image deraining with multi-scale representation; relevant architecture for restoration.", "alignment": {"task": "medium", "method": "high", "modality": "high"}, "extraction": {"design": "Multi-scale state space model for single image deraining", "key_findings": "Effective multi-scale feature extraction for rain removal", "limitations": "Single degradation type"}, "bib_key": "Li2025Representation"}
{"openalex_id": "https://openalex.org/W3155072588", "doi": "https://doi.org/10.1109/tpami.2022.3204461", "title": "Image Super-Resolution Via Iterative Refinement", "abstract": "We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models (Ho et al. 2020), (Sohl-Dickstein et al. 2015) to image-to-image translation, and performs super-resolution through a stochastic iterative denoising process. Output images are initialized with pure Gaussian noise and iteratively refined using a U-Net architecture that is trained on denoising at various noise levels, conditioned on a low-resolution input image. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8× face super-resolution task on CelebA-HQ for which SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GAN baselines do not exceed a fool rate of 34%. We evaluate SR3 on a 4× super-resolution task on ImageNet, where SR3 outperforms baselines in human evaluation and classification accuracy of a ResNet-50 classifier trained on high-resolution images. We further show the effectiveness of SR3 in cascaded image generation, where a generative model is chained with super-resolution models to synthesize high-resolution images with competitive FID scores on the class-conditional 256×256 ImageNet generation challenge.", "authors": ["Chitwan Saharia", "Jonathan Ho", "William Chan", "Tim Salimans", "David J. Fleet", "Mohammad Norouzi"], "year": 2022, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "cited_by_count": 1551, "type": "article", "concepts": ["Artificial intelligence", "Computer science", "Pattern recognition (psychology)", "Image resolution", "Computer vision"], "search_query": "image restoration transformer architecture efficient", "score": 6, "subtopic": "Diffusion Restoration", "rationale": "Diffusion-based super-resolution; relevant as diffusion model application to restoration.", "alignment": {"task": "medium", "method": "medium", "modality": "high"}, "extraction": {"design": "Denoising diffusion probabilistic model adapted for super-resolution", "key_findings": "High-quality SR via iterative refinement process", "limitations": "Slow inference due to iterative diffusion"}, "bib_key": "Saharia2022Image"}
{"openalex_id": "https://openalex.org/W4393065402", "doi": "https://doi.org/10.1007/s11704-024-40231-1", "title": "A survey on large language model based autonomous agents", "abstract": "Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents.", "authors": ["Lei Wang", "Chen Ma", "Xueyang Feng", "Zeyu Zhang", "Hao Yang", "Jingsen Zhang", "Zhiyuan Chen", "Jiakai Tang", "Xu Chen", "Yankai Lin", "Wayne Xin Zhao", "Zhewei Wei", "Ji-Rong Wen"], "year": 2024, "venue": "Frontiers of Computer Science", "cited_by_count": 836, "type": "article", "concepts": ["Computer science", "Artificial intelligence"], "search_query": "large multimodal model tool use visual reasoning", "score": 6.0, "subtopic": "Multi-Agent Systems", "rationale": "Comprehensive survey on LLM-based autonomous agents covering construction, applications, evaluation; directly relevant to multi-agent system design for AgenticIR.", "alignment": {"task": "medium", "method": "high", "modality": "low"}, "extraction": {"design": "Survey of LLM agent construction (memory, planning, action), applications in science/engineering", "key_findings": "Unified framework for LLM agents; identifies key challenges in planning and tool use", "limitations": "Not image-specific; general agent framework"}, "bib_key": "Wang2024ASurvey"}
{"openalex_id": "https://openalex.org/W2735224642", "doi": "https://doi.org/10.1109/cvprw.2017.151", "title": "Enhanced Deep Residual Networks for Single Image Super-Resolution", "abstract": "Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance.", "authors": ["Bee Lim", "Sanghyun Son", "Heewon Kim", "Seungjun Nah", "Kyoung Mu Lee"], "year": 2017, "venue": "", "cited_by_count": 614, "type": "preprint", "concepts": ["Residual", "Benchmark (surveying)", "Convolutional neural network", "Deep learning", "Computer science"], "search_query": "all-in-one image restoration unified model degradation", "score": 6.0, "subtopic": "Super-Resolution", "rationale": "EDSR/MDSR — enhanced deep residual networks for super-resolution, highly influential restoration architecture that introduced multi-scale unified SR model.", "alignment": {"task": "high", "method": "medium", "modality": "high"}, "extraction": {"design": "EDSR: optimized residual network for SR by removing BN; MDSR: multi-scale unified model for different upscaling factors", "key_findings": "EDSR won NTIRE2017 SR Challenge; MDSR handles multiple scales in single model", "limitations": "Single degradation type (super-resolution only)"}, "bib_key": "Lim2017EnhancedDeep"}
{"openalex_id": "https://openalex.org/W4382462760", "doi": "https://doi.org/10.1609/aaai.v37i2.25353", "title": "Exploring CLIP for Assessing the Look and Feel of Images", "abstract": "Measuring the perception of visual content is a long-standing problem in computer vision...", "authors": ["Jianyi Wang", "Kelvin C. K. Chan", "Chen Change Loy"], "year": 2023, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 461, "type": "article", "concepts": ["Computer science", "Perception", "Task (project management)", "Quality (philosophy)", "Prior probability"], "search_query": "visual language model image quality assessment", "score": 6.0, "subtopic": "Image Quality Assessment", "rationale": "CLIP-based IQA for quality and abstract perception — relevant as VLM-based quality assessment in restoration evaluation.", "alignment": {"task": "medium", "method": "high", "modality": "medium"}, "extraction": {"design": "Exploits CLIP visual-language prior with prompt pairing for zero-shot IQA without task-specific training.", "key_findings": "CLIP captures meaningful perceptual priors generalizing to IQA and aesthetic assessment.", "limitations": "IQA only; not applied to guiding restoration."}, "bib_key": "Wang2023ExploringClip"}
{"openalex_id": "https://openalex.org/W2566149141", "doi": "https://doi.org/10.1109/jstsp.2016.2639328", "title": "Fully Deep Blind Image Quality Predictor", "abstract": "In general, owing to the benefits obtained from original information, full-reference image quality assessment (FR-IQA) achieves relatively higher prediction accuracy than no-reference image quality assessment (NR-IQA)...", "authors": ["Jongyoo Kim", "Sanghoon Lee"], "year": 2016, "venue": "IEEE Journal of Selected Topics in Signal Processing", "cited_by_count": 455, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Convolutional neural network", "Image quality", "Pattern recognition (psychology)"], "search_query": "image quality assessment no-reference blind deep learning", "score": 6.0, "subtopic": "Image Quality Assessment", "rationale": "Deep blind IQA (BIECON) using FR-IQA behavior imitation — relevant baseline for no-reference IQA used in restoration quality evaluation.", "alignment": {"task": "medium", "method": "high", "modality": "medium"}, "extraction": {"design": "BIECON: CNN-based NR-IQA mimicking FR-IQA behavior via local quality maps as intermediate targets.", "key_findings": "Achieves NR-IQA performance comparable to FR-IQA methods.", "limitations": "Classic IQA only; no restoration pipeline integration."}, "bib_key": "Kim2016FullyDeep"}
{"openalex_id": "https://openalex.org/W3207673409", "doi": "https://doi.org/10.1016/j.inffus.2021.09.005", "title": "Real-world single image super-resolution: A brief review", "abstract": "", "authors": ["Honggang Chen", "Xiaohai He", "Linbo Qing", "Yuanyuan Wu", "Chao Ren", "Ray E. Sheriff", "Ce Zhu"], "year": 2021, "venue": "Information Fusion", "cited_by_count": 361, "type": "review", "concepts": ["Computer science", "Benchmark (surveying)", "Superresolution", "Artificial intelligence", "Image (mathematics)"], "search_query": "real world image degradation mixed complex restoration", "score": 6.0, "subtopic": "Real-World Degradation", "rationale": "Review of real-world single image SR methods; highly relevant survey covering complex degradation challenges.", "alignment": {"task": "high", "method": "medium", "modality": "high"}, "extraction": {"design": "Comprehensive review of real-world SISR methods covering degradation modeling, blind SR, and benchmark datasets", "key_findings": "N/A (no abstract)", "limitations": "Survey only; SR-focused"}, "bib_key": "Chen2021Single"}
{"openalex_id": "https://openalex.org/W3169893408", "doi": "https://doi.org/10.1007/s40747-021-00428-4", "title": "Methods for image denoising using convolutional neural network: a review", "abstract": "Abstract Image denoising faces significant challenges, arising from the sources of noise. Specifically, Gaussian, impulse, salt, pepper, and speckle noise are complicated sources of noise in imaging. Convolutional neural network (CNN) has increasingly received attention in image denoising task. Several CNN methods for denoising images have been studied. These methods used different datasets for evaluation. In this paper, we offer an elaborate study on different CNN techniques used in image denoising. Different CNN methods for image denoising were categorized and analyzed. Popular datasets used for evaluating CNN image denoising methods were investigated. Several CNN image denoising papers were selected for review and analysis. Motivations and principles of CNN methods were outlined. Some state-of-the-arts CNN image denoising methods were depicted in graphical forms, while other methods were elaborately explained. We proposed a review of image denoising with CNN. Previous and recent papers on image denoising with CNN were selected. Potential challenges and directions for future research were equally fully explicated.", "authors": ["Ademola E. Ilesanmi", "Taiwo Ilesanmi"], "year": 2021, "venue": "Complex & Intelligent Systems", "cited_by_count": 348, "type": "review", "concepts": ["Noise reduction", "Convolutional neural network", "Computer science", "Artificial intelligence", "Non-local means"], "search_query": "prompt-based image restoration learning degradation", "score": 6, "subtopic": "Real-World Degradation", "rationale": "Review of CNN-based image denoising methods; relevant to image restoration but focused on single-task denoising.", "alignment": {"task": "medium", "method": "medium", "modality": "high"}, "extraction": {"design": "Survey of CNN denoising methods including Gaussian, impulse, salt-pepper noise", "key_findings": "Comprehensive review of denoising architectures", "limitations": "Focused only on denoising, not multi-task"}, "bib_key": "Ilesanmi2021MethodsFor"}
{"openalex_id": "https://openalex.org/W4379474533", "doi": "https://doi.org/10.48550/arxiv.2306.01567", "title": "Segment Anything in High Quality", "abstract": "The recent Segment Anything Model (SAM) represents a big leap in scaling up segmentation models...", "authors": ["Ke Lei", "Mingqiao Ye", "Martin Danelljan", "Yifan Liu", "Yu-Wing Tai", "Chi-Keung Tang", "Fisher Yu"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 109, "type": "preprint", "concepts": ["Computer science", "Segmentation", "Security token", "Artificial intelligence", "Quality (philosophy)"], "search_query": "segment anything model SAM zero-shot segmentation", "score": 6.0, "subtopic": "Foundation Models Vision", "rationale": "HQ-SAM improves mask quality for intricate structures — directly relevant to spatially-aware restoration that relies on precise region segmentation.", "alignment": {"task": "medium", "method": "high", "modality": "medium"}, "extraction": {"design": "HQ-SAM adds learnable High-Quality Output Token to SAM's decoder, fusing early+final ViT features; trained on 44K fine-grained masks.", "key_findings": "Improves fine-grained zero-shot segmentation across 10 datasets; only 4h training on 8 GPUs.", "limitations": "Segmentation only; no restoration integration."}, "bib_key": "Lei2023SegmentAnything"}
{"openalex_id": "https://openalex.org/W3016917677", "doi": "https://doi.org/10.1016/j.ins.2020.04.030", "title": "Blind quality assessment for image superresolution using deep two-stream convolutional networks", "abstract": "", "authors": ["Wei Zhou", "Qiuping Jiang", "Yuwang Wang", "Zhibo Chen", "Weiping Li"], "year": 2020, "venue": "Information Sciences", "cited_by_count": 77, "type": "article", "concepts": ["Artificial intelligence", "Computer science", "Discriminative model", "Convolutional neural network", "Pattern recognition (psychology)"], "search_query": "image quality assessment no-reference blind deep learning", "score": 6.0, "subtopic": "Image Quality Assessment", "rationale": "Blind IQA specifically for super-resolution output — directly relevant to evaluating restoration output quality.", "alignment": {"task": "high", "method": "medium", "modality": "high"}, "extraction": {"design": "Two-stream CNN for blind quality assessment of super-resolved images.", "key_findings": "Two-stream architecture captures both distortion and content features for SR quality assessment.", "limitations": "Super-resolution specific; not generalised to other degradation types."}, "bib_key": "Zhou2020BlindQuality"}
{"openalex_id": "https://openalex.org/W4402915908", "doi": "https://doi.org/10.1109/cvprw63382.2024.00367", "title": "SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding", "abstract": "The landscape of publicly available vision foundation models (VFMs), such as CLIP and Segment Anything Model (SAM), is expanding rapidly...", "authors": ["Haoxiang Wang"], "year": 2024, "venue": "", "cited_by_count": 65, "type": "article", "concepts": ["Foundation (evidence)", "Computer science", "Artificial intelligence", "Information retrieval", "Geography"], "search_query": "segment anything model SAM zero-shot segmentation", "score": 6.0, "subtopic": "Foundation Models Vision", "rationale": "Merges SAM (spatial) and CLIP (semantic) into a unified model — directly relevant to VLM-based spatial-aware restoration that requires both spatial segmentation and semantic understanding.", "alignment": {"task": "medium", "method": "high", "modality": "high"}, "extraction": {"design": "Multi-task distillation merging SAM and CLIP into a single ViT using continual learning; SAM-CLIP reduces inference cost vs. deploying both models.", "key_findings": "+6.8%/+5.9% mIoU improvement on Pascal-VOC/COCO-Stuff zero-shot semantic segmentation.", "limitations": "Segmentation/recognition only; no restoration integration."}, "bib_key": "Wang2024Merging"}

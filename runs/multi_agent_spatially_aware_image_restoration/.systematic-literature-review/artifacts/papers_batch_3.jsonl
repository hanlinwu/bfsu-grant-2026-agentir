{"openalex_id": "https://openalex.org/W4385769398", "doi": "https://doi.org/10.3390/sym15081571", "title": "Symmetric Enhancement of Visual Clarity through a Multi-Scale Dilated Residual Recurrent Network Approach for Image Deraining", "abstract": "Images captured during rainy days present the challenge of maintaining a symmetrical balance between foreground elements (like rain streaks) and the background scenery. The interplay between these rain-obscured images is reminiscent of the principle of symmetry, where one element, the rain streak, overshadows or disrupts the visual quality of the entire image. The challenge lies not just in eradicating the rain streaks but in ensuring the background is symmetrically restored to its original clarity. Recently, numerous deraining algorithms that employ deep learning techniques have been proposed, demonstrating promising results. Yet, achieving a perfect symmetrical balance by effectively removing rain streaks from a diverse set of images, while also symmetrically restoring the background details, is a monumental task. To address this issue, we introduce an image-deraining algorithm that leverages multi-scale dilated residual recurrent networks. The algorithm begins by utilizing convolutional activation layers to symmetrically process both the foreground and background features. Then, to ensure the symmetrical dissemination of the characteristics of rain streaks and the background, it employs long short-term memory networks in conjunction with gated recurrent units across various stages. The algorithm then incorporates dilated residual blocks (DRB), composed of dilated convolutions with three distinct dilation factors. This integration expands the receptive field, facilitating the extraction of deep, multi-scale features of both the rain streaks and background information. Furthermore, considering the complex and diverse nature of rain streaks, a channel attention (CA) mechanism is incorporated to capture richer image features and enhance the model’s performance. Ultimately, convolutional layers are employed to fuse the image features, resulting in a derained image. An evaluation encompassing seven benchmark datasets, assessed using five quality metrics against various conventional and modern algorithms, confirms the robustness and flexibility of our approach.", "authors": ["Jameel Ahmed Bhutto", "Ruihong Zhang", "Zia-ur Rahman"], "year": 2023, "venue": "Symmetry", "cited_by_count": 7, "type": "article", "concepts": ["Computer science", "Residual", "Artificial intelligence", "Scale (ratio)", "Process (computing)"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4309427392", "doi": "https://doi.org/10.48550/arxiv.2111.08892", "title": "SAPNet: Segmentation-Aware Progressive Network for Perceptual\\n Contrastive Deraining", "abstract": "Deep learning algorithms have recently achieved promising deraining\\nperformances on both the natural and synthetic rainy datasets. As an essential\\nlow-level pre-processing stage, a deraining network should clear the rain\\nstreaks and preserve the fine semantic details. However, most existing methods\\nonly consider low-level image restoration. That limits their performances at\\nhigh-level tasks requiring precise semantic information. To address this issue,\\nin this paper, we present a segmentation-aware progressive network (SAPNet)\\nbased upon contrastive learning for single image deraining. We start our method\\nwith a lightweight derain network formed with progressive dilated units (PDU).\\nThe PDU can significantly expand the receptive field and characterize\\nmulti-scale rain streaks without the heavy computation on multi-scale images. A\\nfundamental aspect of this work is an unsupervised background segmentation\\n(UBS) network initialized with ImageNet and Gaussian weights. The UBS can\\nfaithfully preserve an image's semantic information and improve the\\ngeneralization ability to unseen photos. Furthermore, we introduce a perceptual\\ncontrastive loss (PCL) and a learned perceptual image similarity loss (LPISL)\\nto regulate model learning. By exploiting the rainy image and groundtruth as\\nthe negative and the positive sample in the VGG-16 latent space, we bridge the\\nfine semantic details between the derained image and the groundtruth in a fully\\nconstrained manner. Comprehensive experiments on synthetic and real-world rainy\\nimages show our model surpasses top-performing methods and aids object\\ndetection and semantic segmentation with considerable efficacy. A Pytorch\\nImplementation is available at\\nhttps://github.com/ShenZheng2000/SAPNet-for-image-deraining.\\n", "authors": ["Shen Zheng", "Changjie Lu", "WU Yu-xiong", "Gaurav Gupta"], "year": 2021, "venue": "arXiv (Cornell University)", "cited_by_count": 4, "type": "preprint", "concepts": ["Computer science", "Artificial intelligence", "Segmentation", "Pattern recognition (psychology)", "Perception"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W3097097973", "doi": "https://doi.org/10.1109/access.2020.3034238", "title": "Wavelet Based Deep Recursive Pyramid Convolution Residual Network for Single Image Rain Removal", "abstract": "Image rain removal aims to separate the background image from the rainy image. During the past three years, the image rain removal with deep convolutional neural networks has achieved impressive performance. However, how to reach tradeoff between high de-raining performance and low model parameters is still a challenge. To address the issue, the paper is devoted to exploring a novel method based on wavelet deep recursive pyramid convolution residual network (WDRPRN), in which discrete wavelet transform is embedded to decompose the rainy image in different frequency domains, and the deep recursive pyramid convolution residual network (DRPRN) can well predict the residual coefficients between rainy image and clean image. In addition, compared with other neural networks, the DRPRN adopts recursive model that can cost fewer parameters. Plentiful of experiments on synthetic and real-world datasets show that the proposed method is significantly superior to the recent state-of-the-art algorithms.", "authors": ["Tian Tian Gong", "Jun Sheng Wang"], "year": 2020, "venue": "IEEE Access", "cited_by_count": 3, "type": "article", "concepts": ["Residual", "Convolution (computer science)", "Computer science", "Pyramid (geometry)", "Artificial intelligence"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4206086042", "doi": "https://doi.org/10.1109/access.2021.3132148", "title": "Universal Framework for Joint Image Restoration and 3D Body Reconstruction", "abstract": "Recent works have demonstrated excellent state-of-the-art achievements in image restoration and 3D body reconstruction from an input image. The 3D body reconstruction task, however, relies heavily on the input image’s quality. A straightforward way to solve this issue is by generating vast degraded datasets and using them in a re-finetuned or newly-crafted body reconstruction network. However, in future usage, these datasets may become obsolete, leaving the newly-crafted network outdated. Unlike this approach, we design a universal framework that is able to utilize prior state-of-the-art restoration works and then self-boosts their performances during test-time while jointly carrying out the 3D body reconstruction. The self-boosting mechanism is adopted via test-time parameter adaptation capable of handling various types of degradation. To accommodate, we also propose a strategy that generates pseudo-data on the fly during test-time, allowing both restoration and reconstruction modules to be learned in a self-supervised manner. With this advantage, the universal framework intelligently enhances the performance without any new dataset or new neural network model involvement. Our experimental results show that using the proposed framework and pseudo-data strategies significantly improves the performances of both scenarios.", "authors": ["Jonathan Samuel Lumentut", "Matthew Marchellus", "Joshua Santoso", "Tae Hyun Kim", "Ju Yong Chang", "In Kyu Park"], "year": 2021, "venue": "IEEE Access", "cited_by_count": 3, "type": "article", "concepts": ["Computer science", "Boosting (machine learning)", "Image restoration", "Artificial intelligence", "Image (mathematics)"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4308900645", "doi": "https://doi.org/10.48550/arxiv.2112.05147", "title": "Learning Deep Context-Sensitive Decomposition for Low-Light Image\\n Enhancement", "abstract": "Enhancing the quality of low-light images plays a very important role in many\\nimage processing and multimedia applications. In recent years, a variety of\\ndeep learning techniques have been developed to address this challenging task.\\nA typical framework is to simultaneously estimate the illumination and\\nreflectance, but they disregard the scene-level contextual information\\nencapsulated in feature spaces, causing many unfavorable outcomes, e.g.,\\ndetails loss, color unsaturation, artifacts, and so on. To address these\\nissues, we develop a new context-sensitive decomposition network architecture\\nto exploit the scene-level contextual dependencies on spatial scales. More\\nconcretely, we build a two-stream estimation mechanism including reflectance\\nand illumination estimation network. We design a novel context-sensitive\\ndecomposition connection to bridge the two-stream mechanism by incorporating\\nthe physical principle. The spatially-varying illumination guidance is further\\nconstructed for achieving the edge-aware smoothness property of the\\nillumination component. According to different training patterns, we construct\\nCSDNet (paired supervision) and CSDGAN (unpaired supervision) to fully evaluate\\nour designed architecture. We test our method on seven testing benchmarks to\\nconduct plenty of analytical and evaluated experiments. Thanks to our designed\\ncontext-sensitive decomposition connection, we successfully realized excellent\\nenhanced results, which fully indicates our superiority against existing\\nstate-of-the-art approaches. Finally, considering the practical needs for\\nhigh-efficiency, we develop a lightweight CSDNet (named LiteCSDNet) by reducing\\nthe number of channels. Further, by sharing an encoder for these two\\ncomponents, we obtain a more lightweight version (SLiteCSDNet for short).\\nSLiteCSDNet just contains 0.0301M parameters but achieves the almost same\\nperformance as CSDNet.\\n", "authors": ["Long Ma", "Risheng Liu", "Jiaao Zhang", "Xin Fan", "Zhongxuan Luo"], "year": 2021, "venue": "arXiv (Cornell University)", "cited_by_count": 5, "type": "preprint", "concepts": ["Computer science", "Context (archaeology)", "Artificial intelligence", "Deep learning", "Encoder"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4293499157", "doi": "https://doi.org/10.1109/access.2022.3202888", "title": "Single Image Raindrop Removal Using a Non-Local Operator and Feature Maps in the Frequency Domain", "abstract": "Taking a photo on a rainy day may result in a photo with raindrops. Images containing raindrops have a significant impact on the visual impression and accuracy when applied to image recognition systems. Thus, an automatic high-quality raindrop removal method is desired for outdoor image processing systems as well as for acquiring good-looking images. Several existing methods have been proposed to tackle this problem, but they often fail to keep global consistency and generate unnatural patterns. In this paper, we tackle this problem by introducing a non-local operator. The non-local operator combines features in distant locations with matrix multiplication and enables consistency in distant locations. In addition, high-frequency components such as edges are more affected in images with raindrops. Inspired by the nature that high-frequency components can be separated from other components in the frequency domain, we also propose to process feature maps in the frequency domain, which are obtained by the fast Fourier transform operation and processed by several convolution layers. Experimental results show that our method effectively removes raindrops and achieves state-of-the-art performance.", "authors": ["Shinya Ezumi", "Masaaki Ikehara"], "year": 2022, "venue": "IEEE Access", "cited_by_count": 4, "type": "article", "concepts": ["Frequency domain", "Computer science", "Convolution (computer science)", "Operator (biology)", "Artificial intelligence"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4385849093", "doi": "https://doi.org/10.48550/arxiv.2308.06998", "title": "Mutual Information-driven Triple Interaction Network for Efficient Image Dehazing", "abstract": "Multi-stage architectures have exhibited efficacy in image dehazing, which usually decomposes a challenging task into multiple more tractable sub-tasks and progressively estimates latent hazy-free images. Despite the remarkable progress, existing methods still suffer from the following shortcomings: (1) limited exploration of frequency domain information; (2) insufficient information interaction; (3) severe feature redundancy. To remedy these issues, we propose a novel Mutual Information-driven Triple interaction Network (MITNet) based on spatial-frequency dual domain information and two-stage architecture. To be specific, the first stage, named amplitude-guided haze removal, aims to recover the amplitude spectrum of the hazy images for haze removal. And the second stage, named phase-guided structure refined, devotes to learning the transformation and refinement of the phase spectrum. To facilitate the information exchange between two stages, an Adaptive Triple Interaction Module (ATIM) is developed to simultaneously aggregate cross-domain, cross-scale, and cross-stage features, where the fused features are further used to generate content-adaptive dynamic filters so that applying them to enhance global context representation. In addition, we impose the mutual information minimization constraint on paired scale encoder and decoder features from both stages. Such an operation can effectively reduce information redundancy and enhance cross-stage feature complementarity. Extensive experiments on multiple public datasets exhibit that our MITNet performs superior performance with lower model complexity.The code and models are available at https://github.com/it-hao/MITNet.", "authors": ["Hao Shen", "Zhong‐Qiu Zhao", "Yulun Zhang", "Zhao Zhang"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 3, "type": "preprint", "concepts": ["Computer science", "Mutual information", "Redundancy (engineering)", "Encoder", "Interaction information"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W2768996629", "doi": "https://doi.org/10.1109/cvpr.2018.00258", "title": "xUnit: Learning a Spatial Activation Function for Efficient Image Restoration", "abstract": "In recent years, deep neural networks (DNNs) achieved unprecedented performance in many low-level vision tasks. However, state-of-the-art results are typically achieved by very deep networks, which can reach tens of layers with tens of millions of parameters. To make DNNs implementable on platforms with limited resources, it is necessary to weaken the tradeoff between performance and efficiency. In this paper, we propose a new activation unit, which is particularly suitable for image restoration problems. In contrast to the widespread per-pixel activation units, like ReLUs and sigmoids, our unit implements a learnable nonlinear function with spatial connections. This enables the net to capture much more complex features, thus requiring a significantly smaller number of layers in order to reach the same performance. We illustrate the effectiveness of our units through experiments with state-of-the-art nets for denoising, de-raining, and super resolution, which are already considered to be very small. With our approach, we are able to further reduce these models by nearly 50% without incurring any degradation in performance.", "authors": ["Idan Kligvasser", "Tamar Rott Shaham", "Tomer Michaeli"], "year": 2018, "venue": "", "cited_by_count": 4, "type": "preprint", "concepts": ["Computer science", "Image restoration", "Deep neural networks", "Pixel", "Activation function"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4405291967", "doi": "https://doi.org/10.3390/app142411565", "title": "MCIDN: Deblurring Network for Metal Corrosion Images", "abstract": "The analysis of corrosion images is crucial in materials science, where acquiring clear images is fundamental for subsequent analysis. The goal of deblurring metal corrosion images is to reconstruct clear images from degraded ones. To the best of our knowledge, this study introduces the first paired blurry-sharp image dataset specifically designed for the metal corrosion domain, filling a critical gap in the existing research. This innovative approach effectively addresses the unique challenges associated with deblurring metal corrosion images. We propose a novel metal corrosion images deblurring network (MCIDN) that employs a dual-domain attention mechanism, integrating both spatial and frequency domains to enhance image clarity. This innovative approach effectively addresses the unique challenges associated with deblurring metal corrosion images. While self-attention is widely used in visual tasks, its quadratic complexity often leads to high computational costs. To address this issue, we introduce a new spatial channel attention module (SCAM) that employs dynamic group convolutions to achieve self-attention, effectively integrating information from local regions and enhancing representation learning capabilities. Recognizing the critical role of frequency components in image restoration, we develop a frequency channel attention module (FCAM) that selectively focuses on different frequency components of images, thereby enhancing deblurring performance. These two attention modules are seamlessly integrated into our network. Compared to existing methods, our approach demonstrates superior performance on datasets of blurry metal corrosion images, achieving a peak signal-to-noise ratio (PSNR) of 32.8645 dB and a structural similarity (SSIM) of 0.9768. These metrics indicate that our method provides clearer and more detailed reconstructions, significantly enhancing the image quality.", "authors": ["Jiaxiang Wang", "Meng Wan", "Pufen Zhang", "Sijie Chang", "Hao Du", "Peng Shi", "Hongying Yu", "Dongbai Sun", "Jue Wang", "Yangang Wang"], "year": 2024, "venue": "Applied Sciences", "cited_by_count": 3, "type": "article", "concepts": ["Deblurring", "Materials science", "Computer science", "Artificial intelligence", "Image processing"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4403661836", "doi": "https://doi.org/10.48550/arxiv.2409.08510", "title": "CasDyF-Net: Image Dehazing via Cascaded Dynamic Filters", "abstract": "Image dehazing aims to restore image clarity and visual quality by reducing atmospheric scattering and absorption effects. While deep learning has made significant strides in this area, more and more methods are constrained by network depth. Consequently, lots of approaches have adopted parallel branching strategies. however, they often prioritize aspects such as resolution, receptive field, or frequency domain segmentation without dynamically partitioning branches based on the distribution of input features. Inspired by dynamic filtering, we propose using cascaded dynamic filters to create a multi-branch network by dynamically generating filter kernels based on feature map distribution. To better handle branch features, we propose a residual multiscale block (RMB), combining different receptive fields. Furthermore, we also introduce a dynamic convolution-based local fusion method to merge features from adjacent branches. Experiments on RESIDE, Haze4K, and O-Haze datasets validate our method's effectiveness, with our model achieving a PSNR of 43.21dB on the RESIDE-Indoor dataset. The code is available at https://github.com/dauing/CasDyF-Net.", "authors": ["Yinglong Wang", "Bin He"], "year": 2024, "venue": "arXiv (Cornell University)", "cited_by_count": 3, "type": "preprint", "concepts": ["Net (polyhedron)", "Image (mathematics)", "Computer science", "Computer vision", "Dynamic imaging"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4375957537", "doi": "https://doi.org/10.48550/arxiv.2305.03997", "title": "Dual Degradation Representation for Joint Deraining and Low-Light Enhancement in the Dark", "abstract": "Rain in the dark poses a significant challenge to deploying real-world applications such as autonomous driving, surveillance systems, and night photography. Existing low-light enhancement or deraining methods struggle to brighten low-light conditions and remove rain simultaneously. Additionally, cascade approaches like ``deraining followed by low-light enhancement'' or the reverse often result in problematic rain patterns or overly blurred and overexposed images. To address these challenges, we introduce an end-to-end model called L$^{2}$RIRNet, designed to manage both low-light enhancement and deraining in real-world settings. Our model features two main components: a Dual Degradation Representation Network (DDR-Net) and a Restoration Network. The DDR-Net independently learns degradation representations for luminance effects in dark areas and rain patterns in light areas, employing dual degradation loss to guide the training process. The Restoration Network restores the degraded image using a Fourier Detail Guidance (FDG) module, which leverages near-rainless detailed images, focusing on texture details in frequency and spatial domains to inform the restoration process. Furthermore, we contribute a dataset containing both synthetic and real-world low-light-rainy images. Extensive experiments demonstrate that our L$^{2}$RIRNet performs favorably against existing methods in both synthetic and complex real-world scenarios. All the code and dataset can be found in \\url{https://github.com/linxin0/Low_light_rainy}.", "authors": ["Xin Lin", "Jingtong Yue", "Chao Ren", "Chunle Guo", "Chongyi Li", "Yang, Ming-Hsuan"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 4, "type": "preprint", "concepts": ["Computer science", "Artificial intelligence", "Degradation (telecommunications)", "Pairwise comparison", "Feature (linguistics)"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4311728785", "doi": "https://doi.org/10.3390/s22249587", "title": "PRAGAN: Progressive Recurrent Attention GAN with Pretrained ViT Discriminator for Single-Image Deraining", "abstract": "Images captured in bad weather are not conducive to visual tasks. Rain streaks in rainy images will significantly affect the regular operation of imaging equipment; to solve this problem, using multiple neural networks is a trend. The ingenious integration of network structures allows for full use of the powerful representation and fitting abilities of deep learning to complete low-level visual tasks. In this study, we propose a generative adversarial network (GAN) with multiple attention mechanisms for image rain removal tasks. Firstly, to the best of our knowledge, we propose a pretrained vision transformer (ViT) as the discriminator in GAN for single-image rain removal for the first time. Secondly, we propose a neural network training method that can use a small amount of data for training while maintaining promising results and reliable visual quality. A large number of experiments prove the correctness and effectiveness of our method. Our proposed method achieves better results on synthetic and real image datasets than multiple state-of-the-art methods, even when using less training data.", "authors": ["Bingcai Wei", "Di Wang", "Zhuang Wang", "Liye Zhang"], "year": 2022, "venue": "Sensors", "cited_by_count": 3, "type": "article", "concepts": ["Discriminator", "Correctness", "Computer science", "Artificial intelligence", "Transformer"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W2985670408", "doi": "https://doi.org/10.48550/arxiv.2005.03155", "title": "NTIRE 2020 Challenge on Image Demoireing: Methods and Results", "abstract": "This paper reviews the Challenge on Image Demoireing that was part of the New Trends in Image Restoration and Enhancement (NTIRE) workshop, held in conjunction with CVPR 2020. Demoireing is a difficult task of removing moire patterns from an image to reveal an underlying clean image. The challenge was divided into two tracks. Track 1 targeted the single image demoireing problem, which seeks to remove moire patterns from a single image. Track 2 focused on the burst demoireing problem, where a set of degraded moire images of the same scene were provided as input, with the goal of producing a single demoired image as output. The methods were ranked in terms of their fidelity, measured using the peak signal-to-noise ratio (PSNR) between the ground truth clean images and the restored images produced by the participants' methods. The tracks had 142 and 99 registered participants, respectively, with a total of 14 and 6 submissions in the final testing stage. The entries span the current state-of-the-art in image and burst image demoireing problems.", "authors": ["Shanxin Yuan", "Radu Timofte", "Aleš Leonardis", "Greg Slabaugh", "Xiaotong Luo", "Jiangtao Zhang", "Yanyun Qu", "Ming Hong", "Yuan Xie", "Cuihua Li", "Dejia Xu", "Yihao Chu", "Qingyan Sun", "Shuai Liu", "Ziyao Zong", "Nan Nan", "Chenghua Li", "Sangmin Kim", "Hyungjoon Nam", "Jisu Kim", "Jechang Jeong", "Manri Cheon", "Sungjun Yoon", "Byungyeon Kang", "JunWoo Lee", "Bolun Zheng", "Xiaohong Liu", "Linhui Dai", "Jun Chen", "Xi Cheng", "Zhenyong Fu", "Jian Yang", "Chul Lee", "An Gia Vien", "Hyunkook Park", "Sabari Nathan", "M. Parisa Beham", "S. Mohamed Mansoor Roomi", "Florian Lemarchand", "Maxime Pelcat", "Erwan Nogues", "Densen Puthussery", "P. S. Hrishikesh", "C. V. Jiji", "Ashish Sinha", "Xuan Zhao"], "year": 2020, "venue": "arXiv (Cornell University)", "cited_by_count": 3, "type": "preprint", "concepts": ["Image (mathematics)", "Computer science", "Artificial intelligence", "Computer vision", "Fidelity"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4313328176", "doi": "https://doi.org/10.32604/cmc.2023.031444", "title": "CLGA Net: Cross Layer Gated Attention Network for Image Dehazing", "abstract": "In this paper, we propose an end-to-end cross-layer gated attention network (CLGA-Net) to directly restore fog-free images. Compared with the previous dehazing network, the dehazing model presented in this paper uses the smooth cavity convolution and local residual module as the feature extractor, combined with the channel attention mechanism, to better extract the restored features. A large amount of experimental data proves that the defogging model proposed in this paper is superior to previous defogging technologies in terms of structure similarity index (SSIM), peak signal to noise ratio (PSNR) and subjective visual quality. In order to improve the efficiency of decoding and encoding, we also describe a fusion residual module and conduct ablation experiments, which prove that the fusion residual is suitable for the dehazing problem. Therefore, we use fusion residual as a fixed module for encoding and decoding. In addition, we found that the traditional defogging model based on the U-net network may cause some information losses in space. We have achieved effective maintenance of low-level feature information through the cross-layer gating structure that better takes into account global and subtle features. We also present the application of our CLGA-Net in challenging scenarios where the best results in both quantity and quality can be obtained. Experimental results indicate that the present cross-layer gating module can be widely used in the same type of network.", "authors": ["Shengchun Wang", "Baoxuan Huang", "Tsz Ho Wong", "Huang Jin-gui", "Hong Deng"], "year": 2022, "venue": "Computers, materials & continua/Computers, materials & continua (Print)", "cited_by_count": 3, "type": "article", "concepts": ["Residual", "Computer science", "Decoding methods", "Encoding (memory)", "Layer (electronics)"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4385767441", "doi": "https://doi.org/10.21203/rs.3.rs-3240803/v1", "title": "A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive Learning for Image Deraining", "abstract": "Abstract Image deraining is a challenging task that involves restoring degraded images affected by rain streaks. While Convolutional Neural Networks (CNNs) have been commonly used for this task, existing approaches often rely on stacked convolutional basic blocks with limited performance and compromised spatial detail. Furthermore, the limited receptive field of convolutional layers leads to incomplete processing of non-uniform rain streaks. To address these concerns, we propose a novel image deraining network that combines CNNs and trans- formers. Our network comprises two stages: an encoder-decoder architecture with a triple attention mechanism to capture valuable features and residual dual branch transformer blocks that enhance local information modeling. To address the transformer’s lack of local information modeling capability, we intro- duce convolution in the self-attentive mechanism of the transformer block and feed-forward network. Additionally, we employ a frequency domain contrastive learning method to enhance contrastive sample information, ensuring that the restored image closely resembles the clear image in the frequency domain space, while still retaining a distinction from the rainy image. Extensive quantitative and qualitative experiments demonstrate that our proposed deraining network outperforms existing methods on public datasets.", "authors": ["Cheng Wang", "Wei Li"], "year": 2023, "venue": "Research Square (Research Square)", "cited_by_count": 2, "type": "preprint", "concepts": ["Computer science", "Transformer", "Convolutional neural network", "Encoder", "Artificial intelligence"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4295350396", "doi": "https://doi.org/10.1007/s40747-022-00865-9", "title": "Multi-scale progressive blind face deblurring", "abstract": "", "authors": ["Hao Zhang", "Canghong Shi", "Xian Zhang", "Linfeng Wu", "Xiaojie Li", "Jing Peng", "Xi Wu", "Jiancheng Lv"], "year": 2022, "venue": "Complex & Intelligent Systems", "cited_by_count": 3, "type": "article", "concepts": ["Deblurring", "Artificial intelligence", "Computer science", "Face (sociological concept)", "Computer vision"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4313008082", "doi": "https://doi.org/10.1109/access.2022.3213025", "title": "Deep Learning for Screen-Shot Image Demoiréing: A Survey", "abstract": "Image demoiréing is an important image processing technology in computer vision, used to remove the moiré from images and improve the image quality. In recent years, the image demoiréing technique based on the deep learning method has gained more attention and achieved good results, but it still has some limitations. This paper aims to provide a review and perspective on the recent advances in deep learning-based image demoiréing techniques. First, the definition and production principle of the image moiré pattern are given. Common datasets and image quality evalution methods in demoiréing studies are analyzed. Then two internationally famous competitions in image demoiréing are introduced. Second, the research status of the supervised demoiréing technique is summarized from four dimensions: sampling method, model network design, baseline model, and training learning strategy. Recent progress made by the mainstream model of unsupervised deep learning in the field of image demoiréing is summarized. The typical application of the image demoiréing technique in panel defect detection and digital radiography is analyzed. The performance and the image quality of the above mentioned models based on different data sets are evaluated in detail. Finally, this paper analyzes and forelocks the problems to be solved in the coming years.", "authors": ["Shouming Hou", "Yabing Wang", "Kai Li", "Yinggang Zhao", "Baoyun Lu", "Liya Fan"], "year": 2022, "venue": "IEEE Access", "cited_by_count": 2, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Deep learning", "Image quality", "Field (mathematics)"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4409370074", "doi": "https://doi.org/10.1609/aaai.v39i3.32257", "title": "SIDL: A Real-World Dataset for Restoring Smartphone Images with Dirty Lenses", "abstract": "Smartphone cameras are ubiquitous in daily life, yet their performance can be severely impacted by dirty lenses, leading to degraded image quality. This issue is often overlooked in image restoration research, which assumes ideal or controlled lens conditions. To address this gap, we introduced SIDL (Smartphone Images with Dirty Lenses), a novel dataset designed to restore images captured through contaminated smartphone lenses. SIDL contains diverse real-world images taken under various lighting conditions and environments. These images feature a wide range of lens contaminants, including water drops, fingerprints, and dust. Each contaminated image is paired with a clean reference image, enabling supervised learning approaches for restoration tasks. To evaluate the challenge posed by SIDL, various state-of-the-art restoration models were trained and compared on this dataset. Their performances achieved some level of restoration but did not adequately address the diverse and realistic nature of the lens contaminants in SIDL. This challenge highlights the need for more robust and adaptable image restoration techniques for restoring images with dirty lenses.", "authors": ["Sooyoung Choi", "Sungyong Park", "Heewon Kim"], "year": 2025, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 1, "type": "article", "concepts": ["Smartphone application", "Computer science", "Smartphone app", "Computer vision", "Optometry"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4391909273", "doi": "https://doi.org/10.21597/jist.1328255", "title": "Enhance or Leave It: An Investigation of the Image Enhancement in Small Object Detection in Aerial Images", "abstract": "Recent years of object detection (OD), a fundamental task in computer vision, have witnessed the rise of numerous practical applications of this sub-field such as face detection, self-driving, security, and more. Although existing deep learning models show significant achievement in object detection, they are usually tested on datasets having mostly clean images. Thus, their performance levels were not measured on degraded images. In addition, images and videos in real-world scenarios often involve several natural artifacts such as noise, haze, rain, dust, and motion blur due to several factors such as insufficient light, atmospheric scattering, and faults in image sensors. This image acquisition-related problem becomes more severe when it comes to detecting small objects in aerial images. In this study, we investigate the small object identification performance of several state-of-the-art object detection models (Yolo 6/7/8) under three conditions (noisy, motion blurred, and rainy). Through this inspection, we evaluate the contribution of an image enhancement scheme so-called MPRNet. For this aim, we trained three OD algorithms with the original clean images of the VisDrone dataset. Followingly, we measured the detection performance of saved YOLO models against (1) clean, (2) degraded, and (3) enhanced counterparts. According to the results, MPRNet-based image enhancement promisingly contributes to the detection performance and YOLO8 outperforms its predecessors. We believe that this work presents useful findings for researchers studying aerial image-based vision tasks, especially under extreme weather and image acquisition conditions", "authors": ["Alpay Tekin", "Ahmet Selman Bozkır"], "year": 2024, "venue": "Iğdır Üniversitesi Fen Bilimleri Enstitüsü Dergisi", "cited_by_count": 3, "type": "article", "concepts": ["Artificial intelligence", "Computer science", "Computer vision", "Object detection", "Motion blur"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4361192474", "doi": "https://doi.org/10.21203/rs.3.rs-2717815/v1", "title": "DC-GAN with Feature Attention for Single Image Dehazing", "abstract": "<title>Abstract</title> In recent years, the frequent occurrence of smog weather has affected people's health and has also had a major impact on computer vision application systems. Images captured in hazy environments suffer from quality degradation and other issues such as color distortion, low contrast, and lack of detail. This study proposes an end-to-end, adversarial neural network-based dehazing technique called DC-GAN that combines Dense and Residual blocks efficiently for improved dehazing performance. In addition, it also consists of channel attention and pixel attention, which can offer more versatility when dealing with different forms of data. The Wasserstein Generative Adversarial Network with Gradient Penality(WGAN-GP) was used as an enhancement method to correct the shortcomings in the original GAN's cost function and create an improvised loss. On the basis of the experiment results, the algorithm used in this paper is able to generate sharp images with high image quality. The processed images were simultaneously analyzed using the objective evaluation metrics Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM). The findings demonstrate that the dehazing effect is favorable compared to other state-of-the-art dehazing algorithms, achieving a PSNR and SSIM of 14.7 and 0.54 for the indoor images, and 16.54 and 0.54 for the outdoor images respectively using the NTIRE 2018 dataset. Using the SOTS dataset, the model achieved a PSNR and SSIM of 23.98 and 0.87 for the indoor images, and 19.88 and 0.83 for the outdoor images.", "authors": ["Tewodros Megabiaw Tassew", "Xuan Nie"], "year": 2023, "venue": "", "cited_by_count": 2, "type": "preprint", "concepts": ["Feature (linguistics)", "Image (mathematics)", "Computer science", "Materials science", "Artificial intelligence"], "search_query": "image denoising deblurring dehazing deraining deep learning"}
{"openalex_id": "https://openalex.org/W4390874575", "doi": "https://doi.org/10.1109/iccv51070.2023.00371", "title": "Segment Anything", "abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive – often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at segment-anything.com to foster research into foundation models for computer vision. We recommend reading the full paper at: arxiv.org/abs/2304.02643.", "authors": ["Alexander M. Kirillov", "Eric Mintun", "Nikhila Ravi", "Hanzi Mao", "Chloe Rolland", "Laura Gustafson", "Tete Xiao", "Spencer Whitehead", "Alexander C. Berg", "Wan‐Yen Lo", "Piotr Dollár", "Ross Girshick"], "year": 2023, "venue": "", "cited_by_count": 7782, "type": "article", "concepts": ["Computer science", "Segmentation", "Task (project management)", "Artificial intelligence", "Shot (pellet)"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4364383051", "doi": "https://doi.org/10.48550/arxiv.2304.04155", "title": "Segment Anything Model (SAM) for Digital Pathology: Assess Zero-shot Segmentation on Whole Slide Imaging", "abstract": "The segment anything model (SAM) was released as a foundation model for image segmentation. The promptable segmentation model was trained by over 1 billion masks on 11M licensed and privacy-respecting images. The model supports zero-shot image segmentation with various segmentation prompts (e.g., points, boxes, masks). It makes the SAM attractive for medical image analysis, especially for digital pathology where the training data are rare. In this study, we evaluate the zero-shot segmentation performance of SAM model on representative segmentation tasks on whole slide imaging (WSI), including (1) tumor segmentation, (2) non-tumor tissue segmentation, (3) cell nuclei segmentation. Core Results: The results suggest that the zero-shot SAM model achieves remarkable segmentation performance for large connected objects. However, it does not consistently achieve satisfying performance for dense instance object segmentation, even with 20 prompts (clicks/boxes) on each image. We also summarized the identified limitations for digital pathology: (1) image resolution, (2) multiple scales, (3) prompt selection, and (4) model fine-tuning. In the future, the few-shot fine-tuning with images from downstream pathological segmentation tasks might help the model to achieve better performance in dense object segmentation.", "authors": ["Ruining Deng", "Can Cui", "Quan Liu", "Tianyuan Yao", "Lucas W. Remedios", "Shunxing Bao", "Bennett A. Landman", "Lee Wheless", "Lori A. Coburn", "Keith T. Wilson", "Yaohong Wang", "Shilin Zhao", "Agnes B. Fogo", "Haichun Yang", "Yucheng Tang", "Yuankai Huo"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 96, "type": "preprint", "concepts": ["Segmentation", "Computer science", "Artificial intelligence", "Scale-space segmentation", "Computer vision"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4396996869", "doi": "https://doi.org/10.1145/3654704", "title": "Zero-Shot Segmentation of Eye Features Using the Segment Anything Model (SAM)", "abstract": "The advent of foundation models signals a new era in artificial intelligence. The Segment Anything Model (SAM) is the first foundation model for image segmentation. In this study, we evaluate SAM's ability to segment features from eye images recorded in virtual reality setups. The increasing requirement for annotated eye-image datasets presents a significant opportunity for SAM to redefine the landscape of data annotation in gaze estimation. Our investigation centers on SAM's zero-shot learning abilities and the effectiveness of prompts like bounding boxes or point clicks. Our results are consistent with studies in other domains, demonstrating that SAM's segmentation effectiveness can be on-par with specialized models depending on the feature, with prompts improving its performance, evidenced by an IoU of 93.34% for pupil segmentation in one dataset. Foundation models like SAM could revolutionize gaze estimation by enabling quick and easy image segmentation, reducing reliance on specialized models and extensive manual annotation.", "authors": ["Virmarie Maquiling", "Sean Anthony Byrne", "Diederick C. Niehorster", "Marcus Nyström", "Enkelejda Kasneci"], "year": 2024, "venue": "Proceedings of the ACM on Computer Graphics and Interactive Techniques", "cited_by_count": 10, "type": "article", "concepts": ["Shot (pellet)", "Segmentation", "Artificial intelligence", "Zero (linguistics)", "Computer science"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4385481295", "doi": "https://doi.org/10.1016/j.media.2023.102918", "title": "Segment anything model for medical image analysis: An experimental study", "abstract": "", "authors": ["Maciej A. Mazurowski", "Haoyu Dong", "Hanxue Gu", "Jichen Yang", "Nicholas Konz", "Yixin Zhang"], "year": 2023, "venue": "Medical Image Analysis", "cited_by_count": 623, "type": "article", "concepts": ["Artificial intelligence", "Image (mathematics)", "Computer science", "Computer vision", "Pattern recognition (psychology)"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4391021462", "doi": "https://doi.org/10.1109/tgrs.2024.3356074", "title": "RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation Based on Visual Foundation Model", "abstract": "Leveraging the extensive training data from SA-1B, the Segment Anything Model (SAM) demonstrates remarkable generalization and zero-shot capabilities. However, as a category-agnostic instance segmentation method, SAM heavily relies on prior manual guidance, including points, boxes, and coarse-grained masks. Furthermore, its performance in remote sensing image segmentation tasks remains largely unexplored and unproven. In this paper, we aim to develop an automated instance segmentation approach for remote sensing images, based on the foundational SAM model and incorporating semantic category information. Drawing inspiration from prompt learning, we propose a method to learn the generation of appropriate prompts for SAM. This enables SAM to produce semantically discernible segmentation results for remote sensing images, a concept we have termed RSPrompter. We also propose several ongoing derivatives for instance segmentation tasks, drawing on recent advancements within the SAM community, and compare their performance with RSPrompter. Extensive experimental results, derived from the WHU building, NWPU VHR-10, and SSDD datasets, validate the effectiveness of our proposed method. The code for our method is publicly available at https://kychen.me/RSPrompter.", "authors": ["Keyan Chen", "Chenyang Liu", "Hao Chen", "Haotian Zhang", "Wenyuan Li", "Zhengxia Zou", "Zhenwei Shi"], "year": 2024, "venue": "IEEE Transactions on Geoscience and Remote Sensing", "cited_by_count": 271, "type": "article", "concepts": ["Segmentation", "Computer science", "Generalization", "Artificial intelligence", "Image segmentation"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4388129777", "doi": "https://doi.org/10.1016/j.jag.2023.103540", "title": "The Segment Anything Model (SAM) for remote sensing applications: From zero to one shot", "abstract": "Segmentation is an essential step for remote sensing image processing. This study aims to advance the application of the Segment Anything Model (SAM), an innovative image segmentation model by Meta AI, in the field of remote sensing image analysis. SAM is known for its exceptional generalization capabilities and zero-shot learning, making it a promising approach to processing aerial and orbital images from diverse geographical contexts. Our exploration involved testing SAM across multi-scale datasets using various input prompts, such as bounding boxes, individual points, and text descriptors. To enhance the model’s performance, we implemented a novel automated technique that combines a text-prompt-derived general example with one-shot training. This adjustment resulted in an improvement in accuracy, underscoring SAM’s potential for deployment in remote sensing imagery and reducing the need for manual annotation. Despite the limitations, encountered with lower spatial resolution images, SAM exhibits promising adaptability to remote sensing data analysis. We recommend future research to enhance the model’s proficiency through integration with supplementary fine-tuning techniques and other networks. Furthermore, we provide the open-source code of our modifications on online repositories, encouraging further and broader adaptations of SAM to the remote sensing domain.", "authors": ["Lucas Prado Osco", "Qiusheng Wu", "Eduardo Lopes de Lemos", "Wesley Nunes Gonçalves", "Ana Paula Marques Ramos", "Jonathan Li", "José Marcato"], "year": 2023, "venue": "International Journal of Applied Earth Observation and Geoinformation", "cited_by_count": 261, "type": "article", "concepts": ["Zero (linguistics)", "Shot (pellet)", "Geography", "Computer science", "Cartography"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4402727760", "doi": "https://doi.org/10.1109/cvpr52733.2024.01525", "title": "EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything", "abstract": "Segment Anything Model (SAM) has emerged as a powerful tool for numerous vision applications. A key component that drives the impressive performance for zero-shot trans-fer and high versatility is a super large Transformer model trained on the extensive high-quality SA -1 B dataset. While beneficial, the huge computation cost of SAM model has limited its applications to wider real-world applications. To address this limitation, we propose EfficientSAMs, light-weight SAM models that exhibits decent performance with largely reduced complexity. Our idea is based on leveraging masked image pretraining, SAMI, which learns to reconstruct features from SAM image encoder for effective visual representation learning. Further, we take SAMI-pretrained light-weight image encoders and mask decoder to build Effi-cientSAMs, and finetune the models on SA -1B for segment anything task. We perform evaluations on multiple vision tasks including image classification, object detection, in-stance segmentation, and semantic segmentation, and find that our proposed pretraining method, SAMI, consistently outperforms other masked image pretraining methods. On segment anything task such as zero-shot instance segmentation, our EfficientSAMs with SAMI-pretrained lightweight image encoders perform favorably with a significant gain (e.g., rv4 AP on COCOILVIS) over other fast SAM models. Our EfficientSAM code and models are available at here.", "authors": ["Yunyang Xiong", "Bala Varadarajan", "Lemeng Wu", "Xiaoyu Xiang", "Fanyi Xiao", "Chenchen Zhu", "Xiaoliang Dai", "Dilin Wang", "Fei Sun", "Forrest Iandola", "Raghuraman Krishnamoorthi", "Vikas Chandra"], "year": 2024, "venue": "", "cited_by_count": 168, "type": "article", "concepts": ["Computer science", "Image (mathematics)", "Artificial intelligence", "Computer vision"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4379141755", "doi": "https://doi.org/10.3390/diagnostics13111947", "title": "Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation", "abstract": "Medical image analysis plays an important role in clinical diagnosis. In this paper, we examine the recent Segment Anything Model (SAM) on medical images, and report both quantitative and qualitative zero-shot segmentation results on nine medical image segmentation benchmarks, covering various imaging modalities, such as optical coherence tomography (OCT), magnetic resonance imaging (MRI), and computed tomography (CT), as well as different applications including dermatology, ophthalmology, and radiology. Those benchmarks are representative and commonly used in model development. Our experimental results indicate that while SAM presents remarkable segmentation performance on images from the general domain, its zero-shot segmentation ability remains restricted for out-of-distribution images, e.g., medical images. In addition, SAM exhibits inconsistent zero-shot segmentation performance across different unseen medical domains. For certain structured targets, e.g., blood vessels, the zero-shot segmentation of SAM completely failed. In contrast, a simple fine-tuning of it with a small amount of data could lead to remarkable improvement of the segmentation quality, showing the great potential and feasibility of using fine-tuned SAM to achieve accurate medical image segmentation for a precision diagnostics. Our study indicates the versatility of generalist vision foundation models on medical imaging, and their great potential to achieve desired performance through fine-turning and eventually address the challenges associated with accessing large and diverse medical datasets in support of clinical diagnostics.", "authors": ["Peilun Shi", "Jianing Qiu", "Sai Mu Dalike Abaxi", "Wei Hao", "Frank P.-W. Lo", "Wu Yuan"], "year": 2023, "venue": "Diagnostics", "cited_by_count": 116, "type": "article", "concepts": ["Segmentation", "Medical imaging", "Optical coherence tomography", "Artificial intelligence", "Computer science"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4411124654", "doi": "https://doi.org/10.1016/j.bspc.2025.108086", "title": "SIT-SAM: A semantic-integration transformer that adapts the Segment Anything Model to zero-shot medical image semantic segmentation", "abstract": "", "authors": ["Wentao Shi", "Junjun He", "Yiqing Shen"], "year": 2025, "venue": "Biomedical Signal Processing and Control", "cited_by_count": 3, "type": "article", "concepts": ["Segmentation", "Computer science", "Transformer", "Artificial intelligence", "Zero (linguistics)"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4386781846", "doi": "https://doi.org/10.3390/s23187884", "title": "Enhancing Agricultural Image Segmentation with an Agricultural Segment Anything Model Adapter", "abstract": "The Segment Anything Model (SAM) is a versatile image segmentation model that enables zero-shot segmentation of various objects in any image using prompts, including bounding boxes, points, texts, and more. However, studies have shown that the SAM performs poorly in agricultural tasks like crop disease segmentation and pest segmentation. To address this issue, the agricultural SAM adapter (ASA) is proposed, which incorporates agricultural domain expertise into the segmentation model through a simple but effective adapter technique. By leveraging the distinctive characteristics of agricultural image segmentation and suitable user prompts, the model enables zero-shot segmentation, providing a new approach for zero-sample image segmentation in the agricultural domain. Comprehensive experiments are conducted to assess the efficacy of the ASA compared to the default SAM. The results show that the proposed model achieves significant improvements on all 12 agricultural segmentation tasks. Notably, the average Dice score improved by 41.48% on two coffee-leaf-disease segmentation tasks.", "authors": ["Yaqin Li", "Dandan Wang", "Yuan Cao", "Hao Li", "Jing Hu"], "year": 2023, "venue": "Sensors", "cited_by_count": 70, "type": "article", "concepts": ["Adapter (computing)", "Segmentation", "Computer science", "Artificial intelligence", "Image segmentation"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4379474533", "doi": "https://doi.org/10.48550/arxiv.2306.01567", "title": "Segment Anything in High Quality", "abstract": "The recent Segment Anything Model (SAM) represents a big leap in scaling up segmentation models, allowing for powerful zero-shot capabilities and flexible prompting. Despite being trained with 1.1 billion masks, SAM's mask prediction quality falls short in many cases, particularly when dealing with objects that have intricate structures. We propose HQ-SAM, equipping SAM with the ability to accurately segment any object, while maintaining SAM's original promptable design, efficiency, and zero-shot generalizability. Our careful design reuses and preserves the pre-trained model weights of SAM, while only introducing minimal additional parameters and computation. We design a learnable High-Quality Output Token, which is injected into SAM's mask decoder and is responsible for predicting the high-quality mask. Instead of only applying it on mask-decoder features, we first fuse them with early and final ViT features for improved mask details. To train our introduced learnable parameters, we compose a dataset of 44K fine-grained masks from several sources. HQ-SAM is only trained on the introduced detaset of 44k masks, which takes only 4 hours on 8 GPUs. We show the efficacy of HQ-SAM in a suite of 10 diverse segmentation datasets across different downstream tasks, where 8 out of them are evaluated in a zero-shot transfer protocol. Our code and pretrained models are at https://github.com/SysCV/SAM-HQ.", "authors": ["Ke Lei", "Mingqiao Ye", "Martin Danelljan", "Yifan Liu", "Yu‐Wing Tai", "Chi–Keung Tang", "Fisher Yu"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 109, "type": "preprint", "concepts": ["Computer science", "Segmentation", "Security token", "Artificial intelligence", "Quality (philosophy)"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4401819803", "doi": "https://doi.org/10.1016/j.media.2024.103310", "title": "MA-SAM: Modality-agnostic SAM adaptation for 3D medical image segmentation", "abstract": "", "authors": ["Cheng Chen", "Juzheng Miao", "Dufan Wu", "Aoxiao Zhong", "Zhiling Yan", "Sekeun Kim", "Jiang Hu", "Zhengliang Liu", "Lichao Sun", "Xiang Li", "Tianming Liu", "Pheng‐Ann Heng", "Quanzheng Li"], "year": 2024, "venue": "Medical Image Analysis", "cited_by_count": 146, "type": "article", "concepts": ["Segmentation", "Computer science", "Artificial intelligence", "Encoder", "Computer vision"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4391272787", "doi": "https://doi.org/10.48550/arxiv.2401.14159", "title": "Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks", "abstract": "We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.", "authors": ["Tianhe Ren", "Shilong Liu", "Ailing Zeng", "Jing Lin", "Kunchang Li", "He Cao", "Jiayu Chen", "Xinyu Huang", "Yukang Chen", "Feng Yan", "Zhaoyang Zeng", "Hao Zhang", "Feng Li", "Jie Yang", "Hongyang Li", "Qing Jiang", "Lei Zhang"], "year": 2024, "venue": "arXiv (Cornell University)", "cited_by_count": 88, "type": "preprint", "concepts": ["Pipeline (software)", "Computer science", "Segmentation", "Benchmark (surveying)", "Artificial intelligence"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4391991763", "doi": "https://doi.org/10.1109/tgrs.2024.3368168", "title": "Adapting Segment Anything Model for Change Detection in VHR Remote Sensing Images", "abstract": "Vision Foundation Models (VFMs) such as the Segment Anything Model (SAM) allow zero-shot or interactive segmentation of visual contents, thus they are quickly applied in a variety of visual scenes. However, their direct use in many Remote Sensing (RS) applications is often unsatisfactory due to the special imaging properties of RS images. In this work, we aim to utilize the strong visual recognition capabilities of VFMs to improve change detection (CD) in very high-resolution (VHR) remote sensing images (RSIs). We employ the visual encoder of FastSAM, a variant of the SAM, to extract visual representations in RS scenes. To adapt FastSAM to focus on some specific ground objects in RS scenes, we propose a convolutional adaptor to aggregate the task-oriented change information. Moreover, to utilize the semantic representations that are inherent to SAM features, we introduce a task-agnostic semantic learning branch to model the semantic latent in bi-temporal RSIs. The resulting method, SAM-CD, obtains superior accuracy compared to the SOTA fully-supervised CD methods and exhibits a sample-efficient learning ability that is comparable to semi-supervised CD methods. To the best of our knowledge, this is the first work that adapts VFMs to CD in VHR RS images.", "authors": ["Lei Ding", "Kun Zhu", "Daifeng Peng", "Hao Tang", "Kuiwu Yang", "Lorenzo Bruzzone"], "year": 2024, "venue": "IEEE Transactions on Geoscience and Remote Sensing", "cited_by_count": 137, "type": "article", "concepts": ["Remote sensing", "Change detection", "Computer science", "Artificial intelligence", "Computer vision"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4365816982", "doi": "https://doi.org/10.48550/arxiv.2304.05396", "title": "SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model", "abstract": "Foundation models have taken over natural language processing and image generation domains due to the flexibility of prompting. With the recent introduction of the Segment Anything Model (SAM), this prompt-driven paradigm has entered image segmentation with a hitherto unexplored abundance of capabilities. The purpose of this paper is to conduct an initial evaluation of the out-of-the-box zero-shot capabilities of SAM for medical image segmentation, by evaluating its performance on an abdominal CT organ segmentation task, via point or bounding box based prompting. We show that SAM generalizes well to CT data, making it a potential catalyst for the advancement of semi-automatic segmentation tools for clinicians. We believe that this foundation model, while not reaching state-of-the-art segmentation performance in our investigations, can serve as a highly potent starting point for further adaptations of such models to the intricacies of the medical domain. Keywords: medical image segmentation, SAM, foundation models, zero-shot learning", "authors": ["Saikat Roy", "Tassilo Wald", "Gregor Koehler", "Maximilian Rokuss", "Nico Disch", "Julius C. Holzschuh", "David Zimmerer", "Klaus H. Maier-Hein"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 64, "type": "preprint", "concepts": ["Segmentation", "Computer science", "Artificial intelligence", "Image (mathematics)", "Image segmentation"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4402727436", "doi": "https://doi.org/10.1109/cvpr52733.2024.02636", "title": "SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation", "abstract": "Zero-shot 6D object pose estimation involves the detection of novel objects with their 6D poses in cluttered scenes, presenting significant challenges for model generalizability. Fortunately, the recent Segment Anything Model (SAM) has showcased remarkable zero-shot transfer performance, which provides a promising solution to tackle this task. Motivated by this, we introduce SAM-6D, a novel framework designed to realize the task through two steps, including instance segmentation and pose estimation. Given the target objects, SAM-6D employs two dedicated sub-networks, namely Instance Segmentation Model (ISM) and Pose Estimation Model (PEM), to perform these steps on cluttered RGB-D images. ISM takes SAM as an advanced starting point to generate all possible object proposals and selectively preserves valid ones through meticulously crafted object matching scores in terms of semantics, appearance and geometry. By treating pose estimation as a partial-to-partial point matching problem, PEM performs a two-stage point matching process featuring a novel design of background tokens to construct dense 3D-3D correspondence, ultimately yielding the pose estimates. Without bells and whistles, SAM-6D outperforms the existing methods on the seven core datasets of the BOP Benchmark for both instance segmentation and pose estimation of novel objects.", "authors": ["Jiehong Lin", "Lihua Liu", "Dekun Lu", "Kui Jia"], "year": 2024, "venue": "", "cited_by_count": 82, "type": "article", "concepts": ["Zero (linguistics)", "Computer vision", "Object (grammar)", "Computer science", "Artificial intelligence"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4390971106", "doi": "https://doi.org/10.1109/bibm58861.2023.10386032", "title": "Segment Anything Model (SAM) for Medical Image Segmentation: A Preliminary Review", "abstract": "Medical image segmentation is a critical component in a variety of clinical applications, facilitating accurate diagnosis and treatment planning. The Segment Anything Model (SAM), a deep learning architecture, has emerged as a promising solution to the challenges inherent in medical image segmentation. SAM’s superior zero-shot capability allows it to generalize effectively, even in the absence of task-specific segmentation samples. This unique characteristic broadens its application potential across various medical image modalities. This paper provides an in-depth review of SAM, focusing on its application in medical image segmentation. The review discusses the advantages of deep learning image segmentation over traditional methods, emphasizing the superior accuracy, efficiency, and automation that deep learning models offer. The paper also highlights the applications of SAM across various medical imaging modalities, demonstrating its versatility and adaptability. A taxonomy of SAM approaches in medical image segmentation is presented, categorizing them based on modality, dimension, organ, dataset, prompt, and performance. Despite the promising results of SAM, challenges remain in the field of medical image segmentation. The paper identifies these challenges and suggests potential directions for future research. In conclusion, this review aims to provide a comprehensive understanding of SAM and its potential to revolutionize medical image analysis and contribute to advancements in healthcare.", "authors": ["Leying Zhang", "Xiaokang Deng", "Lu Yu"], "year": 2023, "venue": "", "cited_by_count": 87, "type": "review", "concepts": ["Segmentation", "Computer science", "Artificial intelligence", "Image segmentation", "Deep learning"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4393159543", "doi": "https://doi.org/10.1609/aaai.v38i7.28514", "title": "SurgicalSAM: Efficient Class Promptable Surgical Instrument Segmentation", "abstract": "The Segment Anything Model (SAM) is a powerful foundation model that has revolutionised image segmentation. To apply SAM to surgical instrument segmentation, a common approach is to locate precise points or boxes of instruments and then use them as prompts for SAM in a zero-shot manner. However, we observe two problems with this naive pipeline: (1) the domain gap between natural objects and surgical instruments leads to inferior generalisation of SAM; and (2) SAM relies on precise point or box locations for accurate segmentation, requiring either extensive manual guidance or a well-performing specialist detector for prompt preparation, which leads to a complex multi-stage pipeline. To address these problems, we introduce SurgicalSAM, a novel end-to-end efficient-tuning approach for SAM to effectively integrate surgical-specific information with SAM’s pre-trained knowledge for improved generalisation. Specifically, we propose a lightweight prototype-based class prompt encoder for tuning, which directly generates prompt embeddings from class prototypes and eliminates the use of explicit prompts for improved robustness and a simpler pipeline. In addition, to address the low inter-class variance among surgical instrument categories, we propose contrastive prototype learning, further enhancing the discrimination of the class prototypes for more accurate class prompting. The results of extensive experiments on both EndoVis2018 and EndoVis2017 datasets demonstrate that SurgicalSAM achieves state-of-the-art performance while only requiring a small number of tunable parameters. The source code is available at https://github.com/wenxi-yue/SurgicalSAM.", "authors": ["Wenxi Yue", "Jing Zhang", "Kun Hu", "Yong Xia", "Jiebo Luo", "Zhiyong Wang"], "year": 2024, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 72, "type": "article", "concepts": ["Surgical instrument", "Class (philosophy)", "Artificial intelligence", "Computer vision", "Segmentation"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4404177154", "doi": "https://doi.org/10.1007/978-3-031-72855-6_14", "title": "IRSAM: Advancing Segment Anything Model for Infrared Small Target Detection", "abstract": "", "authors": ["Mingjin Zhang", "Yuchun Wang", "Jie Guo", "Yunsong Li", "Xinbo Gao", "Jing Zhang"], "year": 2024, "venue": "Lecture notes in computer science", "cited_by_count": 115, "type": "book-chapter", "concepts": ["Computer science", "Infrared", "Artificial intelligence", "Computer vision", "Optics"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4388823372", "doi": "https://doi.org/10.1101/2023.11.17.567630", "title": "CellSAM: A Foundation Model for Cell Segmentation", "abstract": "Abstract Cells are a fundamental unit of biological organization, and identifying them in imaging data – cell segmentation – is a critical task for various cellular imaging experiments. While deep learning methods have led to substantial progress on this problem, most models are specialist models that work well for specific domains but cannot be applied across domains or scale well with large amounts of data. In this work, we present CellSAM, a universal model for cell segmentation that generalizes across diverse cellular imaging data. CellSAM builds on top of the Segment Anything Model (SAM) by developing a prompt engineering approach for mask generation. We train an object detector, CellFinder, to automatically detect cells and prompt SAM to generate segmentations. We show that this approach allows a single model to achieve human-level performance for segmenting images of mammalian cells, yeast, and bacteria collected across various imaging modalities. We show that CellSAM has strong zero-shot performance and can be improved with a few examples via few-shot learning. Additionally, we demonstrate how CellSAM can be applied across diverse bioimage analysis workflows. A deployed version of CellSAM is available at https://cellsam.deepcell.org/ .", "authors": ["Uriah Israel", "Markus Marks", "Rohit Dilip", "Qilin Li", "Changhua Yu", "Emily Laubscher", "Shenyi Li", "Morgan Schwartz", "Elora Pradhan", "Ada Ates", "Martin Abt", "Caitlin Brown", "Edward Pao", "Alexander Pearson-Goulart", "Pietro Perona", "Georgia Gkioxari", "Ross Barnowski", "Yisong Yue", "David Van Valen"], "year": 2023, "venue": "", "cited_by_count": 51, "type": "preprint", "concepts": ["Computer science", "Segmentation", "Workflow", "Artificial intelligence", "Market segmentation"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4394597311", "doi": "https://doi.org/10.1109/wacv57701.2024.00817", "title": "Segment anything, from space?", "abstract": "Recently, the first foundation model developed specifically for image segmentation tasks was developed, termed the \"Segment Anything Model\" (SAM). SAM can segment objects in input imagery based on cheap input prompts, such as one (or more) points, a bounding box, or a mask. The authors examined the zero-shot image segmentation accuracy of SAM on a large number of vision benchmark tasks and found that SAM usually achieved recognition accuracy similar to, or sometimes exceeding, vision models that had been trained on the target tasks. The impressive generalization of SAM for segmentation has major implications for vision researchers working on natural imagery. In this work, we examine whether SAM's performance extends to overhead imagery problems and help guide the community's response to its development. We examine SAM's performance on a set of diverse and widely studied benchmark tasks. We find that SAM does often generalize well to overhead imagery, although it fails in some cases due to the unique characteristics of overhead imagery and its common target objects. We report on these unique systematic failure cases for remote sensing imagery that may comprise useful future research for the community.", "authors": ["Simiao Ren", "Francesco Luzi", "Saad Lahrichi", "Kaleb Kassaw", "Leslie M. Collins", "Kyle Bradbury", "Jordan M. Malof"], "year": 2024, "venue": "", "cited_by_count": 58, "type": "article", "concepts": ["Space (punctuation)", "Computer science", "Operating system"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4388666409", "doi": "https://doi.org/10.1016/j.atech.2023.100367", "title": "The Segment Anything Model (SAM) for accelerating the smart farming revolution", "abstract": "Precision agriculture uses accurate identification and mapping of crop features by automated mechanisms. Using computer vision techniques implemented by supervised deep learning systems to solve many precision agricultural problems necessitates large-scale data collection and prolonged ground truth annotation by humans. The so-called foundation models in Artificial Intelligence (AI) are becoming increasingly significant. Meta AI Research is working on a project called Segment Anything to provide a base model for image segmentation. It can accomplish zero-shot generalisation to strange objects and images without additional training. This study evaluates the performance of the Segment Anything Model (SAM) for the problem of semantic segmentation of objects in the context of precision agriculture.", "authors": ["Alberto Carraro", "Marco Sozzi", "Francesco Marinello"], "year": 2023, "venue": "Smart Agricultural Technology", "cited_by_count": 51, "type": "article", "concepts": ["Computer science", "Segmentation", "Artificial intelligence", "Context (archaeology)", "Identification (biology)"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4367692241", "doi": "https://doi.org/10.48550/arxiv.2305.00035", "title": "SAM on Medical Images: A Comprehensive Study on Three Prompt Modes", "abstract": "The Segment Anything Model (SAM) made an eye-catching debut recently and inspired many researchers to explore its potential and limitation in terms of zero-shot generalization capability. As the first promptable foundation model for segmentation tasks, it was trained on a large dataset with an unprecedented number of images and annotations. This large-scale dataset and its promptable nature endow the model with strong zero-shot generalization. Although the SAM has shown competitive performance on several datasets, we still want to investigate its zero-shot generalization on medical images. As we know, the acquisition of medical image annotation usually requires a lot of effort from professional practitioners. Therefore, if there exists a foundation model that can give high-quality mask prediction simply based on a few point prompts, this model will undoubtedly become the game changer for medical image analysis. To evaluate whether SAM has the potential to become the foundation model for medical image segmentation tasks, we collected more than 12 public medical image datasets that cover various organs and modalities. We also explore what kind of prompt can lead to the best zero-shot performance with different modalities. Furthermore, we find that a pattern shows that the perturbation of the box size will significantly change the prediction accuracy. Finally, Extensive experiments show that the predicted mask quality varied a lot among different datasets. And providing proper prompts, such as bounding boxes, to the SAM will significantly increase its performance.", "authors": ["Dongjie Cheng", "Ziyuan Qin", "Zekun Jiang", "Shaoting Zhang", "Qicheng Lao", "Kang Li"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 54, "type": "preprint", "concepts": ["Computer science", "Generalization", "Segmentation", "Artificial intelligence", "Modalities"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4402915908", "doi": "https://doi.org/10.1109/cvprw63382.2024.00367", "title": "SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding", "abstract": "The landscape of publicly available vision foundation models (VFMs), such as CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed with distinct capabilities stemming from their pre-training objectives. For instance, CLIP excels in semantic understanding, while SAM specializes in spatial understanding for segmentation. In this work, we introduce a simple recipe to efficiently merge VFMs into a unified model that absorbs their expertise. Our method integrates techniques of multitask learning, continual learning, and distillation. Further, it demands significantly less computational cost compared to traditional multi-task training from scratch, and it only needs a small fraction of the pre-training datasets that were initially used to train individual models. By applying our method to SAM and CLIP, we obtain SAM-CLIP : a unified model that combines the capabilities of SAM and CLIP into a single vision transformer. Compared with deploying SAM and CLIP independently, our merged model, SAM-CLIP, reduces storage and compute costs for inference, making it well-suited for edge device applications. We show that SAM-CLIP not only retains the foundational strengths of SAM and CLIP, but also introduces synergistic functionalities, notably in zero-shot semantic segmentation, where SAM-CLIP establishes new state-of-the-art results on 5 benchmarks. It outperforms previous models that are specifically designed for this task by a large margin, including +6.8% and +5.9% mean IoU improvement on Pascal-VOC and COCO-Stuff datasets, respectively.", "authors": ["Haoxiang Wang", "Pavan Kumar Anasosalu Vasu", "Fartash Faghri", "Raviteja Vemulapalli", "Mehrdad Farajtabar", "Sachin Mehta", "Mohammad Rastegari", "Oncel Tuzel", "Hadi Pouransari"], "year": 2024, "venue": "", "cited_by_count": 65, "type": "article", "concepts": ["Foundation (evidence)", "Computer science", "Artificial intelligence", "Information retrieval", "Geography"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4383180710", "doi": "https://doi.org/10.48550/arxiv.2307.01197", "title": "Segment Anything Meets Point Tracking", "abstract": "The Segment Anything Model (SAM) has established itself as a powerful zero-shot image segmentation model, enabled by efficient point-centric annotation and prompt-based models. While click and brush interactions are both well explored in interactive image segmentation, the existing methods on videos focus on mask annotation and propagation. This paper presents SAM-PT, a novel method for point-centric interactive video segmentation, empowered by SAM and long-term point tracking. SAM-PT leverages robust and sparse point selection and propagation techniques for mask generation. Compared to traditional object-centric mask propagation strategies, we uniquely use point propagation to exploit local structure information agnostic to object semantics. We highlight the merits of point-based tracking through direct evaluation on the zero-shot open-world Unidentified Video Objects (UVO) benchmark. Our experiments on popular video object segmentation and multi-object segmentation tracking benchmarks, including DAVIS, YouTube-VOS, and BDD100K, suggest that a point-based segmentation tracker yields better zero-shot performance and efficient interactions. We release our code that integrates different point trackers and video segmentation benchmarks at https://github.com/SysCV/sam-pt.", "authors": ["Frano Rajič", "Lei Ke", "Yu‐Wing Tai", "Chi–Keung Tang", "Martin Danelljan", "Fisher Yu"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 38, "type": "preprint", "concepts": ["Segmentation", "Computer science", "Computer vision", "Artificial intelligence", "Benchmark (surveying)"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4366457071", "doi": "https://doi.org/10.48550/arxiv.2304.08506", "title": "When SAM Meets Medical Images: An Investigation of Segment Anything Model (SAM) on Multi-phase Liver Tumor Segmentation", "abstract": "Learning to segmentation without large-scale samples is an inherent capability of human. Recently, Segment Anything Model (SAM) performs the significant zero-shot image segmentation, attracting considerable attention from the computer vision community. Here, we investigate the capability of SAM for medical image analysis, especially for multi-phase liver tumor segmentation (MPLiTS), in terms of prompts, data resolution, phases. Experimental results demonstrate that there might be a large gap between SAM and expected performance. Fortunately, the qualitative results show that SAM is a powerful annotation tool for the community of interactive medical image segmentation.", "authors": ["Chuanfei Hu", "Xinde Li", "Ju, Shenghong", "Li, Xinde"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 37, "type": "preprint", "concepts": ["Segmentation", "Computer science", "Artificial intelligence", "Computer vision", "Image (mathematics)"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4401008913", "doi": "https://doi.org/10.1016/j.atech.2024.100515", "title": "Leaf only SAM: A segment anything pipeline for zero-shot automated leaf segmentation", "abstract": "", "authors": ["Dominic Williams", "Fraser Macfarlane", "Avril Britten"], "year": 2024, "venue": "Smart Agricultural Technology", "cited_by_count": 38, "type": "article", "concepts": ["Shot (pellet)", "Pipeline (software)", "Segmentation", "Zero (linguistics)", "Artificial intelligence"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4388283708", "doi": "https://doi.org/10.1109/tmm.2023.3330047", "title": "FoodSAM: Any Food Segmentation", "abstract": "In this paper, we explore the zero-shot capability of the Segment Anything Model (SAM) for food image segmentation. To address the lack of class-specific information in SAM-generated masks, we propose a novel framework, called <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">FoodSAM</i> . This innovative approach integrates the coarse semantic mask with SAM-generated masks to enhance semantic segmentation quality. Besides, we recognize that the ingredients in food can be supposed as independent individuals, which motivated us to perform instance segmentation on food images. Furthermore, FoodSAM extends its zero-shot capability to encompass panoptic segmentation by incorporating an object detector, which renders FoodSAM to effectively capture non-food object information. Drawing inspiration from the recent success of promptable segmentation, we also extend FoodSAM to promptable segmentation, supporting various prompt variants. Consequently, FoodSAM emerges as an all-encompassing solution capable of segmenting food items at multiple levels of granularity. Remarkably, this pioneering framework stands as the first-ever work to achieve instance, panoptic, and promptable segmentation on food images. Extensive experiments demonstrate the feasibility and impressing performance of FoodSAM, validating SAM's potential as a prominent and influential tool within the domain of food image segmentation.", "authors": ["Xing Lan", "Jiayi Lyu", "Hanyu Jiang", "Kun Dong", "Zehai Niu", "Yi Zhang", "Jian Xue"], "year": 2023, "venue": "IEEE Transactions on Multimedia", "cited_by_count": 41, "type": "article", "concepts": ["Segmentation", "Computer science", "Market segmentation", "Artificial intelligence", "Scale-space segmentation"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4395015601", "doi": "https://doi.org/10.1088/1742-6596/2722/1/012012", "title": "All-in-SAM: from Weak Annotation to Pixel-wise Nuclei Segmentation with Prompt-based Finetuning", "abstract": "The Segment Anything Model (SAM) is a recently proposed prompt-based segmentation model in a generic zero-shot segmentation approach. With the zero-shot segmentation capacity, SAM achieved impressive flexibility and precision on various segmentation tasks. However, the current pipeline requires manual prompts during the inference stage, which is still resource intensive for biomedical image segmentation. In this paper, instead of using prompts during the inference stage, we introduce a pipeline that utilizes the SAM, called all-in-SAM, through the entire AI development workflow (from annotation generation to model finetuning) without requiring manual prompts during the inference stage. Specifically, SAM is first employed to generate pixel-level annotations from weak prompts (e.g., points, bounding box). Then, the pixel-level annotations are used to finetune the SAM segmentation model rather than training from scratch. Our experimental results reveal two key findings: 1) the proposed pipeline surpasses the state-of-the-art methods in a nuclei segmentation task on the public Monuseg dataset, and 2) the utilization of weak and few annotations for SAM finetuning achieves competitive performance compared to using strong pixelwise annotated data.", "authors": ["Can Cui", "Ruining Deng", "Quan Liu", "Tianyuan Yao", "Shunxing Bao", "Lucas W. Remedios", "Bennett A. Landman", "Yucheng Tang", "Yuankai Huo"], "year": 2024, "venue": "Journal of Physics Conference Series", "cited_by_count": 35, "type": "article", "concepts": ["Annotation", "Segmentation", "Artificial intelligence", "Computer science", "Pixel"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4396825686", "doi": "https://doi.org/10.1016/j.conbuildmat.2024.136573", "title": "Fine-tuning vision foundation model for crack segmentation in civil infrastructures", "abstract": "", "authors": ["Kang Ge", "Chao Wang", "Yutao Guo", "Yutong Tang", "Zhen Hu", "Hongzhu Chen"], "year": 2024, "venue": "Construction and Building Materials", "cited_by_count": 45, "type": "article", "concepts": ["Foundation (evidence)", "Segmentation", "Artificial intelligence", "Computer science", "Computer vision"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4403649753", "doi": "https://doi.org/10.1007/978-3-031-72390-2_60", "title": "MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation", "abstract": "", "authors": ["Taha Koleilat", "Hojat Asgariandehkordi", "Hassan Rivaz", "Yiming Xiao"], "year": 2024, "venue": "Lecture notes in computer science", "cited_by_count": 33, "type": "book-chapter", "concepts": ["Bridging (networking)", "Computer science", "Image segmentation", "Computer vision", "Artificial intelligence"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4402961794", "doi": "https://doi.org/10.1007/978-3-031-72775-7_24", "title": "Open-Vocabulary SAM: Segment and Recognize Twenty-Thousand Classes Interactively", "abstract": "", "authors": ["Haobo Yuan", "Xiangtai Li", "Chong Zhou", "Yining Li", "Kai Chen", "Chen Change Loy"], "year": 2024, "venue": "Lecture notes in computer science", "cited_by_count": 34, "type": "book-chapter", "concepts": ["Computer science", "Vocabulary", "Artificial intelligence", "Natural language processing", "Computer graphics (images)"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4391929768", "doi": "https://doi.org/10.1109/bibe60311.2023.00025", "title": "Zero-Shot Performance of the Segment Anything Model (SAM) in 2D Medical Imaging: A Comprehensive Evaluation and Practical Guidelines", "abstract": "This study evaluates the potential of the “Segment Anything Model” (SAM) as a robust alternative for medical imaging segmentation in a zero-shot learning context. We evaluate SAM's performance across six diverse medical imaging datasets spanning four different imaging modalities. By employing eight unique prompting strategies we reveal comprehensive insights into SAM's adaptability. The Bounding Box strategy, with its variations, matched or even outperformed existing benchmarks. On the Breast Ultrasound Images dataset, SAM notably outperformed SOTA models, attesting to its capability as a robust zero-shot segmentation tool. Conversely, challenges arose with datasets having indistinct boundaries and inconsistent annotations, as in skin lesion images. The study also establishes a practical set of guidelines aimed at optimizing SAM's clinical usage. The findings underscore SAM's potential as a powerful, versatile tool for medical imaging segmentation, alleviating the burden of manual segmentation and potentially improving ground truth masks for labeling new datasets. With its minimal resource requirements and promising results, SAM represents an exciting advancement in medical imaging analysis.", "authors": ["Christian Mattjie", "Luís Vinícius de Moura", "Rafaela Ravazio", "Lucas Silveira Kupssinskü", "Otávio Parraga", "Marcelo Mussi Delucis", "Rodrigo C. Barros"], "year": 2023, "venue": "", "cited_by_count": 33, "type": "article", "concepts": ["Zero (linguistics)", "Shot (pellet)", "Computer science", "Medical imaging", "Medical physics"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4389213248", "doi": "https://doi.org/10.1007/978-3-031-47401-9_23", "title": "SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation", "abstract": "", "authors": ["An Wang", "Mobarakol Islam", "Mengya Xu", "Yang Zhang", "Hongliang Ren"], "year": 2023, "venue": "Lecture notes in computer science", "cited_by_count": 38, "type": "book-chapter", "concepts": ["Computer science", "Robustness (evolution)", "Minimum bounding box", "Generalizability theory", "Bounding overwatch"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4402916510", "doi": "https://doi.org/10.1109/cvprw63382.2024.00526", "title": "Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero-shot Medical Image Segmentation", "abstract": "The Segment Anything Model (SAM) and CLIP are remarkable vision foundation models (VFMs). SAM, a prompt-driven segmentation model, excels in segmentation tasks across diverse domains, while CLIP is renowned for its zero-shot recognition capabilities. However, their unified potential has not yet been explored in medical image segmentation. To adapt SAM, to medical imaging, existing methods primarily rely on tuning strategies that require extensive data or prior prompts tailored to the specific task, making it particularly challenging when only a limited number of data samples are available. This work presents an in-depth exploration of integrating SAM and CLIP into a unified framework for medical image segmentation. Specifically, we propose a simple unified framework, SaLIP, for organ segmentation. Initially, SAM is used for part-based segmentation within the image, followed by CLIP to retrieve the mask corresponding to the region of interest (ROI) from the pool of SAM’s generated masks. Finally, SAM is prompted by the retrieved ROI to segment a specific organ. Thus, SaLIP is training/fine-tuning free and does not rely on domain expertise or labeled data for prompt engineering. Our method shows substantial enhancements in zero-shot segmentation, showcasing notable improvements in DICE scores across diverse segmentation tasks like brain (63.46%), lung (50.11%), and fetal head (30.82%), when compared to un-prompted SAM. Code and text prompts are available at SaLIP.", "authors": ["Sidra Aleem", "Fangyijie Wang", "Mayug Maniparambil", "Eric Arazo", "Julia Dietlmeier", "Kathleen M. Curran", "Noel E. O’Connor", "Suzanne Little"], "year": 2024, "venue": "", "cited_by_count": 26, "type": "article", "concepts": ["Cascade", "Shot (pellet)", "Computer science", "Segmentation", "Artificial intelligence"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4402781391", "doi": "https://doi.org/10.1109/cvpr52733.2024.01074", "title": "One-Prompt to Segment All Medical Images", "abstract": "Large foundation models, known for their strong zero-shot generalization, have excelled in visual and language applications. However, applying them to medical image segmentation, a domain with diverse imaging types and target labels, remains an open challenge. Current approaches, such as adapting interactive segmentation models like Segment Anything Model (SAM), require user prompts for each sample during inference. Alternatively, trans-fer learning methods like few/one-shot models demand la-beled samples, leading to high costs. This paper intro-duces a new paradigm toward the universal medical image segmentation, termed ‘One-Prompt Segmentation.’ One-Prompt Segmentation combines the strengths of one-shot and interactive methods. In the inference stage, with just one prompted sample, it can adeptly handle the un-seen task in a single forward pass. We train One-Prompt Model on 64 open-source medical datasets, accompanied by the collection of over 3,000 clinician-labeled prompts. Tested on 14 previously unseen datasets, the One-Prompt Model showcases superior zero-shot segmentation capabil-ities, outperforming a wide range of related methods. The code and data is released as https://github.com/KidsWithTokens/one-prompt.", "authors": ["Junde Wu", "Min Xu"], "year": 2024, "venue": "", "cited_by_count": 29, "type": "article", "concepts": ["Computer science", "Computer vision", "Medical imaging", "Artificial intelligence"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4409363472", "doi": "https://doi.org/10.1609/aaai.v39i19.34255", "title": "TinySAM: Pushing the Envelope for Efficient Segment Anything Model", "abstract": "Recently segment anything model (SAM) has shown powerful segmentation capability and has drawn great attention in computer vision fields. Massive following works have developed various applications based on the pre-trained SAM and achieved impressive performance on downstream vision tasks. However, SAM consists of heavy architectures and requires massive computational capacity, which hinders the further application of SAM on computation constrained edge devices. To this end, in this paper we propose a framework to obtain a tiny segment anything model (TinySAM) while maintaining the strong zero-shot performance. We first propose a full-stage knowledge distillation method with hard prompt sampling and hard mask weighting strategy to distill a lightweight student model. We also adapt the post-training quantization to the prompt-based segmentation task and further reduce the computational cost. Moreover, a hierarchical segmenting everything strategy is proposed to accelerate the everything inference by 2× with almost no performance degradation. With all these proposed methods, our TinySAM leads to orders of magnitude computational reduction and pushes the envelope for efficient segment anything task. Extensive experiments on various zero-shot transfer tasks demonstrate the significantly advantageous performance of our TinySAM against counterpart methods.", "authors": ["Han Shu", "Wenshuo Li", "Yehui Tang", "Yiman Zhang", "Yihao Chen", "Houqiang Li", "Yunhe Wang", "Xinghao Chen"], "year": 2025, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 20, "type": "article", "concepts": ["Envelope (radar)", "Computer science", "Telecommunications", "Radar"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4392154986", "doi": "https://doi.org/10.3390/rs16050797", "title": "Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model’s Generalizability in Permafrost Mapping", "abstract": "This paper assesses trending AI foundation models, especially emerging computer vision foundation models and their performance in natural landscape feature segmentation. While the term foundation model has quickly garnered interest from the geospatial domain, its definition remains vague. Hence, this paper will first introduce AI foundation models and their defining characteristics. Built upon the tremendous success achieved by Large Language Models (LLMs) as the foundation models for language tasks, this paper discusses the challenges of building foundation models for geospatial artificial intelligence (GeoAI) vision tasks. To evaluate the performance of large AI vision models, especially Meta’s Segment Anything Model (SAM), we implemented different instance segmentation pipelines that minimize the changes to SAM to leverage its power as a foundation model. A series of prompt strategies were developed to test SAM’s performance regarding its theoretical upper bound of predictive accuracy, zero-shot performance, and domain adaptability through fine-tuning. The analysis used two permafrost feature datasets, ice-wedge polygons and retrogressive thaw slumps because (1) these landform features are more challenging to segment than man-made features due to their complicated formation mechanisms, diverse forms, and vague boundaries; (2) their presence and changes are important indicators for Arctic warming and climate change. The results show that although promising, SAM still has room for improvement to support AI-augmented terrain mapping. The spatial and domain generalizability of this finding is further validated using a more general dataset EuroCrops for agricultural field mapping. Finally, we discuss future research directions that strengthen SAM’s applicability in challenging geospatial domains.", "authors": ["Wenwen Li", "Chia-Yu Hsu", "Sizhe Wang", "Yezhou Yang", "Hyunho Lee", "Anna Liljedahl", "Chandi Witharana", "Yili Yang", "Brendan M. Rogers", "Samantha T. Arundel", "Matthew B. Jones", "Kenton McHenry", "Patricia Solís"], "year": 2024, "venue": "Remote Sensing", "cited_by_count": 28, "type": "article", "concepts": ["Generalizability theory", "Permafrost", "Foundation (evidence)", "Geology", "Oceanography"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4391109864", "doi": "https://doi.org/10.1038/s41467-024-44824-z", "title": "Segment anything in medical images", "abstract": "", "authors": ["Jun Ma", "Yuting He", "Feifei Li", "Lin Han", "Chenyu You", "Bo Wang"], "year": 2024, "venue": "Nature Communications", "cited_by_count": 1973, "type": "article", "concepts": ["Computer science", "Computational biology", "Medicine", "Biology"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4397001004", "doi": "https://doi.org/10.1016/j.compag.2024.109045", "title": "An innovative segment anything model for precision poultry monitoring", "abstract": "", "authors": ["Xiao Yang", "Haixing Dai", "Zihao Wu", "Ramesh Bahadur Bist", "Sachin Subedi", "Jin Sun", "Guoyu Lu", "Changying Li", "Tianming Liu", "Lilong Chai"], "year": 2024, "venue": "Computers and Electronics in Agriculture", "cited_by_count": 23, "type": "article", "concepts": ["Computer science", "Engineering", "Artificial intelligence"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4390190003", "doi": "https://doi.org/10.1109/iccvw60793.2023.00184", "title": "Semantic Segmentation using Foundation Models for Cultural Heritage: an Experimental Study on Notre-Dame de Paris", "abstract": "The zero-shot performance of foundation models has captured a lot of attention. Specifically, the Segment Anything Model (SAM) has gained popularity in computer vision due to its label-free segmentation capabilities. Our study proposes using SAM on cultural heritage data, specifically images of Notre-Dame de Paris, with a controlled vocabulary. SAM can successfully identify objects within the cathedral. To further improve segmentation, we utilized Grounding DINO to detect objects and CLIP to automatically add labels from the segmentation masks generated by SAM. Our study demonstrates the usefulness of foundation models for zero-shot semantic segmentation of cultural heritage data.", "authors": ["Kévin Réby", "Anaïs Guilhelm", "Livio De Luca"], "year": 2023, "venue": "", "cited_by_count": 16, "type": "article", "concepts": ["Segmentation", "Foundation (evidence)", "Popularity", "Computer science", "Cultural heritage"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4382319386", "doi": "https://doi.org/10.48550/arxiv.2306.13731", "title": "How to Efficiently Adapt Large Segmentation Model(SAM) to Medical Images", "abstract": "The emerging scale segmentation model, Segment Anything (SAM), exhibits impressive capabilities in zero-shot segmentation for natural images. However, when applied to medical images, SAM suffers from noticeable performance drop. To make SAM a real ``foundation model\" for the computer vision community, it is critical to find an efficient way to customize SAM for medical image dataset. In this work, we propose to freeze SAM encoder and finetune a lightweight task-specific prediction head, as most of weights in SAM are contributed by the encoder. In addition, SAM is a promptable model, while prompt is not necessarily available in all application cases, and precise prompts for multiple class segmentation are also time-consuming. Therefore, we explore three types of prompt-free prediction heads in this work, include ViT, CNN, and linear layers. For ViT head, we remove the prompt tokens in the mask decoder of SAM, which is named AutoSAM. AutoSAM can also generate masks for different classes with one single inference after modification. To evaluate the label-efficiency of our finetuning method, we compare the results of these three prediction heads on a public medical image segmentation dataset with limited labeled data. Experiments demonstrate that finetuning SAM significantly improves its performance on medical image dataset, even with just one labeled volume. Moreover, AutoSAM and CNN prediction head also has better segmentation accuracy than training from scratch and self-supervised learning approaches when there is a shortage of annotations.", "authors": ["Xinrong Hu", "Xiaowei Xu", "Yiyu Shi"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 29, "type": "preprint", "concepts": ["Computer science", "Segmentation", "Artificial intelligence", "Encoder", "Inference"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4402753822", "doi": "https://doi.org/10.1109/cvpr52733.2024.02207", "title": "Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation", "abstract": "The success of large language models has inspired the computer vision community to explore image segmentation foundation model that is able to zero/few-shot generalize through prompt engineering. Segment-Anything (SAM), among others, is the state-of-the-art image segmentation foundation model demonstrating strong zero/few-shot generalization. Despite the success, recent studies reveal the weakness of SAM under strong distribution shift. In particular, SAM performs awkwardly on corrupted natural images, camouflaged images, medical images, etc. Motivated by the observations, we aim to develop a self-training based strategy to adapt SAM to target distribution. Given the unique challenges of large source dataset, high computation cost and incorrect pseudo label, we propose a weakly supervised self-training architecture with anchor regularization and low-rank finetuning to improve the robustness and computation efficiency of adaptation. We validate the effectiveness on 5 types of downstream segmentation tasks including natural clean/corrupted images, medical images, camouflaged images and robotic images. Our proposed method is task-agnostic in nature and outperforms pre-trained SAM and state-of-the-art domain adaptation methods on almost all downstream tasks with the same testing prompt inputs.", "authors": ["Haojie Zhang", "Yongyi Su", "Xun Xu", "Kui Jia"], "year": 2024, "venue": "", "cited_by_count": 24, "type": "article", "concepts": ["Generalization", "Foundation (evidence)", "Segmentation", "Adaptation (eye)", "Computer science"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4405594810", "doi": "https://doi.org/10.1016/j.rineng.2024.103784", "title": "Tea leaf disease detection using segment anything model and deep convolutional neural networks", "abstract": "", "authors": ["A. Balasundaram", "Puma Sundaresan", "Arnav Bhavsar", "Mishti Mattu", "Muthu Subash Kavitha", "Ayesha Shaik"], "year": 2024, "venue": "Results in Engineering", "cited_by_count": 28, "type": "article", "concepts": ["Convolutional neural network", "Artificial intelligence", "Computer science", "Pattern recognition (psychology)"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4386987615", "doi": "https://doi.org/10.1016/j.icarus.2023.115797", "title": "A flexible deep learning crater detection scheme using Segment Anything Model (SAM)", "abstract": "Craters are one of the most important morphological features in planetary exploration. To that extent, detecting, mapping and counting craters is a mainstream process in planetary science, done primarily manually, which is a very laborious, time-consuming and inconsistent process. Recently, machine learning (ML) and computer vision have been successfully applied for both detecting craters and estimating their size. Existing ML models for automated crater detection have been trained in specific types of data e.g. digital elevation model (DEM), images and associated metadata from orbiters such as the Lunar Reconnaissance Orbiter Camera (LROC) etc.. Due to that, each of the resulting ML schemes is applicable and reliable only to the type of data used during the training process. Data from different sources, angles and setups can compromise the reliability of these ML schemes. In this paper we present a flexible crater detection scheme that is based on the recently proposed Segment Anything Model (SAM) from META AI. SAM is a prompt-able segmentation system with zero-shot generalization to unfamiliar objects and images without the need for additional training. Using SAM, without additional training and fine-tuning, we can successfully identify crater-looking objects in various types of data (e,g, raw satellite images Level-1 and 2 products, DEMs etc.) for different setups (e.g. Lunar, Mars) and different capturing angles. Moreover, using shape indexes, we only keep the segmentation masks of crater-like features. These masks are subsequently fitted with a circle or an ellipse, recovering both the location and the size/geometry of the detected craters.", "authors": ["Iraklis Giannakis", "Anshuman Bhardwaj", "Lydia Sam", "Georgios Leontidis"], "year": 2023, "venue": "Icarus", "cited_by_count": 39, "type": "article", "concepts": ["Impact crater", "Orbiter", "Computer science", "Artificial intelligence", "Contouring"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4406322219", "doi": "https://doi.org/10.1109/tgrs.2025.3529031", "title": "PointSAM: Pointly-Supervised Segment Anything Model for Remote Sensing Images", "abstract": "Segment anything model (SAM) is an advanced foundational model for image segmentation, which is gradually being applied to remote sensing images (RSIs). Due to the domain gap between RSIs and natural images, traditional methods typically use SAM as a source pretrained model and fine-tune it with fully supervised masks. Unlike these methods, our work focuses on fine-tuning SAM using more convenient and challenging point annotations. Leveraging SAM’s zero-shot capability, we adopt a self-training framework that iteratively generates pseudolabels. However, noisy labels in pseudolabels can cause error accumulation. To address this, we introduce prototype-based regularization (PBR), where target prototypes are extracted from the dataset and matched to predicted prototypes using the Hungarian algorithm to guide learning in the correct direction. In addition, RSIs have complex backgrounds and densely packed objects, making it possible for point prompts to mistakenly group multiple objects as one. To resolve this, we propose a negative prompt calibration (NPC) method based on the nonoverlapping nature of instance masks, where overlapping masks are used as negative signals to refine segmentation. Combining these techniques, we present a novel pointly-supervised SAM (PointSAM). We conduct experiments on three RSI datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method significantly outperforms direct testing with SAM, SAM2, and other comparison methods. In addition, PointSAM can act as a point-to-box converter for oriented object detection, achieving promising results and indicating its potential for other point-supervised tasks. The code is available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/Lans1ng/PointSAM</uri>.", "authors": ["Nanqing Liu", "Xun Xu", "Yongyi Su", "Haojie Zhang", "Heng-Chao Li"], "year": 2025, "venue": "IEEE Transactions on Geoscience and Remote Sensing", "cited_by_count": 38, "type": "article", "concepts": ["Remote sensing", "Computer science", "Computer vision", "Artificial intelligence", "Geology"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4402980275", "doi": "https://doi.org/10.1109/icme57554.2024.10687602", "title": "PA-SAM: Prompt Adapter SAM for High-Quality Image Segmentation", "abstract": "The Segment Anything Model (SAM) has exhibited outstanding performance in various image segmentation tasks. Despite being trained with over a billion masks, SAM faces challenges in mask prediction quality in numerous scenarios, especially in real-world contexts. In this paper, we introduce a novel prompt-driven adapter into SAM, namely Prompt Adapter Segment Anything Model (PA-SAM), aiming to enhance the segmentation mask quality of the original SAM. By exclusively training the prompt adapter, PA-SAM extracts detailed information from images and optimizes the mask decoder feature at both sparse and dense prompt levels, improving the segmentation performance of SAM to produce high-quality masks. Experimental results demonstrate that our PA-SAM outperforms other SAM-based methods in high-quality, zero-shot, and open-set segmentation. We’re making the source code and models available at https://github.com/xzz2/pa-sam.", "authors": ["Zhaozhi Xie", "Bochen Guan", "Weihao Jiang", "Muyang Yi", "Yue Ding", "Hongtao Lu", "Lei Zhang"], "year": 2024, "venue": "", "cited_by_count": 15, "type": "article", "concepts": ["Adapter (computing)", "Computer science", "Artificial intelligence", "Computer vision", "Image segmentation"], "search_query": "segment anything model SAM zero-shot segmentation"}
{"openalex_id": "https://openalex.org/W4382462760", "doi": "https://doi.org/10.1609/aaai.v37i2.25353", "title": "Exploring CLIP for Assessing the Look and Feel of Images", "abstract": "Measuring the perception of visual content is a long-standing problem in computer vision. Many mathematical models have been developed to evaluate the look or quality of an image. Despite the effectiveness of such tools in quantifying degradations such as noise and blurriness levels, such quantification is loosely coupled with human language. When it comes to more abstract perception about the feel of visual content, existing methods can only rely on supervised models that are explicitly trained with labeled data collected via laborious user study. In this paper, we go beyond the conventional paradigms by exploring the rich visual language prior encapsulated in Contrastive Language-Image Pre-training (CLIP) models for assessing both the quality perception (look) and abstract perception (feel) of images without explicit task-specific training. In particular, we discuss effective prompt designs and show an effective prompt pairing strategy to harness the prior. We also provide extensive experiments on controlled datasets and Image Quality Assessment (IQA) benchmarks. Our results show that CLIP captures meaningful priors that generalize well to different perceptual assessments.", "authors": ["Jianyi Wang", "Kelvin C. K. Chan", "Chen Change Loy"], "year": 2023, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "cited_by_count": 461, "type": "article", "concepts": ["Computer science", "Perception", "Task (project management)", "Quality (philosophy)", "Prior probability"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W2067404301", "doi": "https://doi.org/10.1111/j.1365-2648.2007.04569.x", "title": "The qualitative content analysis process", "abstract": "Inductive content analysis is used in cases where there are no previous studies dealing with the phenomenon or when it is fragmented. A deductive approach is useful if the general aim was to test a previous theory in a different situation or to compare categories at different time periods.", "authors": ["Satu Elo", "Helvi Kyngäs"], "year": 2008, "venue": "Journal of Advanced Nursing", "cited_by_count": 21175, "type": "article", "concepts": ["Operationalization", "Content analysis", "Phenomenon", "Computer science", "Content (measure theory)"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4402704606", "doi": "https://doi.org/10.1109/cvpr52733.2024.02408", "title": "Q-Instruct: Improving Low-Level Visual Abilities for Multi-Modality Foundation Models", "abstract": "Multi-modality large language models (MLLMs), as represented by GPT-4V, have introduced a paradigm shift for visual perception and understanding tasks, that a variety of abilities can be achieved within one foundation model. While current MLLMs demonstrate primary low-level visual abilities from the identification of low-level visual attributes (e.g., clarity, brightness) to the evaluation on image quality, there's still an imperative to further improve the accuracy of MLLMs to substantially alleviate human burdens. To address this, we collect the first dataset consisting of human natural language feedback on low-level vision. Each feedback offers a comprehensive description of an image's low-level visual attributes, culminating in an overall quality assessment. The constructed Q-Pathway dataset includes 58K detailed human feedbacks on 18,973 multi-sourced images with diverse low-level appearance. To ensure MLLMs can adeptly handle diverse queries, we further propose a GPT-participated transformation to convert these feedbacks into a rich set of 200K instruction-response pairs, termed Q-Instruct. Experimental results indicate that the Q-Instruct consistently elevates various low-level visual capabilities across multiple base models. We anticipate that our datasets can pave the way for a future that foundation models can assist humans on low-level visual tasks.", "authors": ["Haoning Wu", "Zicheng Zhang", "Erli Zhang", "Chaofeng Chen", "Liang Liao", "Annan Wang", "Kaixin Xu", "Chunyi Li", "Jingwen Hou", "Guangtao Zhai", "Geng Xue", "Wenxiu Sun", "Qiong Yan", "Weisi Lin"], "year": 2024, "venue": "", "cited_by_count": 62, "type": "article", "concepts": ["Modality (human–computer interaction)", "Foundation (evidence)", "Computer science", "Artificial intelligence", "Human–computer interaction"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W2137295153", "doi": "https://doi.org/10.1137/1114019", "title": "Non-Parametric Estimation of a Multivariate Probability Density", "abstract": "Previous article Next article Non-Parametric Estimation of a Multivariate Probability DensityV. A. EpanechnikovV. A. Epanechnikovhttps://doi.org/10.1137/1114019PDFBibTexSections ToolsAdd to favoritesExport CitationTrack CitationsEmail SectionsAbout[1] Emanuel Parzen, On estimation of a probability density function and mode, Ann. Math. Statist., 33 (1962), 1065–1076 MR0143282 0116.11302 CrossrefGoogle Scholar[2] Murray Rosenblatt, Remarks on some nonparametric estimates of a density function, Ann. Math. Statist., 27 (1956), 832–837 MR0079873 0073.14602 CrossrefGoogle Scholar[3] G. M. Manija, Remarks on non-parametric estimates of a two-dimensional density function, Soobšč. Akad. Nauk Gruzin. SSR, 27 (1961), 385–390 MR0143303 Google Scholar[4] E. A. Nadaraya, Estimation of a bivariate probability density, Soobshch. Akad. Nauk Gruz. SSR, 36 (1964), 267–268 Google Scholar[5] R. E. Bellman, , I. Glicksberg and , O. A. Gross, Some aspects of the mathematical theory of control processes, Rand Corporation, Santa Monica, Calif., Rep. No. R-313, 1958rm xix+244 MR0094281 0086.11703 Google Scholar Previous article Next article FiguresRelatedReferencesCited byDetails Kernel-based learning of birth process from evolving spatiotemporal RFS data stream in SMC CPHD filter for multi-target trackingSignal Processing, Vol. 203 Cross Ref Assessing spatial connectivity effects on daily streamflow forecasting using Bayesian-based graph neural networkScience of The Total Environment, Vol. 855 Cross Ref Joint Source-Channel Decoding of Polar Codes for HEVC-Based Video StreamingACM Transactions on Multimedia Computing, Communications, and Applications, Vol. 18, No. 4 Cross Ref A novel structure adaptive new information priority discrete grey prediction model and its application in renewable energy generation forecastingApplied Energy, Vol. 325 Cross Ref A novel structure adaptive fractional discrete grey forecasting model and its application in China’s crude oil production predictionExpert Systems with Applications, Vol. 207 Cross Ref Age-varying effects of repeated emergency department presentations for children in Canada6 May 2022 | Journal of Health Services Research & Policy, Vol. 27, No. 4 Cross Ref Probabilistic adaptive power pinch analysis for islanded hybrid energy storage systemsJournal of Energy Storage, Vol. 54 Cross Ref Should I stay or should I fly? Migration phenology, individual-based migration decision and seasonal changes in foraging behaviour of Common Woodpigeons17 August 2022 | The Science of Nature, Vol. 109, No. 5 Cross Ref Machine learning and optimization based decision-support tool for seed variety selection24 September 2022 | Annals of Operations Research, Vol. 65 Cross Ref Estimating regional income indicators under transformations and access to limited population auxiliary information16 September 2022 | Journal of the Royal Statistical Society: Series A (Statistics in Society), Vol. 37 Cross Ref Automated calibration of model-driven reconstructions in atom probe tomography1 July 2022 | Journal of Physics D: Applied Physics, Vol. 55, No. 37 Cross Ref Point and interval forecasting of ultra-short-term wind power based on a data-driven method and hybrid deep learning modelEnergy, Vol. 254 Cross Ref Deep attention ConvLSTM-based adaptive fusion of clear-sky physical prior knowledge and multivariable historical information for probabilistic prediction of photovoltaic powerExpert Systems with Applications, Vol. 202 Cross Ref The Taxonomy of Mineral Occurrence Rarity and Endemicity21 October 2022 | The Canadian Mineralogist, Vol. 60, No. 5 Cross Ref Estimation of wind speed distribution with time window and new kernel functionJournal of Renewable and Sustainable Energy, Vol. 14, No. 5 Cross Ref Future crop risk estimation due to drought, extreme temperature, hail, lightning, and tornado at the census tract level in louisiana23 August 2022 | Frontiers in Environmental Science, Vol. 10 Cross Ref Translocation detection from Hi‐C data via scan statistics10 August 2022 | Biometrics, Vol. 8 Cross Ref Does restricting therapeutic antibiotics use influence efficiency of pig farms? Evidence from Denmark’s Yellow Card Initiative18 May 2022 | European Review of Agricultural Economics, Vol. 49, No. 4 Cross Ref Computational approach to modeling microbiome landscapes associated with chronic human disease progression4 August 2022 | PLOS Computational Biology, Vol. 18, No. 8 Cross Ref Spillovers between exchange rate pressure and CDS bid-ask spreads, reserve assets and oil prices using the quantile ARDL modelInternational Economics, Vol. 170 Cross Ref Liability Structure and Risk Taking: Evidence from the Money Market Fund Industry18 June 2021 | Journal of Financial and Quantitative Analysis, Vol. 57, No. 5 Cross Ref Security risk assessment of wind integrated power system using Parzen window density estimation6 January 2022 | Electrical Engineering, Vol. 104, No. 4 Cross Ref Predator or prey? Effects of farm growth on neighbouring farms25 July 2022 | Journal of Agricultural Economics, Vol. 4 Cross Ref Multimodel Errors and Emergence Times in Climate Attribution StudiesJournal of Climate, Vol. 35, No. 14 Cross Ref The hierarchical structure of galactic haloes: generalized N -dimensional clustering with C lu STAR-ND20 June 2022 | Monthly Notices of the Royal Astronomical Society, Vol. 514, No. 4 Cross Ref Effective End-to-End Learning Framework for Economic DispatchIEEE Transactions on Network Science and Engineering, Vol. 9, No. 4 Cross Ref 4-D Gesture Sensing Using Reconfigurable Virtual Array Based on a 60-GHz FMCW MIMO Radar SensorIEEE Transactions on Microwave Theory and Techniques, Vol. 70, No. 7 Cross Ref Estimation of drift and diffusion functions from unevenly sampled time-series data27 July 2022 | Physical Review E, Vol. 106, No. 1 Cross Ref Detecting space–time patterns of disease risk under dynamic background population20 April 2022 | Journal of Geographical Systems, Vol. 24, No. 3 Cross Ref Wind power prediction based on PSO-KalmanEnergy Reports, Vol. 8 Cross Ref Visual cluster separation using high-dimensional sharpened dimensionality reduction21 April 2022 | Information Visualization, Vol. 21, No. 3 Cross Ref Interval Wind-Speed Forecasting Model Based on Quantile Regression Bidirectional Minimal Gated Memory Network and Kernel Density Estimation17 June 2022 | Arabian Journal for Science and Engineering, Vol. 4 Cross Ref Spatio-temporal process monitoring using exponentially weighted spatial LASSO2 June 2022 | Journal of Quality Technology, Vol. 58 Cross Ref Bridge deformation prediction based on SHM data using improved VMD and conditional KDEEngineering Structures, Vol. 261 Cross Ref Nonparametric extrapolation of extreme quantiles: a comparison study7 October 2021 | Stochastic Environmental Research and Risk Assessment, Vol. 36, No. 6 Cross Ref Schedule Performance as a Baseline for the Experimental Analysis of Coordinated Behavior: Same or Different Units of Analysis?24 February 2022 | The Psychological Record, Vol. 72, No. 2 Cross Ref Rainfall intensity and catchment size control storm runoff in a gullied blanket peatlandJournal of Hydrology, Vol. 609 Cross Ref Creating a Healthy Environment for Children: GIS Tools for Improving the Quality of the Social Welfare Management System10 June 2022 | International Journal of Environmental Research and Public Health, Vol. 19, No. 12 Cross Ref Conditional catheter‐related thrombosis free probability and risk‐adapted choices of catheter for lung cancer13 May 2022 | Thoracic Cancer, Vol. 13, No. 12 Cross Ref Search for an anomalous excess of charged-current quasielastic νe interactions with the MicroBooNE experiment using Deep-Learning-based reconstruction13 June 2022 | Physical Review D, Vol. 105, No. 11 Cross Ref Spatio-temporal wind speed prediction based on Clayton Copula function with deep learning fusionRenewable Energy, Vol. 192 Cross Ref Dynamic disease screening by joint modelling of survival and longitudinal data27 May 2022 | Journal of the Royal Statistical Society: Series C (Applied Statistics), Vol. 23 Cross Ref Uncertainties in the Assessment of COVID-19 Risk: A Study of People’s Exposure to High-Risk Environments Using Individual-Level Activity Data20 September 2021 | Annals of the American Association of Geographers, Vol. 112, No. 4 Cross Ref Incremental software product line verification - A performance analysis with dead variable code17 March 2022 | Empirical Software Engineering, Vol. 27, No. 3 Cross Ref Theory of evolutionary spectra for heteroskedasticity and autocorrelation robust inference in possibly misspecified and nonstationary modelsJournal of Econometrics, Vol. 117 Cross Ref An Unconventional Technique for Choosing the Kernel Function Blur Coefficients in Nonparametric Regression18 August 2022 | Measurement Techniques, Vol. 65, No. 2 Cross Ref Specificities of ERD lateralization during motion execution Cross Ref Kernel density estimation for circular data: a Fourier series-based plug-in approach for bandwidth selection21 April 2022 | Journal of Nonparametric Statistics, Vol. 34, No. 2 Cross Ref Concurrent Effects between Geomagnetic Storms and Magnetospheric Substorms6 April 2022 | Universe, Vol. 8, No. 4 Cross Ref Deep non-crossing probabilistic wind speed forecasting with multi-scale featuresEnergy Conversion and Management, Vol. 257 Cross Ref Efficient and robust propensity‐score‐based methods for population inference using epidemiologic cohorts6 September 2021 | International Statistical Review, Vol. 90, No. 1 Cross Ref Estimation of a Nonlinear Functional of the Probability Density of a Three-Dimensional Random Variable to Improve the Computational Efficiency of Nonparametric Decision Rules28 August 2022 | Optoelectronics, Instrumentation and Data Processing, Vol. 58, No. 2 Cross Ref The relation between belief in a just world and early processing of deserved and undeserved outcomes: An ERP study13 February 2022 | Social Neuroscience, Vol. 17, No. 2 Cross Ref Mutual information scaling for tensor network machine learning20 January 2022 | Machine Learning: Science and Technology, Vol. 3, No. 1 Cross Ref Automatically extracting surfaces of reinforced concrete bridges from terrestrial laser scanning point cloudsAutomation in Construction, Vol. 135 Cross Ref OnlineSTLProceedings of the VLDB Endowment, Vol. 15, No. 7 Cross Ref On the application of generative adversarial networks for nonlinear modal analysisMechanical Systems and Signal Processing, Vol. 166 Cross Ref Interval Prediction Method for Solar Radiation Based on Kernel Density Estimation and Machine LearningComplexity, Vol. 2022 Cross Ref A three-step local smoothing approach for estimating the mean and covariance functions of spatio-temporal Data20 March 2021 | Annals of the Institute of Statistical Mathematics, Vol. 74, No. 1 Cross Ref Phase I monitoring of serially correlated nonparametric profiles by mixed‐effects modeling28 July 2021 | Quality and Reliability Engineering International, Vol. 38, No. 1 Cross Ref Data-driven prosumer-centric energy scheduling using convolutional neural networksApplied Energy, Vol. 308 Cross Ref The (un)predictable magnetosphere: the role of the internal dynamics3 March 2022 | Journal of Plasma Physics, Vol. 88, No. 1 Cross Ref Quick Selecting Kernel Blur Coefficients to Estimate Probability Density for Independent Random Variables8 July 2022 | Optoelectronics, Instrumentation and Data Processing, Vol. 58, No. 1 Cross Ref Robust analogs to the coefficient of variation20 August 2020 | Journal of Applied Statistics, Vol. 49, No. 2 Cross Ref Nonparametric Mass Imputation for Data Integration17 November 2020 | Journal of Survey Statistics and Methodology, Vol. 10, No. 1 Cross Ref Regional wind power probabilistic forecasting based on an improved kernel density estimation, regular vine copulas, and ensemble learningEnergy, Vol. 238 Cross Ref Probabilistic Revenue Analysis of Microgrid Considering Source-Load and Forecast UncertaintiesIEEE Access, Vol. 10 Cross Ref Data-Enhancement Strategies in Weather-Related Health Studies14 January 2022 | International Journal of Environmental Research and Public Health, Vol. 19, No. 2 Cross Ref Model-based techniques for traffic congestion detection Cross Ref Object-based cluster validation with densitiesPattern Recognition, Vol. 121 Cross Ref Towards Robust Waveform-Based Acoustic ModelsIEEE/ACM Transactions on Audio, Speech, and Language Processing, Vol. 30 Cross Ref Nonparametric Survival Analysis20 July 2022 Cross Ref A Combined Approach for Monitoring Monthly Surface Water/Ice Dynamics of Lesser Slave Lake Via Earth Observation DataIEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, Vol. 15 Cross Ref Structural characterisation of nanoalloys for (photo)catalytic applications with the Sapphire library1 January 2022 | Faraday Discussions, Vol. 8 Cross Ref Joint modeling of multivariate nonparametric longitudinal data and survival data: A local smoothing approach25 September 2021 | Statistics in Medicine, Vol. 40, No. 29 Cross Ref Shock Reduction Technique on Thin Plate Structure by Wave Refraction Using an Elastic PatchShock and Vibration, Vol. 2021 Cross Ref Persistent meanders and eddies lead to quasi-steady Lagrangian transport patterns in a weak western boundary current12 January 2021 | Scientific Reports, Vol. 11, No. 1 Cross Ref Caenorhabditis elegans exhibits positive gravitaxis14 September 2021 | BMC Biology, Vol. 19, No. 1 Cross Ref Niche partitioning among social clusters of a resident estuarine apex predator15 November 2021 | Behavioral Ecology and Sociobiology, Vol. 75, No. 12 Cross Ref Study of the Method for Verification of the Hypothesis on Independence of Two-Dimensional Random Quantities Using a Nonparametric Classifier13 May 2022 | Optoelectronics, Instrumentation and Data Processing, Vol. 57, No. 6 Cross Ref Epanechnikov kernel for PDF estimation applied to equalization and blind source separationSignal Processing, Vol. 189 Cross Ref Age-coherent extensions of the Lee–Carter model29 April 2021 | Scandinavian Actuarial Journal, Vol. 2021, No. 10 Cross Ref compaso : A new halo finder for competitive assignment to spherical overdensities19 October 2021 | Monthly Notices of the Royal Astronomical Society, Vol. 509, No. 1 Cross Ref Nonparametric Multivariate Density Estimation: Case Study of Cauchy Mixture Model26 October 2021 | Mathematics, Vol. 9, No. 21 Cross Ref A method of sequentially generating a set of components of a multidimensional random variable using a nonparametric pattern recognition algorithm1 November 2021 | Computer Optics, Vol. 45, No. 6 Cross Ref Year-round spatial distribution and migration phenology of a rapidly declining trans-Saharan migrant—evidence of winter movements and breeding site fidelity in European turtle doves13 October 2021 | Behavioral Ecology and Sociobiology, Vol. 75, No. 11 Cross Ref Efficiency of European universities: A comparison of peersResearch Policy, Vol. 50, No. 9 Cross Ref Heads Or Tails: A Framework To Model Supply Chain Heterogeneous Messages Cross Ref Research of Wind Power Generation Characteristics in Southern China Based on Improved Non-Parametric Kernel Density Estimation Cross Ref Fast multivariate empirical cumulative distribution function with connection to kernel density estimationComputational Statistics & Data Analysis, Vol. 162 Cross Ref Some confidence intervals and insights for the proportion below the relative poverty line13 October 2021 | SN Business & Economics, Vol. 1, No. 10 Cross Ref The HARPS search for southern extra-solar planets18 October 2021 | Astronomy & Astrophysics, Vol. 654 Cross Ref The sectorally heterogeneous and time-varying price elasticities of energy demand in ChinaEnergy Economics, Vol. 102 Cross Ref Outlier accommodation with semiparametric density processes: A study of Antarctic snow density modelling29 September 2021 | Statistical Modelling, Vol. 124 Cross Ref SAFE-STOP System: Tactical Intention Awareness Based Emergency Collision Avoidance for Malicious Cut-in of Surrounding Vehicle Cross Ref Violin graphs to supervise the energy performance of PV arrays Cross Ref Application of a novel structure-adaptative grey model with adjustable time power item for nuclear energy consumption forecastingApplied Energy, Vol. 298 Cross Ref The continuous wavelet derived by smoothing function and its application in cosmology5 August 2021 | Communications in Theoretical Physics, Vol. 73, No. 9 Cross Ref Inertial Sensor Algorithms to Characterize Turning in Neurological Patients With Turn HesitationsIEEE Transactions on Biomedical Engineering, Vol. 68, No. 9 Cross Ref Consistent inference for predictive regressions in persistent economic systemsJournal of Econometrics, Vol. 224, No. 1 Cross Ref Identifying convergence in nitrogen oxides emissions from motor vehicles in China: A spatial panel data approachJournal of Cleaner Production, Vol. 316 Cross Ref Estimating parameters of a stochastic cell invasion model with fluorescent cell cycle labelling using approximate Bayesian computation22 September 2021 | Journal of The Royal Society Interface, Vol. 18, No. 182 Cross Ref Nonparametric pattern recognition algorithm for testing a hypothesis of the independence of random variables1 September 2021 | Computer Optics, Vol. 5, No. 45 Cross Ref Noise and error analysis and optimization in particle-based kinetic plasma simulationsJournal of Computational Physics, Vol. 440 Cross Ref Unsupervised Learning Methods for Molecular Simulation Data4 May 2021 | Chemical Reviews, Vol. 121, No. 16 Cross Ref A robust dynamic screening system by estimation of the longitudinal data distribution26 May 2020 | Journal of Quality Technology, Vol. 53, No. 4 Cross Ref Ellipsoidal one-class constraint acquisition for quadratically constrained programmingEuropean Journal of Operational Research, Vol. 293, No. 1 Cross Ref Uncertainty quantification for Multiphase-CFD simulations of bubbly flows: a machine learning-based Bayesian approach supported by high-resolution experimentsReliability Engineering & System Safety, Vol. 212 Cross Ref Effective disease surveillance by using covariate information30 July 2021 | Statistics in Medicine, Vol. 59 Cross Ref Quasi‐maximum likelihood and the kernel block bootstrap for nonlinear dynamic models6 January 2021 | Journal of Time Series Analysis, Vol. 42, No. 4 Cross Ref between and to the of level and in the of in Research, Vol. 68, No. 2 Cross Ref energy demand spatio-temporal data Vol. Cross Ref Learning for Statistical of June 2021 | Vol. 29 Cross Ref joint distribution analysis of the under semiparametric distribution for the in of Engineering and Science, Vol. No. 2 Cross Ref model of Econometrics, Vol. No. 2 Cross Ref of Based on and Network with the Quantile May 2021 | Applied Vol. 11, No. 11 Cross Ref An to multivariate probabilistic and Vol. 4 Cross Ref based on deep learning model with attention for hybrid system under Energy, Vol. 170 Cross Ref and March 2022 | Mathematics, Vol. No. 1 Cross Ref Uncertainty Analysis of Wind Power Based on Data Cross Ref kernel density estimation Systems with Applications, Vol. Cross Ref network modelling of the of a with the Physics Vol. Cross Ref March 2021 | Journal of Computational and Vol. No. 2 Cross Ref modeling of for July 2020 | Reviews, Vol. 40, No. 3 Cross Ref for Cross Ref on the size of February 2021 | of the of Vol. No. 9 Cross Ref Uncertainty analysis of wind power probability density forecasting based on and quantile Vol. Cross Ref of Methods in Data of March 2021 | International Journal of Vol. 10, No. 3 Cross Ref Dynamic spatial analysis of China: and spatial convergence Research, Vol. No. 3 Cross Ref the Hypothesis of the Independence of Two-Dimensional Random Using a Nonparametric for August 2021 | Optoelectronics, Instrumentation and Data Processing, Vol. 57, No. 2 Cross Ref A Study of Nonparametric Kernel with of Vol. No. 1 Cross Ref Fast Modelling, Vol. Cross Ref for of Coefficients for Kernel of Probability March 2021 | Measurement Techniques, Vol. No. 11 Cross Ref of laser fusion Vol. Cross Ref A of density based clustering September 2020 | Frontiers of Computer Science, Vol. 15, No. 1 Cross Ref of Information and Regression for of Acoustic Data during on Structural and Construction, Vol. No. 1 Cross Ref of Heterogeneous in January 2021 | Research Vol. No. 2 Cross Ref for Time Series of Cross Ref Data-driven Kernel-based Probabilistic for Time Series Reduction Cross Ref The for Robust Energy Estimation Cross Ref from August 2020 Cross Ref during Chain for Detecting Using from 2020 | Vol. No. 1 Cross Ref Method for of in Using and 2020 | Remote Sensing, Vol. 13, No. 1 Cross Ref of a Framework for Environmental of 2020 | Journal of Science and Engineering, Vol. 9, No. 1 Cross Ref a for kernel density estimation, and of Vol. No. Cross Ref Analysis of the of the mean of the kernel probability density estimation in the of and random variables1 January 2021 | No. 3 Cross Ref Energy Management of April 2021 Cross Ref for Detecting of July 2021 Cross Ref of a Analysis Method to the of COVID-19 Kernel Density Estimation Using July 2021 Cross Ref On generative as the for August 2021 | Engineering, Vol. 2 Cross Ref and in the January 2021 | Journal of Vol. 27, No. 1 Cross Ref The in January 2021 | Vol. No. 5 Cross Ref Kernel Based for MIMO Radar in Access, Vol. 9 Cross Ref The diffusion of diffusion and Computational Analysis, Vol. Cross Ref of of components of a multidimensional random variable based on a nonparametric pattern recognition algorithm1 January 2021 | No. 9 Cross Ref in energy efficiency of and its for performance Journal of and Engineering, Vol. Cross Ref of Nonlinear Systems with Cross Ref Probability density forecasting of wind power based on quantile neural Systems, Vol. Cross Ref Hypothesis testing based on a of of Econometrics, Vol. No. 2 Cross Ref Analysis of distribution human in the of Lake in Vol. Cross Ref in and May 2020 | Scientific Reports, Vol. 10, No. 1 Cross Ref Assessment of regressions for in the insights from and historical June 2020 | Vol. 17, No. 12 Cross Ref hybrid and for September 2020 | Journal on Communications and Vol. No. 1 Cross Ref of March 2021 | Physics of Vol. No. 12 Cross Ref price and July 2020 | European Review of Agricultural Economics, Vol. No. 5 Cross Ref A of | Applied Vol. 27, No. Cross Ref Risk Estimation to and in November 2020 | Frontiers in Earth Science, Vol. 8 Cross Ref Learning to a A Transactions on Systems Technology, Vol. No. 6 Cross Ref for cell lung Analysis based on and Radiation Vol. Cross Ref Fast for the of the Kernel Density April 2021 | Optoelectronics, Instrumentation and Data Processing, Vol. No. 6 Cross Ref Using Deep Learning: With Systems and November 2020 | Journal of in Earth Systems, Vol. No. 11 Cross Ref search in the A Research Vol. Cross Ref A new algorithm based on and algorithm to the optimization Computing, Vol. Cross Ref of data methods and clustering model in the of of of Vol. No. 3 Cross Ref Energy Efficient in of Cross Ref Statistical of and for July 2020 | Journal International, Vol. No. 1 Cross Ref Unsupervised of Deep Bayesian Cross Ref for Visual of Data Cross Ref nonlinear of covariance Annals of Statistics, Vol. No. 5 Cross Ref modeling and prediction approach for using deep Journal of and Mass Vol. Cross Ref in model for May 2020 | Structural and Vol. No. 4 Cross Ref A Kernel Outlier August | Review, Vol. No. 5 Cross Ref emissions in China: spatial patterns and Research, Vol. 11, No. 9 Cross Ref A Analysis of the Effects of on the Performance of Transactions on Applications, Vol. No. 5 Cross Ref during A for the of based on September 2020 | Journal of and Management, Vol. No. 3 Cross Ref Dynamic risk assessment with network and clustering Engineering & System Safety, Vol. Cross Ref with exchange and Some from Modelling, Vol. Cross Ref A probabilistic verification application of random", "authors": ["V. A. Epanechnikov"], "year": 1969, "venue": "Theory of Probability and Its Applications", "cited_by_count": 1799, "type": "article", "concepts": ["Multivariate statistics", "Mathematics", "Multivariate kernel density estimation", "Estimation", "Density estimation"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4409383105", "doi": "https://doi.org/10.1016/j.isprsjprs.2025.03.028", "title": "RSGPT: A remote sensing vision language model and benchmark", "abstract": "", "authors": ["Yuan Hu", "Jianlong Yuan", "Congcong Wen", "Xiao‐Nan Lu", "Yu Liu", "Li Xiang"], "year": 2025, "venue": "ISPRS Journal of Photogrammetry and Remote Sensing", "cited_by_count": 64, "type": "article", "concepts": ["Benchmark (surveying)", "Computer science", "Remote sensing", "Artificial intelligence", "Computer vision"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W2952481429", "doi": "https://doi.org/10.1038/s41598-017-17204-5", "title": "QuPath: Open source software for digital pathology image analysis", "abstract": "", "authors": ["Peter Bankhead", "Maurice B. Loughrey", "José A. Fernández", "Yvonne Dombrowski", "Darragh G. McArt", "Philip D. Dunne", "Stephen McQuaid", "Ronan T. Gray", "Liam Murray", "Helen G. Coleman", "Jacqueline A. James", "Manuel Salto‐Tellez", "Peter W. Hamilton"], "year": 2017, "venue": "Scientific Reports", "cited_by_count": 7975, "type": "article", "concepts": ["Computer science", "Scripting language", "Software", "Digital pathology", "Extensibility"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W1480729244", "doi": "https://doi.org/10.1159/000369778", "title": "Aspirin plus Clopidogrel as Secondary Prevention after Stroke or Transient Ischemic Attack: A Systematic Review and Meta-Analysis", "abstract": "Compared with monotherapy, short-term aspirin in combination with clopidogrel is more effective as secondary prevention of stroke or TIA without increasing the risk of hemorrhagic stroke and major bleeding events. Long-term combination therapy does not reduce the risk of stroke recurrence, and is associated with increased major bleeding events. The clinical applicability of the findings of this systematic review, however, needs to be confirmed in future clinical trials.", "authors": ["Qinghua Zhang", "Chao Wang", "Maoyong Zheng", "Yanxia Li", "Jincun Li", "Liping Zhang", "Xiao Shang", "Chuanzhu Yan"], "year": 2014, "venue": "Cerebrovascular Diseases", "cited_by_count": 11545, "type": "review", "concepts": ["Medicine", "Clopidogrel", "Aspirin", "Internal medicine", "Stroke (engine)"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W3107527779", "doi": "https://doi.org/10.1093/nar/gkaa1074", "title": "The STRING database in 2021: customizable protein–protein networks, and functional characterization of user-uploaded gene/measurement sets", "abstract": "Cellular life depends on a complex web of functional associations between biomolecules. Among these associations, protein-protein interactions are particularly important due to their versatility, specificity and adaptability. The STRING database aims to integrate all known and predicted associations between proteins, including both physical interactions as well as functional associations. To achieve this, STRING collects and scores evidence from a number of sources: (i) automated text mining of the scientific literature, (ii) databases of interaction experiments and annotated complexes/pathways, (iii) computational interaction predictions from co-expression and from conserved genomic context and (iv) systematic transfers of interaction evidence from one organism to another. STRING aims for wide coverage; the upcoming version 11.5 of the resource will contain more than 14 000 organisms. In this update paper, we describe changes to the text-mining system, a new scoring-mode for physical interactions, as well as extensive user interface features for customizing, extending and sharing protein networks. In addition, we describe how to query STRING with genome-wide, experimental data, including the automated detection of enriched functionalities and potential biases in the user's query data. The STRING resource is available online, at https://string-db.org/.", "authors": ["Damian Szklarczyk", "Annika L. Gable", "Katerina Nastou", "David Lyon", "Rebecca Kirsch", "Sampo Pyysalo", "Nadezhda T. Doncheva", "Marc Legeay", "Tao Fang", "Peer Bork", "Lars Juhl Jensen", "Christian von Mering"], "year": 2020, "venue": "Nucleic Acids Research", "cited_by_count": 8221, "type": "article", "concepts": ["Biology", "Upload", "Gene", "String (physics)", "Computational biology"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4400070365", "doi": "https://doi.org/10.1109/lsp.2024.3420083", "title": "Vision-Language Consistency Guided Multi-Modal Prompt Learning for Blind AI Generated Image Quality Assessment", "abstract": "Recently, textual prompt tuning has shown inspirational performance in adapting Contrastive Language-Image Pre-training (CLIP) models to natural image quality assessment. However, such uni-modal prompt learning method only tunes the language branch of CLIP models. This is not enough for adapting CLIP models to AI generated image quality assessment (AGIQA) since AGIs visually differ from natural images. In addition, the consistency between AGIs and user input text prompts, which correlates with the perceptual quality of AGIs, is not investigated to guide AGIQA. In this letter, we propose vision-language consistency guided multi-modal prompt learning for blind AGIQA, dubbed CLIP-AGIQA. Specifically, we introduce learnable textual and visual prompts in language and vision branches of CLIP models, respectively. Moreover, we design a text-to-image alignment quality prediction task, whose learned vision-language consistency knowledge is used to guide the optimization of the above multi-modal prompts. Experimental results on two public AGIQA datasets demonstrate that the proposed method outperforms state-of-the-art quality assessment models.", "authors": ["Jun Fu", "Wei Zhou", "Qiuping Jiang", "Hantao Liu", "Guangtao Zhai"], "year": 2024, "venue": "IEEE Signal Processing Letters", "cited_by_count": 24, "type": "article", "concepts": ["Computer science", "Consistency (knowledge bases)", "Artificial intelligence", "Image quality", "Quality (philosophy)"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W2089739736", "doi": "https://doi.org/10.1097/00006324-200111000-00006", "title": "Customized Corneal Ablation: the Quest for Supervision", "abstract": "Customized Corneal Ablation: the Quest for Supervision. Scott M. MacRae, Ronald R. Krueger, Raymond A. Applegate. Thorofare, NJ: Slack Inc, 2001. Pages: 416. Price: $205.00. ISBN 1-55642-488-4. Mohan Merchea , College of Optometry , The Ohio State University , Columbus, Ohio Occasionally clinicians and scientists are fortunate enough to experience the evolution of innovative concepts in vision science. The introduction of wavefront aberration measurement in the human visual system was such an event. It was not a novel concept when Liang and colleagues designed a rapid and repeatable method of measuring higher order aberrations in the human eye; rather it was a concept that evolved in conjunction with technological advances in other fields, including adaptive optics, refractive surgery, and contact lens manufacturing. The excitement generated by this developing topic was apparent at the 2000 Academy Research Symposium and by the numerous abstracts and papers presented at the AAO and ARVO annual meetings. This is the first textbook devoted to the emerging science of aberration detection, image quality, and customized refractive therapy. The title of this textbook implies a focus solely on the surgical management of refractive error by corneal photoablation; rather, the contents of this book provide a great breadth of information on ocular image degradation by optical limits such as aberrations and diffraction, neural limits to visual function, and retinal photoreceptor imaging. In chapter 2 of the introductory subsection, the reader is provided with a detailed review of several seminal research papers on the optical and neural limits of human vision and the benefits realized by monochromatic and chromatic aberration correction. Additional factors that affect aberration measurements such as accommodation, decentration, and temporal effects are also addressed here, but the role of tear film stability on wavefront measurement is not mentioned. Chapter 3 presents the reader with a brief historical review of the development of objective and subjective instruments for wavefront detection, including interferometric and the ray tracing methods on which most modern instruments are based. Chapter 4 describes the fundamentals of an adaptive optics ophthalmoscope for photoreceptor imaging. Although the chapter seems unrelated in a text on custom correction of refractive error, the reader is fortunately provided with insight into another application of wavefront sensing in the ophthalmic field. This section concludes with a description of how whole-eye wavefront aberrations or corneal topography aberrations are used to guide corneal ablation and the brief review of the physics of lasers. Section 2, Wavefront Guided Custom Ablation, is divided into basic and clinical science sections. The basic science section primarily contains several chapters that I felt would be more appropriately placed in the introductory section. The chapters covering the assessment of optical quality, visual performance assessment, corneal biomechanics, and technology requirements are relevant to any customized refractive treatment wavefront or topography guided ablation. Chapters 6, 7, and 8 provide the language of traditional methods of optical image quality: PSF, LSF, MTF, PTF, and OTF with which the reader needs to be familiar to interpret wavefront aberration maps in relation to traditional image quality assessments. Zernike polynomial wavefront quantification is also introduced in this section, and the importance of a standardized method of data presentation is highlighted. Chapter 9 provides an extensive review of changes in corneal shape and power that occur with any refractive surgical procedure, ablative or incisional. Roberts and Dupps propose a biomechanical model for changes in the corneal structure in chapter 9. They emphasize that current customized ablation techniques being developed do not address changes in the cornea that occur during surgical procedures. Chapters 10 and 11 cover the technological requirements for custom treatment, including laser attributes, topography and wavefront resolution, and eye tracking. The basic science subsection of part 2 is appropriate reading for anyone interested in whole-eye or corneal aberration changes from refractive surgery. The clinical science subsection is a thorough review of the clinical instruments being developed to measure ocular wavefront aberration. Several techniques, including Hartmann-Shack, Tscherning, Tracey ray tracing, Spatially Resolved Refractometer, and a Scanning Slit Refractometer are described in detail in chapters 12 through 17. Section 3, Corneal Topography Guided Ablations, is similarly divided into basic and clinical subsections. How topography instruments are used to estimate corneal curvature and height is reviewed, and how this data is used to estimate the wavefront error of the cornea is presented. Also, the advantages over wavefront-guided procedures are provided. Chapter 20 describes the limitations of corneal wavefront error for custom ablation based on factors, including arbitrary reference surface calculation, a lack of biomechanical modeling, and ignoring aberrations induced from other ocular components. The clinical science subsection provides a summary of the current instruments available using software for topography-guided custom ablation, including Topolink, Corneal Interactive Programmed Topographic Ablation (CIPTA), Topographic Simulated Customized Ablation (TOSCA), and Contoured Ablation Patterns (CAP). Section 4, Surgeon Guided Customized Ablation, presents management strategies for decentered ablations, including masking techniques and eccentric ablations and for steep central islands. Chapter 27 describes how ablation zone diameter can influence symptoms such as glare. This section concludes with a description of cross-cylinder and bitoric ablation profiles. Section 5 addresses the future of customization. The final section in this insightful text provides a positive outlook for the development of alternative techniques for custom refractive correction, including contact lenses and intraocular lenses. The authors were thoughtful enough to include the Standards for Reporting the Optical Aberrations of Eyes from the Optical Society of America, which includes recommendations for reference axis selection, Zernike polynomial indexing standards, and a description of a standard aberrator for calibration of wavefront-sensing instruments. Also included in the appendices are the American National Standards Institute (ANSI) Standards in Corneal Topography. The index covers most topics and will allow readers to easily locate the desired information. Labeling errors of some figures through the text are a very minor complaint. Overall, the figures and tables were of high quality and easily understood. In summary, this textbook will be a valuable reference to clinicians and vision scientists who are interested in familiarizing themselves with optical image quality measurement techniques and the current instruments available for customized refractive surgery. FIGUREFigure", "authors": ["Mohan Merchea"], "year": 2001, "venue": "Optometry and Vision Science", "cited_by_count": 87, "type": "article", "concepts": ["Ablation", "Optometry", "Ophthalmology", "Computer science", "Medicine"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4404536559", "doi": "https://doi.org/10.1007/978-3-031-72904-1_9", "title": "A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment", "abstract": "", "authors": ["Tianhe Wu", "Kede Ma", "Jie Liang", "Yujiu Yang", "Lei Zhang"], "year": 2024, "venue": "Lecture notes in computer science", "cited_by_count": 24, "type": "book-chapter", "concepts": ["Computer science", "Quality (philosophy)", "Natural language processing", "Image quality", "Artificial intelligence"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W2116634113", "doi": "https://doi.org/10.1093/bioinformatics/btg405", "title": "affy—analysis of <i>Affymetrix GeneChip</i> data at the probe level", "abstract": "The affy package is an R package of functions and classes for the analysis of oligonucleotide arrays manufactured by Affymetrix. The package is currently in its second release, affy provides the user with extreme flexibility when carrying out an analysis and make it possible to access and manipulate probe intensity data. In this paper, we present the main classes and functions in the package and demonstrate how they can be used to process probe-level data. We also demonstrate the importance of probe-level analysis when using the Affymetrix GeneChip platform.", "authors": ["Laurent Gautier", "Leslie Cope", "Benjamin M. Bolstad", "Rafael A. Irizarry"], "year": 2004, "venue": "Bioinformatics", "cited_by_count": 5349, "type": "article", "concepts": ["Computer science", "Gene chip analysis", "R package", "Flexibility (engineering)", "Data mining"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4402448026", "doi": "https://doi.org/10.3390/hydrology11090148", "title": "The Implementation of Multimodal Large Language Models for Hydrological Applications: A Comparative Study of GPT-4 Vision, Gemini, LLaVa, and Multimodal-GPT", "abstract": "Large Language Models (LLMs) combined with visual foundation models have demonstrated significant advancements, achieving intelligence levels comparable to human capabilities. This study analyzes the latest Multimodal LLMs (MLLMs), including Multimodal-GPT, GPT-4 Vision, Gemini, and LLaVa, with a focus on hydrological applications such as flood management, water level monitoring, agricultural water discharge, and water pollution management. We evaluated these MLLMs on hydrology-specific tasks, testing their response generation and real-time suitability in complex real-world scenarios. Prompts were designed to enhance the models’ visual inference capabilities and contextual comprehension from images. Our findings reveal that GPT-4 Vision demonstrated exceptional proficiency in interpreting visual data, providing accurate assessments of flood severity and water quality. Additionally, MLLMs showed potential in various hydrological applications, including drought prediction, streamflow forecasting, groundwater management, and wetland conservation. These models can optimize water resource management by predicting rainfall, evaporation rates, and soil moisture levels, thereby promoting sustainable agricultural practices. This research provides valuable insights into the potential applications of advanced AI models in addressing complex hydrological challenges and improving real-time decision-making in water resource management", "authors": ["Likith Kadiyala", "Omer Mermer", "R. Dinesh Jackson Samuel", "Yusuf Sermet", "İbrahim Demir"], "year": 2024, "venue": "Hydrology", "cited_by_count": 35, "type": "article", "concepts": ["Computer science", "Multimodal therapy", "Environmental science", "Remote sensing", "Geology"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4413146669", "doi": "https://doi.org/10.1109/cvpr52734.2025.02245", "title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis", "abstract": "In the quest for artificial general intelligence, Multi-modal Large Language Models (MLLMs) have emerged as a focal point in recent advancements. However, the predominant focus remains on developing their capabilities in static image understanding. The potential of MLLMs to process sequential visual data is still insufficiently explored, highlighting the lack of a comprehensive, high-quality assessment of their performance. In this paper, we introduce Video-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of MLLMs in Video analysis. Our work distinguishes from existing benchmarks through four key features: 1) Diversity in video types, spanning 6 primary visual domains with 30 subfields to ensure broad scenario generalizability; 2) Duration in temporal dimension, encompassing both short-, medium-, and long-term videos, ranging from 11 seconds to 1 hour, for robust contextual dynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides video frames, including subtitles and audios, to unveil the all-round capabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual labeling by expert annotators to facilitate precise and reliable model assessment. With Video-MME, we extensively evaluate various state-of-the-art MLLMs, and reveal that Gemini 1.5 Pro is the best-performing commercial model, significantly outperforming the open-source models with an average accuracy of 75%, compared to 71.9% for GPT-4o. The results also demonstrate that Video-MME is a universal benchmark that applies to both image and video MLLMs. Further analysis indicates that subtitle and audio information could significantly enhance video understanding. Besides, a decline in MLLM performance is observed as video duration increases for all models. Our dataset along with these findings underscores the need for further improvements in handling longer sequences and multi-modal data, shedding light on future MLLM development. Project page: https://video-mme.github.io.", "authors": ["Chaoyou Fu", "Yuhan Dai", "Yongdong Luo", "Lei Li", "Shuhuai Ren", "Renrui Zhang", "Zihan Wang", "Chenyu Zhou", "Yunhang Shen", "Mengdan Zhang", "Peixian Chen", "Yanwei Li", "Shaohui Lin", "Sirui Zhao", "Ke Li", "Tong Xu", "Xiawu Zheng", "Enhong Chen", "Caifeng Shan", "Ran He", "Xing Sun"], "year": 2025, "venue": "", "cited_by_count": 38, "type": "article", "concepts": ["Benchmark (surveying)", "Computer science", "Modal", "Artificial intelligence", "Geography"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W2809254203", "doi": "https://doi.org/10.1007/s13244-018-0639-9", "title": "Convolutional neural networks: an overview and application in radiology", "abstract": "", "authors": ["Rikiya Yamashita", "Mizuho Nishio", "Richard Kinh Gian", "Kaori Togashi"], "year": 2018, "venue": "Insights into Imaging", "cited_by_count": 4417, "type": "review", "concepts": ["Convolutional neural network", "Computer science", "Artificial intelligence", "Leverage (statistics)", "Overfitting"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4387076386", "doi": "https://doi.org/10.48550/arxiv.2309.14181", "title": "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision", "abstract": "The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. a) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. b) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions on 499 images, and a GPT-involved comparison pipeline between outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we further measure their visual quality assessment ability to align with human opinion scores. Specifically, we design a softmax-based strategy that enables MLLMs to predict quantifiable quality scores, and evaluate them on various existing image quality assessment (IQA) datasets. Our evaluation across the three abilities confirms that MLLMs possess preliminary low-level visual skills. However, these skills are still unstable and relatively imprecise, indicating the need for specific enhancements on MLLMs towards these abilities. We hope that our benchmark can encourage the research community to delve deeper to discover and enhance these untapped potentials of MLLMs. Project Page: https://q-future.github.io/Q-Bench.", "authors": ["Haoning Wu", "Zicheng Zhang", "Erli Zhang", "Chaofeng Chen", "Liang Liao", "Annan Wang", "Chunyi Li", "Wenxiu Sun", "Qiong Yan", "Guangtao Zhai", "Weisi Lin"], "year": 2023, "venue": "arXiv (Cornell University)", "cited_by_count": 20, "type": "preprint", "concepts": ["Perception", "Computer science", "Benchmark (surveying)", "Construct (python library)", "Correctness"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W2166667242", "doi": "https://doi.org/10.1017/s0140525x01003922", "title": "The magical number 4 in short-term memory: A reconsideration of mental storage capacity", "abstract": "Miller (1956) summarized evidence that people can remember about seven chunks in short-term memory (STM) tasks. However, that number was meant more as a rough estimate and a rhetorical device than as a real capacity limit. Others have since suggested that there is a more precise capacity limit, but that it is only three to five chunks. The present target article brings together a wide variety of data on capacity limits suggesting that the smaller capacity limit is real. Capacity limits will be useful in analyses of information processing only if the boundary conditions for observing them can be carefully described. Four basic conditions in which chunks can be identified and capacity limits can accordingly be observed are: (1) when information overload limits chunks to individual stimulus items, (2) when other steps are taken specifically to block the recording of stimulus items into larger chunks, (3) in performance discontinuities caused by the capacity limit, and (4) in various indirect effects of the capacity limit. Under these conditions, rehearsal and long-term memory cannot be used to combine stimulus items into chunks of an unknown size; nor can storage mechanisms that are not capacity-limited, such as sensory memory, allow the capacity-limited storage mechanism to be refilled during recall. A single, central capacity limit averaging about four chunks is implicated along with other, noncapacity-limited sources. The pure STM capacity limit expressed in chunks is distinguished from compound STM limits obtained when the number of separately held chunks is unclear. Reasons why pure capacity estimates fall within a narrow range are discussed and a capacity limit for the focus of attention is proposed.", "authors": ["Nelson Cowan"], "year": 2001, "venue": "Behavioral and Brain Sciences", "cited_by_count": 6665, "type": "article", "concepts": ["Mental capacity", "Limit (mathematics)", "Stimulus (psychology)", "Computer science", "Short-term memory"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W3120387768", "doi": "https://doi.org/10.1016/s0140-6736(20)32656-8", "title": "RETRACTED: 6-month consequences of COVID-19 in patients discharged from hospital: a cohort study", "abstract": "", "authors": ["Chaolin Huang", "Lixue Huang", "Yeming Wang", "Xia Li", "Lili Ren", "Xiaoying Gu", "Liang Kang", "Li Guo", "Min Liu", "Xing Zhou", "Jianfeng Luo", "Zhenghui Huang", "Shengjin Tu", "Yue Zhao", "Li Chen", "Decui Xu", "Yanping Li", "Caihong Li", "Peng Lü", "Yong Li", "Wuxiang Xie", "Dan Cui", "Lianhan Shang", "Guohui Fan", "Jiuyang Xu", "Geng Wang", "Ying Wang", "Jingchuan Zhong", "Chen Wang", "Jianwei Wang", "Dingyu Zhang", "Bin Cao"], "year": 2021, "venue": "The Lancet", "cited_by_count": 4238, "type": "article", "concepts": ["Medicine", "Cohort", "Pneumonia", "Logistic regression", "Emergency medicine"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W1787224781", "doi": "https://doi.org/10.1371/journal.pone.0130140", "title": "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation", "abstract": "Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.", "authors": ["Sebastian Bach", "Alexander Binder", "Grégoire Montavon", "Frederick Klauschen", "Klaus‐Robert Müller", "Wojciech Samek"], "year": 2015, "venue": "PLoS ONE", "cited_by_count": 4413, "type": "article", "concepts": ["MNIST database", "Computer science", "Artificial intelligence", "Pixel", "Pascal (unit)"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4390193237", "doi": "https://doi.org/10.4274/dir.2023.232496", "title": "Educating the next generation of radiologists: a comparative report of ChatGPT and e-learning resources", "abstract": "Rapid technological advances have transformed medical education, particularly in radiology, which depends on advanced imaging and visual data. Traditional electronic learning (e-learning) platforms have long served as a cornerstone in radiology education, offering rich visual content, interactive sessions, and peer-reviewed materials. They excel in teaching intricate concepts and techniques that necessitate visual aids, such as image interpretation and procedural demonstrations. However, Chat Generative Pre-Trained Transformer (ChatGPT), an artificial intelligence (AI)-powered language model, has made its mark in radiology education. It can generate learning assessments, create lesson plans, act as a round-the-clock virtual tutor, enhance critical thinking, translate materials for broader accessibility, summarize vast amounts of information, and provide real-time feedback for any subject, including radiology. Concerns have arisen regarding ChatGPT's data accuracy, currency, and potential biases, especially in specialized fields such as radiology. However, the quality, accessibility, and currency of e-learning content can also be imperfect. To enhance the educational journey for radiology residents, the integration of ChatGPT with expert-curated e-learning resources is imperative for ensuring accuracy and reliability and addressing ethical concerns. While AI is unlikely to entirely supplant traditional radiology study methods, the synergistic combination of AI with traditional e-learning can create a holistic educational experience.", "authors": ["İsmail Meşe", "Ceylan Altıntaş Taşlıçay", "Beyza Nur Kuzan", "Taha Yusuf Kuzan", "Ali Kemal Sivrioğlu"], "year": 2023, "venue": "Diagnostic and Interventional Radiology", "cited_by_count": 23, "type": "article", "concepts": ["Medicine", "Multimedia", "Flexibility (engineering)", "Chatbot", "Learning styles"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4387969352", "doi": "https://doi.org/10.1145/3581783.3611969", "title": "AesCLIP: Multi-Attribute Contrastive Learning for Image Aesthetics Assessment", "abstract": "Image aesthetics assessment (IAA) aims at predicting the aesthetic quality of images. Recently, large pre-trained vision-language models, like CLIP, have shown impressive performances on various visual tasks. When it comes to IAA, a straightforward way is to finetune the CLIP image encoder using aesthetic images. However, this can only achieve limited success without considering the uniqueness of multimodal data in the aesthetics domain. People usually assess image aesthetics according to fine-grained visual attributes, e.g., color, light and composition. However, how to learn aesthetics-aware attributes from CLIP-based semantic space has not been addressed before. With this motivation, this paper presents a CLIP-based multi-attribute contrastive learning framework for IAA, dubbed AesCLIP. Specifically, AesCLIP consists of two major components, i.e., aesthetic attribute-based comment classification and attribute-aware learning. The former classifies the aesthetic comments into different attribute categories. Then the latter learns an aesthetic attribute-aware representation by contrastive learning, aiming to mitigate the domain shift from the general visual domain to the aesthetics domain. Extensive experiments have been done by using the pre-trained AesCLIP on four popular IAA databases, and the results demonstrate the advantage of AesCLIP over the state-of-the-arts. The source code will be public at https://github.com/OPPOMKLab/AesCLIP.", "authors": ["Xiangfei Sheng", "Leida Li", "Pengfei Chen", "Jinjian Wu", "Weisheng Dong", "Yuzhe Yang", "Liwu Xu", "Yaqian Li", "Guangming Shi"], "year": 2023, "venue": "", "cited_by_count": 26, "type": "article", "concepts": ["Computer science", "Domain (mathematical analysis)", "Artificial intelligence", "Representation (politics)", "Image (mathematics)"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4401726215", "doi": "https://doi.org/10.1109/tpami.2024.3445770", "title": "Q-Bench+: A Benchmark for Multi-Modal Foundation Models on Low-Level Vision From Single Images to Pairs", "abstract": "The rapid development of Multi-modality Large Language Models (MLLMs) has navigated a paradigm shift in computer vision, moving towards versatile foundational models. However, evaluating MLLMs in low-level visual perception and understanding remains a yet-to-explore domain. To this end, we design benchmark settings to emulate human language responses related to low-level vision: the low-level visual perception (A1) via visual question answering related to low-level attributes (e.g. clarity, lighting); and the low-level visual description (A2), on evaluating MLLMs for low-level text descriptions. Furthermore, given that pairwise comparison can better avoid ambiguity of responses and has been adopted by many human experiments, we further extend the low-level perception-related questionanswering and description evaluations of MLLMs from single images to image pairs. Specifically, for perception (A1), we carry out the LLVisionQA+ dataset, comprising 2,990 single images and 1,999 image pairs each accompanied by an open-ended question about its low-level features; for description (A2), we propose the LLDescribe+ dataset, evaluating MLLMs for low-level descriptions on 499 single images and 450 pairs. Additionally, we evaluate MLLMs on assessment (A3) ability, i.e. predicting score, by employing a softmax-based approach to enable all MLLMs to generate quantifiable quality ratings, tested against human opinions in 7 image quality assessment (IQA) datasets. With 24 MLLMs under evaluation, we demonstrate that several MLLMs have decent low-level visual competencies on single images, but only GPT-4V exhibits higher accuracy on pairwise comparisons than single image evaluations (like humans). We hope that our benchmark will motivate further research into uncovering and enhancing these nascent capabilities of MLLMs. Datasets will be available at https://github.com/Q-Future/Q-Bench.", "authors": ["Zicheng Zhang", "Haoning Wu", "Erli Zhang", "Guangtao Zhai", "Weisi Lin"], "year": 2024, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "cited_by_count": 17, "type": "article", "concepts": ["Modal", "Artificial intelligence", "Benchmark (surveying)", "Computer science", "Foundation (evidence)"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4312219151", "doi": "https://doi.org/10.1145/3573891", "title": "Dynamic Convolution-based Encoder-Decoder Framework for Image Captioning in Hindi", "abstract": "In sequence-to-sequence modeling tasks, such as image captioning, machine translation, and visual question answering, encoder-decoder architectures are state of the art. An encoder, convolutional neural network (CNN) encodes input images into fixed dimensional vector representation in the image captioning task, whereas a decoder, a recurrent neural network, performs language modeling and generates the target descriptions. Recent CNNs use the same operation over every pixel; however, all the image pixels are not equally important. To address this, the proposed method uses a dynamic convolution-based encoder for image encoding or feature extraction, Long-Short-Term-Memory as a decoder for language modeling, and X-Linear attention to make the system robust. Encoders, attentions, and decoders are important aspects of the image captioning task; therefore, we experiment with various encoders, decoders, and attention mechanisms. Most of the works for image captioning have been carried out for the English language in the existing literature. We propose a novel approach for caption generation from images in Hindi. Hindi, widely spoken in South Asia and India, is the fourth most-spoken language globally; it is India’s official language. The proposed method utilizes dynamic convolution operation on the encoder side to obtain a better image encoding quality. The Hindi image captioning dataset is manually created by translating the popular MSCOCO dataset from English to Hindi. In terms of BLEU scores, the performance of the proposed method is compared with other baselines, and the results obtained show that the proposed method outperforms different baselines. Manual human assessment in terms of adequacy and fluency of the captions generated further determines the efficacy of the proposed method in generating good-quality captions.", "authors": ["Santosh Kumar Mishra", "Sushant Sinha", "Sriparna Saha", "Pushpak Bhattacharyya"], "year": 2022, "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing", "cited_by_count": 19, "type": "article", "concepts": ["Closed captioning", "Computer science", "Hindi", "Encoder", "Artificial intelligence"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4319662928", "doi": "https://doi.org/10.1371/journal.pdig.0000198", "title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models", "abstract": "We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making.", "authors": ["Tiffany H. Kung", "Morgan Cheatham", "Arielle Medenilla", "Czarina Sillos", "Lorie De Leon", "Camille Elepaño", "Maria Madriaga", "Rimel Aggabao", "Giezel Diaz-Candido", "James Maningo", "Victor Tseng"], "year": 2023, "venue": "PLOS Digital Health", "cited_by_count": 3279, "type": "article", "concepts": ["Concordance", "United States Medical Licensing Examination", "Medical education", "Computer science", "Licensure"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4390987121", "doi": "https://doi.org/10.57197/jdr-2023-0051", "title": "Empowering the Visually Impaired: Translating Handwritten Digits into Spoken Language with HRNN-GOA and Haralick Features", "abstract": "Visual impairment poses significant challenges to individuals in their daily lives, limiting their access to information encoded in the visual domain. This paper presents a novel approach to empower the visually impaired by developing a system capable of translating handwritten digits into spoken language. The proposed system leverages a combination of advanced deep learning (DL) architecture, Hopfield Recurrent Neural Network-Grasshopper Optimization Algorithm (HRNN-GOA), and traditional image-processing techniques such as Haralick features. The system employs HRNN-GOA as the core model for handwritten digit recognition. HRNN-GOA exhibits superior sequential learning capabilities, capturing intricate patterns in the handwritten digits. Additionally, Haralick features are extracted from the input images, providing complementary texture-based information. The fusion of DL and traditional features aims to enhance the robustness and accuracy of the recognition process. The experimental results demonstrate the effectiveness of the proposed approach in accurately recognising handwritten digits. The HRNN-GOA model achieves state-of-the-art performance in digit classification tasks, while the incorporation of Haralick features further refines the recognition process, especially in cases with complex textures or variations in writing styles. The simulation results are compared against state-of-the-art strategies in terms of many metrics, including accuracy, precision, recall, specificity, area under the curve, F1-score, and false-positive rate. The proposed system has the potential to significantly improve the independence and quality of life for individuals with visual impairments by providing seamless access to numerical information in a spoken format. Future endeavours could explore the extension of this framework to recognise and translate more complex handwritten symbols or characters. Additionally, user experience studies and real-world deployment assessments will be crucial for refining the system and ensuring its practical utility in diverse scenarios.", "authors": ["Mohammed Alshehri", "Sunil Kumar Sharma", "Priya Gupta", "Sapna Ratan Shah"], "year": 2024, "venue": "Journal of Disability Research", "cited_by_count": 24, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Robustness (evolution)", "Speech recognition", "Feature extraction"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4405016353", "doi": "https://doi.org/10.1007/978-3-031-78125-4_4", "title": "CLIP-AGIQA: Boosting the Performance of AI-Generated Image Quality Assessment with CLIP", "abstract": "", "authors": ["Zhenchen Tang", "Zichuan Wang", "Bo Peng", "Jing Dong"], "year": 2024, "venue": "Lecture notes in computer science", "cited_by_count": 10, "type": "book-chapter", "concepts": ["Computer science", "Boosting (machine learning)", "Image quality", "Artificial intelligence", "Quality assessment"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W2115609473", "doi": "https://doi.org/10.1002/mpr.168", "title": "The World Mental Health (WMH) Survey Initiative version of the World Health Organization (WHO) Composite International Diagnostic Interview (CIDI)", "abstract": "This paper presents an overview of the World Mental Health (WMH) Survey Initiative version of the World Health Organization (WHO) Composite International Diagnostic Interview (CIDI) and a discussion of the methodological research on which the development of the instrument was based. The WMH-CIDI includes a screening module and 40 sections that focus on diagnoses (22 sections), functioning (four sections), treatment (two sections), risk factors (four sections), socio-demographic correlates (seven sections), and methodological factors (two sections). Innovations compared to earlier versions of the CIDI include expansion of the diagnostic sections, a focus on 12-month as well as lifetime disorders in the same interview, detailed assessment of clinical severity, and inclusion of information on treatment, risk factors, and consequences. A computer-assisted version of the interview is available along with a direct data entry software system that can be used to keypunch responses to the paper-and-pencil version of the interview. Computer programs that generate diagnoses are also available based on both ICD-10 and DSM-IV criteria. Elaborate CD-ROM-based training materials are available to teach interviewers how to administer the interview as well as to teach supervisors how to monitor the quality of data collection.", "authors": ["Ronald C. Kessler", "T. Bedirhan Üstün"], "year": 2004, "venue": "International Journal of Methods in Psychiatric Research", "cited_by_count": 4759, "type": "article", "concepts": ["CIDI", "Mental health", "Medical diagnosis", "Psychology", "Focus group"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W2128030851", "doi": "https://doi.org/10.1093/brain/awr179", "title": "Sensitivity of revised diagnostic criteria for the behavioural variant of frontotemporal dementia", "abstract": "Based on the recent literature and collective experience, an international consortium developed revised guidelines for the diagnosis of behavioural variant frontotemporal dementia. The validation process retrospectively reviewed clinical records and compared the sensitivity of proposed and earlier criteria in a multi-site sample of patients with pathologically verified frontotemporal lobar degeneration. According to the revised criteria, 'possible' behavioural variant frontotemporal dementia requires three of six clinically discriminating features (disinhibition, apathy/inertia, loss of sympathy/empathy, perseverative/compulsive behaviours, hyperorality and dysexecutive neuropsychological profile). 'Probable' behavioural variant frontotemporal dementia adds functional disability and characteristic neuroimaging, while behavioural variant frontotemporal dementia 'with definite frontotemporal lobar degeneration' requires histopathological confirmation or a pathogenic mutation. Sixteen brain banks contributed cases meeting histopathological criteria for frontotemporal lobar degeneration and a clinical diagnosis of behavioural variant frontotemporal dementia, Alzheimer's disease, dementia with Lewy bodies or vascular dementia at presentation. Cases with predominant primary progressive aphasia or extra-pyramidal syndromes were excluded. In these autopsy-confirmed cases, an experienced neurologist or psychiatrist ascertained clinical features necessary for making a diagnosis according to previous and proposed criteria at presentation. Of 137 cases where features were available for both proposed and previously established criteria, 118 (86%) met 'possible' criteria, and 104 (76%) met criteria for 'probable' behavioural variant frontotemporal dementia. In contrast, 72 cases (53%) met previously established criteria for the syndrome (P < 0.001 for comparison with 'possible' and 'probable' criteria). Patients who failed to meet revised criteria were significantly older and most had atypical presentations with marked memory impairment. In conclusion, the revised criteria for behavioural variant frontotemporal dementia improve diagnostic accuracy compared with previously established criteria in a sample with known frontotemporal lobar degeneration. Greater sensitivity of the proposed criteria may reflect the optimized diagnostic features, less restrictive exclusion features and a flexible structure that accommodates different initial clinical presentations. Future studies will be needed to establish the reliability and specificity of these revised diagnostic guidelines.", "authors": ["Katya Rascovsky", "John R. Hodges", "David S. Knopman", "Mario F. Mendez", "Joel H. Kramer", "John Neuhaus", "John C. van Swieten", "Harro Seelaar", "Elise G.P. Dopper", "Chiadi U. Onyike", "Argye E. Hillis", "Keith A. Josephs", "Bradley F. Boeve", "Andrew Kertesz", "William W. Seeley", "Katherine P. Rankin", "Julene K. Johnson", "Maria-Luisa Gorno-Tempini", "Howard J. Rosen", "Caroline E. Prioleau-Latham", "Albert Lee", "Christopher Kipps", "Patricia Lillo", "Olivier Piguet", "Jonathan D. Rohrer", "Martin N. Rossor", "Jason D. Warren", "Nick C. Fox", "Douglas Galasko", "David P. Salmon", "Sandra E. Black", "Marsel Mesulam", "Sandra Weıntraub", "Brad C. Dickerson", "Janine Diehl‐Schmid", "Florence Pasquier", "Vincent Deramecourt", "Florence Lebert", "Yolande A.L. Pijnenburg", "Tiffany W. Chow", "Facundo Manes", "Jordan Grafman", "Stefano F. Cappa", "Morris Freedman", "Murray Grossman", "Bruce L. Miller"], "year": 2011, "venue": "Brain", "cited_by_count": 5104, "type": "article", "concepts": ["Frontotemporal dementia", "Frontotemporal lobar degeneration", "Apathy", "Dementia", "Psychology"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W2050788534", "doi": "https://doi.org/10.1044/jshr.0103.227", "title": "Myoelastic-Aerodynamic Theory of Voice Production", "abstract": "No AccessJournal of Speech and Hearing ResearchResearch Article1 Sep 1958Myoelastic-Aerodynamic Theory of Voice Production Janwillem van den Berg Janwillem van den Berg Google Scholar https://doi.org/10.1044/jshr.0103.227 SectionsAboutPDF ToolsAdd to favoritesDownload CitationTrack Citations ShareFacebookTwitterLinked In Additional Resources FiguresReferencesRelatedDetailsCited by The Journal of the Acoustical Society of America153:5 (2803)1 May 2023 Effect of functional electric stimulation on phonation in an ex vivo aged ovine model Bernhard Jakubaß, Gregor Peters, Stefan Kniesburges, Marion Semmler, Andrijana Kirsch, Claus Gerstenberger, Markus Gugatschka and Michael Döllinger Journal of Voice37:3 (305-313)1 May 2023Integrative Insights into the Myoelastic-Aerodynamic Theory and Acoustics of Phonation. Scientific Tribute to Donald G. MillerJan G. Švec, Harm K. Schutte, C. Julian Chen and Ingo R. Titze Maryam Naghibolhosseini, Ahmed M Yousef, Mohsen Zayernouri, Stephanie RC Zacharias and Dimitar D Deliyski (2023) Deep Learning for High-Speed Laryngeal Imaging Analysis 2023 International Conference on Computational Intelligence and Knowledge Economy (ICCIKE)10.1109/ICCIKE58312.2023.10131757979-8-3503-3826-3 Journal of Voice1 Mar 2023Mechanical Parameters Based on High-Speed Videoendoscopy of the Vocal Folds in Patients With Ectodermal DysplasiaFranziska Pelka, Maria Ensthaler, Olaf Wendler, Stefan Kniesburges, Anne Schützenberger and Marion Semmler Journal of Mammalian Evolution30:1 (79-94)1 Mar 2023The vocal apparatus: An understudied tool to reconstruct the evolutionary history of echolocation in bats?Nicolas L. M. Brualla, Laura A. B. Wilson, Michael Doube, Richard T. Carter, Alan G. McElligott and Daisuke Koyabu Journal of Voice1 Feb 2023Dynamic System Coupling in Voice ProductionChristian T. Herbst, Coen P.H. Elemans, Isao T. Tokuda, Vasileios Chatziioannou and Jan G. Švec Journal of Voice1 Nov 2022Empirical Evaluation of the Role of Vocal Fold Collision on Relative Fundamental Frequency in Voicing OffsetMatti D. Groll, Sean D. Peterson, Matías Zañartu, Jennifer M. Vojtech and Cara E. Stepp Research Studies in Music Education44:3 (491-508)1 Oct 2022Exploring perceptions and experiences of female secondary school singers in Aotearoa New ZealandCalvin P Baker, Te Oti Rakena and Suzanne C Purdy Current Opinion in Behavioral Sciences46 (101140)1 Aug 2022Selection levels on vocal individuality: strategic use or byproductMegan T Wyman, Britta Walkenhorst and Marta B. Manser Chaos, Solitons & Fractals159 (112188)1 Jun 2022Controlling chaotic oscillations in a symmetric two-mass model of the vocal foldsOriol Guasch, Annemie Van Hirtum, A. Inés Fernández and Marc Arnela Transgender Health31 May 2022Gender-Affirming Voice Modification for Transgender Women: Characteristics and OutcomesMichelle Adessa, Zoe Weston, Jeremy Ruthberg and Paul C. Bryson Psychology of Music50:3 (933-944)1 May 2022Augmented visual-feedback of airflow: Immediate effects on voice-source characteristics of students of singingFilipa Martins Batista Lã, Johan Sundberg and Svante Granqvist Journal of Voice36:3 (335-343)1 May 2022Multiparameter Voice Assessment in Dysphonics: Correlation Between Objective and Perceptual ParametersSV Narasimhan and Rajesh Rashmi International Journal of Head and Neck Surgery12:4 (125-130)15 Apr 2022Anatomy and Physiology of PhonationCandace Hrelec and Emily Zhang Brain Sciences12:4 (427)23 Mar 2022Phonetic Effects in the Perception of VOT in a Prevoicing LanguageViktor Kharlamov Journal of Phonetics91 (101138)1 Mar 2022Voicing in Qaqet: Prenasalization and language contactMarija Tabain, Marc Garellek, Birgit Hellwig, Adele Gregory and Richard Beare Tissue Engineering Part B: Reviews28:1 (182-205)1 Feb 2022Bioreactors for Vocal Fold Tissue EngineeringAna M. Gracioso Martins, Andreea Biehl, Daphne Sze and Donald O. Freytes Rostam D. Farhadieh, Ajay R. Sud, Edwin Morrison, and Wayne A.J. Morrison (2022) Pharyngeal Reconstruction Plastic Surgery - Principles and Practice10.1016/B978-0-323-65381-7.00026-5 Markus Gugatschka, David Hortobagyi and Liang Ker (2022) Anatomy and Microanatomy of the Larynx Textbook of Surgery of Larynx and Trachea10.1007/978-3-031-09621-1_3 Aude Lagier and Antoine Giovanni (2022) Physiology of the Larynx Textbook of Surgery of Larynx and Trachea10.1007/978-3-031-09621-1_1 EURASIP Journal on Advances in Signal Processing2021:11 Dec 2021Voice production model based on phonation biophysicsRaissa Bezerra Rocha, Wamberto José Lira de Queiroz and Marcelo Sampaio de Alencar Journal of Voice1 Oct 2021Multiparametric Analysis of Speaking Fundamental Frequency in Genetically Related Speakers Using Different Speech Materials: Some Forensic ImplicationsJulio Cesar Cavalcanti, Anders Eriksson and Plinio A. Barbosa Journal of Voice1 Aug 2021Influence of Reduced Saliva Production on Phonation in Patients With Ectodermal DysplasiaMarion Semmler, Stefan Kniesburges, Franziska Pelka, Maria Ensthaler, Olaf Wendler and Anne Schützenberger Phonetica78:2 (113-140)27 Apr 20211 Apr 2021Voice onset time and constriction duration in Warlpiri stops (Australia)Rikke L. Bundgaard-Nielsen and Carmel O'Shannessy Journal of Speech, Language, and Hearing Research64:4 (1197-1209)14 Apr 2021The Relationship Between Voice Onset Time and Increase in Vocal Effort and Fundamental FrequencyMatti D. Groll, Surbhi Hablani and Cara E. Stepp The Journal of the Acoustical Society of America149:3 (1657-1673)1 Mar 2021 Fluid-structure-acoustic interactions in an ex vivo porcine phonation model Marion Semmler, David A. Berry, Anne Schützenberger and Michael Döllinger Scientific Reports11:18 Jan 2021Subglottal pressure oscillations in anechoic and resonant conditions and their influence on excised larynx phonationsHugo Lehoux, Vít Hampala and Jan G. Švec Alberto Bolletta, Samir Mardini and Hung-Chi Chen (2021) Recipient Vessels: Voice Reconstruction Recipient Vessels in Reconstructive Microsurgery10.1007/978-3-030-75389-4_8 Frontiers in Zoology17:11 Dec 2020Vocal tract anatomy of king penguins: morphological traits of two-voiced sound productionHannah Joy Kriesell, Céline Le Bohec, Alexander F. Cerwenka, Moritz Hertel, Jean-Patrice Robin, Bernhard Ruthensteiner, Manfred Gahr, Thierry Aubin and Daniel Normen Düring Journal of Voice1 Dec 2020Effect of Ventricular Folds on Vocalization Fundamental Frequency in Domestic Pigs (Sus scrofa domesticus)Christian T. Herbst, Takeshi Nishimura, Maxime Garcia, Kishin Migimatsu and Isao T. Tokuda The Journal of the Acoustical Society of America148:1 (EL65-EL71)1 Jul 2020Effects of consonantal constrictions on voice qualityAdam J. Chong, Megan Risdal, Ann Aly, Jesse Zymet and Patricia Keating Proceedings of the National Academy of Sciences117:9 (4718-4723)3 Mar 2020High-fidelity continuum modeling predicts avian voiced sound productionWeili Jiang, Jeppe H. Rasmussen, Qian Xue, Ming Ding, Xudong Zheng and Coen P. H. Elemans Trends in Neurosciences43:2 (115-126)1 Feb 2020A Hierarchy of Autonomous Systems for Vocal ProductionYisi S. Zhang and Asif A. Ghazanfar EMC - Otorrinolaringología49:1 (1-16)1 Feb 2020Uso forzado de la vozL. Crevier Buchman, A. Mattei and A. Giovanni The Journal of the Acoustical Society of America147:1 (245-259)1 Jan 2020Longer vowel duration correlates with greater tongue root advancement at vowel offset: Acoustic and articulatory data from Italian and PolishStefano Coretta Takeshi Nishimura (2020) Primate Vocal Anatomy and Physiology: Similarities and Differences Between Humans and Nonhuman Primates The Origins of Language Revisited10.1007/978-981-15-4250-3_2 Lisa Bartha-Doering, Peter Birkholz, Cori Casanova, Felix de Jong, Wivine Decoster, Ilter Denizoglu, Rolf Dierichs, Christian Dobel, Michèle Kaufmann-Meyer, Malte Kob, Anders Löfqvist, Dirk Mürbe, Christiane Neuschaefer-Rube, Christo Pantev, Bernhard Richter, Ken Roßlau, Oskar Schindler, Harm K. Schutte, Ad Snik, Claudia Spahn, Kurt Stephan and Jürgen Wendler (2020) Basics of Phoniatrics Phoniatrics I10.1007/978-3-662-46780-0_1 Sid M. Khosla and Hayley Born (2020) Laryngeal Physiology Neurologic and Neurodegenerative Diseases of the Larynx10.1007/978-3-030-28852-5_2 Matthew R. Hoffman, Maia N. Braden and J. Scott McMurray (2020) Physiology of Voice Production Multidisciplinary Management of Pediatric Voice and Swallowing Disorders10.1007/978-3-030-26191-7_6 The Journal of the Acoustical Society of America146:5 (3184-3202)1 Nov 2019Refining algorithmic estimation of relative fundamental frequency: Accounting for sample characteristics and fundamental frequency estimation methodJennifer M. Vojtech, Roxanne K. Segina, Daniel P. Buckley, Katharine R. Kolin, Monique C. Tardif, J. Pieter Noordzij and Cara E. Stepp Journal of Voice1 Sep 2019Acoustic and Aerodynamic Comparisons of Voice Qualities Produced After Voice TrainingNicholas A. Barone, Christy L. Ludlow and Cari M. Tellis Audiology and Speech Research15:3 (223-231)31 Jul 2019Effect of Speech Stimulus, Gender, and Age on Phonation Threshold Measures in Korean AdultsJu Eun Park, Seong Hee Choi, Kyoung Jae Lee and Chul-Hee Choi Head & Neck41:7 (2324-2331)1 Jul 2019Predicting glottal closure insufficiency using fundamental frequency contour analysisJacob T. Cohen, Alma Cohen, Limor Benyamini, Yossi Adi and Joseph Keshet The Anatomical Record302:5 (703-717)1 May 2019Anatomy and Functional Morphology of the Mysticete Rorqual Whale Larynx: Phonation Positions of the U‐FoldJULIETTE DAMIEN, OLIVIER ADAM, DORIAN CAZAU, PAUL WHITE, JEFFREY T. LAITMAN and JOY S. REIDENBERG Movement Disorders Clinical Practice6:3 (243-249)1 Mar 2019Relationship Between Respiratory Sensory Perception, Speech, and Swallow in Parkinson's DiseaseKaren W. Hegland, Michelle Troche and Alexandra Brandimore Journal of Phonetics72 (52-65)1 Jan 2019Voice onset time and beyond: Exploring laryngeal contrast in 19 languagesTaehong Cho, D.H. Whalen and Gerard Docherty Journal of Voice1 Jan 2019In Vivo Quantification of the Intraglottal Pressure: Modal Phonation and Voice OnsetPhilippe H. DeJonckere and Jean Lebacq (2018) Speech Articulation Phonetics10.1017/9781108289849.002 Ratree Wayland (2020) Phonetics Journal of Mechanics34:6 (791-800)1 Dec 2018Three Dimensional FSI Modelling of Sulcus Vocalis Disorders of Vocal FoldsA. Vazifehdoostsaleh, N. Fatouraee, M. Navidbakhsh and F. Izadi The Laryngoscope128:10 (2367-2374)1 Oct 2018Clinical relevance of endoscopic three-dimensional imaging for quantitative assessment of phonationMarion Semmler, Michael Döllinger, Rita R. Patel, Anke Ziethe and Anne Schützenberger The Journal of Experimental Biology221:16 (jeb172247)15 Aug 2018 Quantifying syringeal dynamics in vitro using electroglottography Jeppe H. Rasmussen, Christian T. Herbst and Coen P. H. Elemans The Journal of Experimental Biology221:12 (jeb171801)15 Jun 2018Japanese macaque phonatory physiologyChristian T. Herbst, Hiroki Koda, Takumi Kunieda, Juri Suzuki, Maxime Garcia, W. Tecumseh Fitch and Takeshi Nishimura Computer Methods in Biomechanics and Biomedical Engineering21:8 (532-540)11 Jun 2018Numerical analysis and comparison of flow fields in normal larynx and larynx with unilateral vocal fold paralysisAmirhossein Bagheri Sarvestani, Ebrahim Goshtasbi Rad and Kamyar Iravani Journal of Medical Devices12:11 Mar 2018Design and Evaluation of a Mechanically Driven Artificial Speech DeviceTyler G. Tuttle and Byron D. Erath Annual Review of Linguistics4:1 (255-279)14 Jan 2018The Biology and Evolution of Speech: A Comparative AnalysisW. Tecumseh Fitch Anthropological Science126:1 (3-8)The descended larynx and the descending larynxTAKESHI NISHIMURA Anthropological Science126:1 (19-27)Non-invasive documentation of primate voice production using electroglottographyCHRISTIAN T. HERBST and JACOB C. DUNN Anthropological Science126:1 (9-17)Excised larynx experimentation: history, current developments, and prospects for bioacoustic researchMAXIME GARCIA and CHRISTIAN T. HERBST Procedia Computer Science126 (423-430)Voice source modelling using modified LF model with reduced parametersAnis Ben Aicha Scientific Reports7:112 Sep 2017In situ vocal fold properties and pitch prediction by dynamic actuation of the songbird syrinxDaniel N. Düring, Benjamin J. Knörlein and Coen P. H. Elemans Scientific Reports7:15 Sep 2017Acoustic allometry revisited: morphological determinants of fundamental frequency in primate vocal productionMaxime Garcia, Christian T. Herbst, Daniel L. Bowling, Jacob C. Dunn and W. Tecumseh Fitch Tissue and Cell49:3 (427-434)1 Jun 2017Structurally and functionally characterized in vitro model of rabbit vocal fold epitheliumMasanobu Mizuta, Takashi Kurita, Emily E. Kimball and Bernard Rousseau Journal of Voice31:3 (378.e1-378.e11)1 May 2017Metabolic Mechanisms of Vocal FatigueChayadevie Nanjundeswaran, Jessie VanSwearingen and Katherine Verdolini Abbott The Laryngoscope127:5 (1102-1108)1 May 2017A New Conceptual Approach for Voice Feminization: 12 Years of ExperienceHyung‐Tae Kim International Journal of Pharma and Bio Sciences8:26 Apr 2017Creation of voice database, acoustic analysis and standardisation of normal indian voicesLATHADEVI H T, MALIPATIL SR and S P GUGGARI GOUDAR The Journal of the Acoustical Society of America141:3 (1715-1725)1 Mar 2017Relations among subglottal pressure, breathing, and acoustic parameters of sentence-level prominence in GermanCaterina Petrone, Susanne Fuchs and Laura L. Koenig Nowa Audiofonologia6:4 (16-20)26 Oct 2020Process of voice production – an overview of the current literaturePaulina Krasnodębska, Tomasz Wolak and Agata Szkiełkowska Pattern Recognition and Image Analysis27:1 (139-151)1 Jan 2017Determination of a vocal source by the spectral ratio methodV. N. Sorokin and A. S. Leonov Journal of Voice31:1 (116.e1-116.e5)1 Jan 2017The Potential Role of Subglottal Convergence Angle and MeasurementXinlin Xu, Jingan Wang, Erin E. Devine, Yong Wang, Hua Zhong, Maxwell R. Courtright, Li Zhou, PeiYun Zhuang and Jack J. Jiang Petr Hájek, Pavel Švancara, Jaromír Horáček and Jan G. Švec (2017) Numerical Simulation of the Self-oscillating Vocal Folds in Interaction with Vocal Tract Shaped for Particular Czech Vowels Recent Global Research and Education: Technological Challenges10.1007/978-3-319-46490-9_43 MOJ Clinical & Medical Case Reports5:39 Dec 2016Management of Laryngo Tracheal Injury Our ExperienceSanjeev Mohanty The Journal of the Acoustical Society of America140:4 (2614-2635)1 Oct 2016Mechanics of human voice production and controlZhaoyan Zhang Journal of Speech, Language, and Hearing Research59:5 (1002-1017)1 Oct 2016Exploring the Clinical Utility of Relative Fundamental Frequency as an Objective Measure of Vocal HyperfunctionNelson Roy, Rebecca A. Fetrow, Ray M. Merrill and Christopher Dromey The Laryngoscope126:7 (1589-1594)1 Jul 2016Nonstimulated rabbit phonation model: Cricothyroid approximationCarolyn K. Novaleski, Tsuyoshi Kojima, Siyuan Chang, Haoxiang Luo, Carla V. Valenzuela and Bernard Rousseau PLOS Computational Biology12:6 (e1004907)16 Jun 2016Predicting Achievable Fundamental Frequency Ranges in Vocalization Across SpeciesIngo Titze, Tobias Riede, Ted Mau and Frédéric E. Theunissen Computer Speech & Language36 (365-394)1 Mar 2016Nonlinear interactive source-filter models for speechTurgay Koc and Tolga Ciloglu Journal of Biomechanical Science and Engineering11:4 (16-00414-16-00414)A possible common physical principle that underlies animal vocalization: theoretical considerations with an unsteady airflow-structure interaction modelShinji DEGUCHI Cells Tissues Organs202:5-6 (355-368)Evaluation of Dying Vocal Fold Epithelial Cells by Ultrastructural Features and TUNEL MethodCarolyn K. Novaleski, Masanobu Mizuta and Bernard Rousseau Christian T. Herbst (2016) Biophysics of Vocal Production in Mammals Vertebrate Sound Production and Acoustic Communication10.1007/978-3-319-27721-9_6 Daniel N. Düring and Coen P. H. Elemans (2016) Embodied Motor Control of Avian Vocal Production Vertebrate Sound Production and Acoustic Communication10.1007/978-3-319-27721-9_5 W. Tecumseh Fitch (2016) Vertebrate Bioacoustics: Prospects and Open Problems Vertebrate Sound Production and Acoustic Communication10.1007/978-3-319-27721-9_10 W. Tecumseh Fitch and Roderick A. Suthers (2016) Vertebrate Vocal Production: An Introductory Overview Vertebrate Sound Production and Acoustic Communication10.1007/978-3-319-27721-9_1 Communication Sciences & Disorders20:4 (607-616)31 Dec 2016Pattern Analysis of Voice Onset and Offset in Normal Adults Using High-Speed Digital Imaging: The Role of Arytenoid Cartilage MovementsSeong Hee Choi, Chi-Sun Oh and Chul-Hee Choi Nature Communications6:11 Dec 2015Universal mechanisms of sound production and control in birds and mammalsC.P.H Elemans, J.H. Rasmussen, C.T. Herbst, D.N. Düring, S.A. Zollinger, H. Brumm, K. Srivastava, N. Svane, M. Ding, O.N. Larsen, S.J. Sober and J.G. Švec Journal of Phonetics52 (35-45)1 Sep 2015Assessing respiratory contributions to f 0 declination in German across varying speech tasks and respiratory demandsSusanne Fuchs, Caterina Petrone, Amélie Rochet-Capellan, Uwe D. Reichel and Laura L. Koenig Ross D. Farhadieh and Wayne A.J. Morrison (2015) Pharyngeal reconstruction Plastic and reconstructive surgery10.1002/9781118655412.ch28 Journal of Experimental Biology218:7 (991-998)1 Apr 2015 Functional morphology of the Alligator mississippiensis larynx with implications for vocal production Tobias Riede, Zhiheng Li, Isao T. Tokuda and Colleen G. Farmer Brad H. Story (2015) Mechanisms of Voice Production The Handbook of Speech Production10.1002/9781118584156.ch3 The Journal of the Acoustical Society of America137:3 (1493-1502)1 Mar 2015Three speech sounds, one motor action: Evidence for speech-motor disparity from English flap productionDonald Derrick, Ian Stavness and Bryan Gick Cancer Research75:1 (31-39)1 Jan 2015A Noninvasive Procedure for Early-Stage Discrimination of Malignant and Precancerous Vocal Fold Lesions Based on Laryngeal Dynamics AnalysisJakob Unger, Jörg Lohscheller, Maximilian Reiter, Christian S. and Maria Computer Methods in Biomechanics and Biomedical Dec of in vocal fold and The Journal of the Acoustical Society of Nov of the of pressure on vocal fold H. and Zhang Journal of Nov of Vocal Fold A Review K. Computer Speech & Sep source analysis to and Journal of Jul Fold Dynamics for Frequency Journal of and Jul of a model of and Jul International Journal of & Jun a Larynx: A in Clinical and The Apr of and fields in excised Jun and The Journal of the Acoustical Society of Jan and in Sid Khosla and Peters, R. J. Van K. Van and The Jan of and laryngeal D. Journal of Experimental Nov in an T. Herbst, Jan G. Švec, Jörg Lohscheller, S. and W. Tecumseh Fitch Medical Engineering & Aug dynamics of vocal using normal P. and Journal of Jul and Acoustic of Vocal A. and Brad H. Story Audiology - Communication de Carla José and of Biomedical Dec in Vocal Fold the of E. and W. Journal of Nov of Differences in Using High-Speed Videoendoscopy and F. Maria E. and Dimitar D. Deliyski Aug Production of T. Herbst, S. Jörg Lohscheller, Ingo R. Titze, and W. Tecumseh Fitch Research in Jun the Voicing of English Voicing Voicing and Journal of Mar Voice in Digital Signal Mar of dynamics of vocal using and P. David E. D. José C. and The Feb of on acoustic characteristics of A. N. E. and Acoustical Science and of on the of flow the Control in and Speech of Paul and L. T. V. Aerodynamic and Acoustic Theory of Voice Production Forensic The Journal of the Acoustical Society of Dec acoustic correlates of human vocal fold modeling and laryngeal D. Matías Zañartu, F. Dimitar D. Deliyski and E. Oct analysis of speech and Journal of Experimental Sep Subglottal pressure and fundamental frequency control in of Alligator mississippiensis Tobias Riede, Isao T. Tokuda and C. G. Farmer of Voice The Journal of the Acoustical Society of Apr the and as phonation onset Zhang Mar of and Threshold Biomechanical for Voice and The Journal of the Acoustical Society of Mar acoustic model of the subglottal for speech C. Matías and R. Medical Engineering & Mar of the glottal into a C. M. and M. Döllinger The Journal of & Feb S Donald F. of for Phonation and J. of the Respiratory System Journal of the International Dec acoustic correlates of the contrast in M. T. and Journal of Speech, Language, and Hearing Oct of Vocal on Relative Fundamental Frequency Voicing Offset and E. E. and T. Brain and Oct mechanisms for vocal production in birds – and to human speech and and Journal of Experimental Sep and of vocal Tobias Plastic and Reconstructive Aug of the and Christopher J. F. Kim and Hung-Chi Chen on Voice and Voice Jul of Voice R. The Journal of the Acoustical Society of Mar of the glottal and Laryngeal and Pharyngeal - Head and Neck and and of the vocal Handbook of Mammalian Vocalization - An S. and Jennifer L. of the larynx and production of Handbook of Mammalian Vocalization - An The comparison of properties using F. J. and Jack J. Jiang Journal of Jul with vocal properties of their vocal Riede, J. and Ingo R. Titze Tissue Engineering Part B: Sep Engineering for the Vocal Fold K. and Ken Feb tract in and and Journal of Jan of of on acoustic correlates of the contrast in B. Speech Physiology to Journal of Nov and Aerodynamic for Voicing of A L. C. and W. The Oct of Voice in the by an S. and J. Mar in A. P. A. and Paul F. Acoustical Science and the interaction of the flow the of Oct in unilateral laryngeal theoretical Richard and Antoine Giovanni Journal of Sep as a The use of in a J. The Journal of the Acoustical Society of Jul of modeling of and voice with excised larynx T. Tokuda, Jan G. Švec and of & Feb Voice in by Laryngeal of M. and J. The Journal of the Acoustical Society of Feb of acoustic on an model of the vocal Zañartu, and R. Research Mar of voiced using models of the vocal and subglottal R. and EMC - Jan de la and The Journal of the Acoustical Society of Oct", "authors": ["Janwillem van den Berg"], "year": 1958, "venue": "Journal of Speech and Hearing Research", "cited_by_count": 511, "type": "article", "concepts": ["Aerodynamics", "Production (economics)", "Speech production", "Computer science", "Speech recognition"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4404515348", "doi": "https://doi.org/10.1016/j.inffus.2024.102790", "title": "TextFusion: Unveiling the power of textual semantics for controllable image fusion", "abstract": "", "authors": ["Chunyang Cheng", "Tianyang Xu", "Xiaojun Wu", "Hui Li", "Xi Li", "Zhangyong Tang", "Josef Kittler"], "year": 2024, "venue": "Information Fusion", "cited_by_count": 25, "type": "article", "concepts": ["Computer science", "Semantics (computer science)", "Image (mathematics)", "Power (physics)", "Fusion"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W2582406074", "doi": "https://doi.org/10.1186/s12913-017-2031-8", "title": "Acceptability of healthcare interventions: an overview of reviews and development of a theoretical framework", "abstract": "", "authors": ["Mandeep Sekhon", "Martin Cartwright", "Jill Francis"], "year": 2017, "venue": "BMC Health Services Research", "cited_by_count": 3572, "type": "article", "concepts": ["Nursing research", "Health informatics", "Health administration", "Medicine", "Public health"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W1909740415", "doi": "https://doi.org/10.1186/s12880-015-0068-x", "title": "Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool", "abstract": "", "authors": ["Abdel Aziz Taha", "Allan Hanbury"], "year": 2015, "venue": "BMC Medical Imaging", "cited_by_count": 2601, "type": "article", "concepts": ["Computer science", "Segmentation", "Metric (unit)", "Scale-space segmentation", "Image segmentation"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4220959955", "doi": "https://doi.org/10.1002/cl2.1230", "title": "<i>PRISMA2020</i> : An R package and Shiny app for producing PRISMA 2020‐compliant flow diagrams, with interactivity for optimised digital transparency and Open Synthesis", "abstract": "We have developed a user-friendly tool for producing PRISMA 2020-compliant flow diagrams for users with coding experience and, importantly, for users without prior experience in coding by making use of Shiny (https://estech.shinyapps.io/prisma_flowdiagram/). This free-to-use tool will make it easier to produce clear and PRISMA 2020-compliant systematic review flow diagrams. Significantly, users can also produce interactive flow diagrams for the first time, allowing readers of their reviews to smoothly and swiftly explore and navigate to further details of the methods and results of a review. We believe this tool will increase use of PRISMA flow diagrams, improve the compliance and quality of flow diagrams, and facilitate strong science communication of the methods and results of systematic reviews by making use of interactivity. We encourage the systematic review community to make use of the tool, and provide feedback to streamline and improve their usability and efficiency.", "authors": ["Neal Haddaway", "Matthew J. Page", "Chris C. Pritchard", "Luke A. McGuinness"], "year": 2022, "venue": "Campbell Systematic Reviews", "cited_by_count": 2683, "type": "article", "concepts": ["Interactivity", "Transparency (behavior)", "Computer science", "World Wide Web", "Computer security"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W2048505725", "doi": "https://doi.org/10.1016/j.jalz.2014.01.001", "title": "A conceptual framework for research on subjective cognitive decline in preclinical Alzheimer's disease", "abstract": "There is increasing evidence that subjective cognitive decline (SCD) in individuals with unimpaired performance on cognitive tests may represent the first symptomatic manifestation of Alzheimer's disease (AD). The research on SCD in early AD, however, is limited by the absence of common standards. The working group of the Subjective Cognitive Decline Initiative (SCD-I) addressed this deficiency by reaching consensus on terminology and on a conceptual framework for research on SCD in AD. In this publication, research criteria for SCD in pre-mild cognitive impairment (MCI) are presented. In addition, a list of core features proposed for reporting in SCD studies is provided, which will enable comparability of research across different settings. Finally, a set of features is presented, which in accordance with current knowledge, increases the likelihood of the presence of preclinical AD in individuals with SCD. This list is referred to as SCD plus.", "authors": ["Frank Jessen", "Rebecca E. Amariglio", "Martin P.J. van Boxtel", "Monique M.B. Breteler", "Mathieu Ceccaldi", "Gaël Chételat", "Bruno Dubois", "Carole Dufouil", "Kathryn A. Ellis", "Wiesje M. van der Flier", "Lidia Glodzik", "Argonde C. van Harten", "Mony J. de Leon", "Pauline McHugh", "Michelle M. Mielke", "José Luís Molinuevo", "Lisa Mosconi", "Ricardo S. Osorio", "Audrey Perrotin", "Ronald C. Petersen", "Laura A. Rabin", "Lorena Rami", "‌Barry Reisberg", "Dorene M. Rentz", "Perminder S. Sachdev", "Vincent de La Sayette", "Andrew J. Saykin", "Philip Scheltens", "Melanie Shulman", "Melissa J. Slavin", "Reisa A. Sperling", "Robert Stewart", "Olga Uspenskaya", "Bruno Vellas", "Pieter Jelle Visser", "Michael Wagner", "Subjective Cognitive Decline Initiative (SCD‐I) Working Group"], "year": 2014, "venue": "Alzheimer s & Dementia", "cited_by_count": 2885, "type": "review", "concepts": ["Comparability", "Cognitive decline", "Terminology", "Disease", "Cognition"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W2510708214", "doi": "https://doi.org/10.2337/dc13-s011", "title": "Standards of Medical Care in Diabetes—2013", "abstract": "Clear evidence from well-conducted, generalizable RCTs that are adequately powered, including: c Evidence from a well-conducted multicenter trial c Evidence from a meta-analysis that incorporated quality ratings in the analysis Compelling nonexperimental evidence, i.e., \"all or none\" rule developed by the Centre for Evidence-Based Medicine at the University of Oxford Supportive evidence from well-conducted RCTs that are adequately powered, including:", "authors": [], "year": 2012, "venue": "Diabetes Care", "cited_by_count": 4416, "type": "article", "concepts": ["Medicine", "Diabetes mellitus", "MEDLINE", "Family medicine", "Endocrinology"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W4401990587", "doi": "https://doi.org/10.1109/icmew63481.2024.10645451", "title": "Q-Boost: On Visual Quality Assessment Ability of Low-Level Multi-Modality Foundation Models", "abstract": "Recent advancements in Multi-modality Large Language Models (MLLMs) have demonstrated remarkable capabilities in complex high-level vision tasks. However, the exploration of MLLM potential in visual quality assessment, a vital aspect of low-level vision, remains limited. To address this gap, we introduce Q-Boost, a novel strategy designed to enhance low-level MLLMs in image quality assessment (IQA) and video quality assessment (VQA) tasks, which is structured around two pivotal components: 1) Triadic-Tone Integration: Ordinary prompt design simply oscillates between the binary extremes of positive and negative. Q-Boost innovates by incorporating a ‘middle ground’ approach through neutral prompts, allowing for a more balanced and detailed assessment. 2) Multi-Prompt Ensemble: Multiple quality-centric prompts are used to mitigate bias and acquire more accurate evaluation. The experimental results show that the low-level MLLMs exhibit outstanding zeros-shot performance on the IQA/VQA tasks equipped with Q-Boost strategy.", "authors": ["Zicheng Zhang", "Haoning Wu", "Zhongpeng Ji", "Chunyi Li", "Erli Zhang", "Wei Sun", "Xiaohong Liu", "Xiongkuo Min", "Fengyu Sun", "Shangling Jui", "Weisi Lin", "Guangtao Zhai"], "year": 2024, "venue": "", "cited_by_count": 13, "type": "article", "concepts": ["Modality (human–computer interaction)", "Foundation (evidence)", "Computer science", "Quality (philosophy)", "Artificial intelligence"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W2150353123", "doi": "https://doi.org/10.2196/jmir.8.2.e9", "title": "eHealth Literacy: Essential Skills for Consumer Health in a Networked World", "abstract": "Electronic health tools provide little value if the intended users lack the skills to effectively engage them. With nearly half the adult population in the United States and Canada having literacy levels below what is needed to fully engage in an information-rich society, the implications for using information technology to promote health and aid in health care, or for eHealth, are considerable. Engaging with eHealth requires a skill set, or literacy, of its own. The concept of eHealth literacy is introduced and defined as the ability to seek, find, understand, and appraise health information from electronic sources and apply the knowledge gained to addressing or solving a health problem. In this paper, a model of eHealth literacy is introduced, comprised of multiple literacy types, including an outline of a set of fundamental skills consumers require to derive direct benefits from eHealth. A profile of each literacy type with examples of the problems patient-clients might present is provided along with a resource list to aid health practitioners in supporting literacy improvement with their patient-clients across each domain. Facets of the model are illustrated through a set of clinical cases to demonstrate how health practitioners can address eHealth literacy issues in clinical or public health practice. Potential future applications of the model are discussed.", "authors": ["Cameron D. Norman", "Harvey A. Skinner"], "year": 2006, "venue": "Journal of Medical Internet Research", "cited_by_count": 2380, "type": "review", "concepts": ["eHealth", "Health literacy", "Literacy", "Health care", "Information literacy"], "search_query": "visual language model image quality assessment"}
{"openalex_id": "https://openalex.org/W2566149141", "doi": "https://doi.org/10.1109/jstsp.2016.2639328", "title": "Fully Deep Blind Image Quality Predictor", "abstract": "In general, owing to the benefits obtained from original information, full-reference image quality assessment (FR-IQA) achieves relatively higher prediction accuracy than no-reference image quality assessment (NR-IQA). By fully utilizing reference images, conventional FR-IQA methods have been investigated to produce objective scores that are close to subjective scores. In contrast, NR-IQA does not consider reference images; thus, its performance is inferior to that of FR-IQA. To alleviate this accuracy discrepancy between FR-IQA and NR-IQA methods, we propose a blind image evaluator based on a convolutional neural network (BIECON). To imitate FR-IQA behavior, we adopt the strong representation power of a deep convolutional neural network to generate a local quality map, similar to FR-IQA. To obtain the best results from the deep neural network, replacing hand-crafted features with automatically learned features is necessary. To apply the deep model to the NR-IQA framework, three critical problems must be resolved: 1) lack of training data; 2) absence of local ground truth targets; and 3) different purposes of feature learning. BIECON follows the FR-IQA behavior using the local quality maps as intermediate targets for conventional neural networks, which leads to NR-IQA prediction accuracy that is comparable with that of state-of-the-art FR-IQA methods.", "authors": ["Jongyoo Kim", "Sanghoon Lee"], "year": 2016, "venue": "IEEE Journal of Selected Topics in Signal Processing", "cited_by_count": 455, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Convolutional neural network", "Image quality", "Pattern recognition (psychology)"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W2797079651", "doi": "https://doi.org/10.1016/j.patcog.2018.04.016", "title": "Blind image quality prediction by exploiting multi-level deep representations", "abstract": "", "authors": ["Fei Gao", "Jun Yu", "Suguo Zhu", "Qingming Huang", "Qi Tian"], "year": 2018, "venue": "Pattern Recognition", "cited_by_count": 120, "type": "article", "concepts": ["Artificial intelligence", "Computer science", "Pattern recognition (psychology)", "Image (mathematics)", "Quality (philosophy)"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W2971622202", "doi": "https://doi.org/10.1109/access.2019.2938900", "title": "A Survey of DNN Methods for Blind Image Quality Assessment", "abstract": "Blind image quality assessment (BIQA) methods aim to predict quality of images as perceived by humans without access to a reference image. Recently, deep learning methods have gained substantial attention in the research community and have proven useful for BIQA. Although previous study of deep neural networks (DNN) methods is presented, some novelty DNN methods, which are recently proposed, are not summarized for BIQA. In this paper, we provide a survey covering various DNN methods for BIQA. First, we systematically analyze the existing DNN-based quality assessment methods according to the role of DNN. Then, we compare the prediction performance of various DNN methods on the synthetic databases (LIVE, TID2013, CSIQ, LIVE multiply distorted) and authentic databases (LIVE challenge), providing important information that can help understand the underlying properties between different DNN methods for BIQA. Finally, we describe some emerging challenges in designing and training DNN-based BIQA, along with few directions that are worth further investigations in the future.", "authors": ["Xiaohan Yang", "Fan Li", "Hantao Liu"], "year": 2019, "venue": "IEEE Access", "cited_by_count": 73, "type": "article", "concepts": ["Novelty", "Computer science", "Artificial intelligence", "Artificial neural network", "Image quality"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W2083704334", "doi": "https://doi.org/10.1109/icip.2014.7025102", "title": "Deep learning network for blind image quality assessment", "abstract": "Nowadays, blind image quality assessment (BIQA) has been intensively studied with machine learning, such as support vector machine (SVM) and k-means. Existing BIQA metrics, however, do not perform robust for various kinds of distortion types. We believe this problem is because those frequently used traditional machine learning techniques exploit shallow architectures, which only contain one single layer of nonlinear feature transformation, and thus cannot highly mimic the mechanism of human visual perception to image quality. The recent advance of deep neural network (DNN) can help to solve this problem, since the DNN is found to better capture the essential attributes of images. We in this paper therefore introduce a new Deep learning based Image Quality Index (DIQI) for blind quality assessment. Extensive studies are conducted on the new TID2013 database and confirm the effectiveness of our DIQI relative to classical full-reference and state-of-the-art reduced- and no-reference IQA approaches.", "authors": ["Ke Gu", "Guangtao Zhai", "Xiaokang Yang", "Wenjun Zhang"], "year": 2014, "venue": "", "cited_by_count": 61, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Distortion (music)", "Support vector machine", "Machine learning"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W3193517049", "doi": "https://doi.org/10.1109/iccv48922.2021.01008", "title": "Learning Conditional Knowledge Distillation for Degraded-Reference Image Quality Assessment", "abstract": "An important scenario for image quality assessment (IQA) is to evaluate image restoration (IR) algorithms. The state-of-the-art approaches adopt a full-reference paradigm that compares restored images with their corresponding pristine-quality images. However, pristine-quality images are usually unavailable in blind image restoration tasks and real-world scenarios. In this paper, we propose a practical solution named degraded-reference IQA (DR-IQA), which exploits the inputs of IR models, degraded images, as references. Specifically, we extract reference information from degraded images by distilling knowledge from pristine-quality images. The distillation is achieved through learning a reference space, where various degraded images are encouraged to share the same feature statistics with pristine-quality images. And the reference space is optimized to capture deep image priors that are useful for quality assessment. Note that pristine-quality images are only used during training. Our work provides a powerful and differentiable metric for blind IRs, especially for GAN-based methods. Extensive experiments show that our results can even be close to the performance of full-reference settings.", "authors": ["Heliang Zheng", "Huan Yang", "Jianlong Fu", "Zheng-Jun Zha", "Jiebo Luo"], "year": 2021, "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "cited_by_count": 52, "type": "article", "concepts": ["Computer science", "Artificial intelligence", "Metric (unit)", "Image quality", "Quality (philosophy)"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W2963833613", "doi": "https://doi.org/10.1016/j.mri.2018.07.003", "title": "A machine-learning framework for automatic reference-free quality assessment in MRI", "abstract": "", "authors": ["Thomas Küstner", "Sergios Gatidis", "Annika Liebgott", "Martin Schwartz", "Lars Mauch", "Petros Martirosian", "H. R. Schmidt", "NF Schwenzer", "Konstantin Nikolaou", "Fabian Bamberg", "Bin Yang", "Fritz Schick"], "year": 2018, "venue": "Magnetic Resonance Imaging", "cited_by_count": 61, "type": "article", "concepts": ["Computer science", "Image quality", "Artificial intelligence", "Quality assurance", "Scanner"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W3016917677", "doi": "https://doi.org/10.1016/j.ins.2020.04.030", "title": "Blind quality assessment for image superresolution using deep two-stream convolutional networks", "abstract": "", "authors": ["Wei Zhou", "Qiuping Jiang", "Yuwang Wang", "Zhibo Chen", "Weiping Li"], "year": 2020, "venue": "Information Sciences", "cited_by_count": 77, "type": "article", "concepts": ["Artificial intelligence", "Computer science", "Discriminative model", "Convolutional neural network", "Pattern recognition (psychology)"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W2752223497", "doi": "https://doi.org/10.48550/arxiv.1708.08190", "title": "A Probabilistic Quality Representation Approach to Deep Blind Image Quality Prediction", "abstract": "Blind image quality assessment (BIQA) remains a very challenging problem due to the unavailability of a reference image. Deep learning based BIQA methods have been attracting increasing attention in recent years, yet it remains a difficult task to train a robust deep BIQA model because of the very limited number of training samples with human subjective scores. Most existing methods learn a regression network to minimize the prediction error of a scalar image quality score. However, such a scheme ignores the fact that an image will receive divergent subjective scores from different subjects, which cannot be adequately represented by a single scalar number. This is particularly true on complex, real-world distorted images. Moreover, images may broadly differ in their distributions of assigned subjective scores. Recognizing this, we propose a new representation of perceptual image quality, called probabilistic quality representation (PQR), to describe the image subjective score distribution, whereby a more robust loss function can be employed to train a deep BIQA model. The proposed PQR method is shown to not only speed up the convergence of deep model training, but to also greatly improve the achievable level of quality prediction accuracy relative to scalar quality score regression methods. The source code is available at https://github.com/HuiZeng/BIQA_Toolbox.", "authors": ["Hui Zeng", "Lei Zhang", "Alan C. Bovik"], "year": 2017, "venue": "arXiv (Cornell University)", "cited_by_count": 61, "type": "preprint", "concepts": ["Probabilistic logic", "Artificial intelligence", "Quality (philosophy)", "Computer science", "Representation (politics)"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W2296265007", "doi": "https://doi.org/10.1109/icip.2015.7351221", "title": "Difference of Gaussian statistical features based blind image quality assessment: A deep learning approach", "abstract": "Nowadays, natural scene statistics (NSS) based blind image quality assessment (BIQA) models trained by machine learning, tend to achieve excellent performance. However, BIQA is still a very challenging research topic due to the lack of reference images. The key of further improvement lies in feature mining and pooling strategy decision. In this work, a new BIQA model is proposed to utilize local normalized multi-scale difference of Gaussian (DoG) response in distorted images as features which show a high correlation with perceptual quality. Then, a three-step-framework based deep neural network (DNN) is designed and employed as the pooling strategy. Compared with the support vector machine (SVM), the proposed three-step-framework DNN can excavate better feature representation, leading to more accurate predictions and stronger generalization ability. The proposed model achieves state-of-the-art performance on two authoritative databases and excellent generalization ability in cross database experiments.", "authors": ["Yaqi Lv", "Gangyi Jiang", "Mei Yu", "Haiyong Xu", "Feng Shao", "Shanshan Liu"], "year": 2015, "venue": "", "cited_by_count": 45, "type": "article", "concepts": ["Pooling", "Computer science", "Support vector machine", "Artificial intelligence", "Generalization"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W2925250639", "doi": "https://doi.org/10.1109/access.2019.2905615", "title": "No-Reference Quality Assessment for Pansharpened Images via Opinion-Unaware Learning", "abstract": "The high-quality pansharpened image with both high spatial resolution and high spectral fidelity is highly desirable in various applications. However, existing pansharpening methods may lead to spatial distortion and spectral distortion. To measure the degrees of distortion caused by the pansharpening methods, we conduct in-deep studies on the subjective and objective quality assessment of pansharpened images. We built a subjective database consisting of 360 images generated from 20 couples of panchromatic (PAN)/multispectral (MS) images using 18 pansharpening methods. Based on the database, we proposed a no-reference quality assessment method to blindly predict the quality of pansharpened images via opinion-unaware learning. The proposed method first extracted features from the MS images' spectral bands and typical information indexes which comprehensively reflect spatial distortion, spectral distortion, and the effects of pansharpening on applications. Based on the features extracted from the pristine MS image training dataset, a benchmark multivariate Gaussian (MVG) model is learned. The distance between the benchmark MVG and the MVG fitted on the test image is calculated to measure the quality. The experimental results show the superiority of our method on our database.", "authors": ["Bingzhong Zhou", "Feng Shao", "Xiangchao Meng", "Randi Fu", "Yo‐Sung Ho"], "year": 2019, "venue": "IEEE Access", "cited_by_count": 21, "type": "article", "concepts": ["Panchromatic film", "Multispectral image", "Computer science", "Artificial intelligence", "Distortion (music)"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W2793583884", "doi": "https://doi.org/10.1109/icip.2017.8296869", "title": "Deep blind image quality assessment by employing FR-IQA", "abstract": "In this paper, we propose a convolutional neural network (CNN)-based no-reference image quality assessment (NR-IQA). Though deep learning has yielded superior performance in a number of computer vision studies, applying the deep CNN to the NR-IQA framework is not straightforward, since we face a few critical problems: 1) lack of training data; 2) absence of local ground truth targets. To alleviate these problems, we employ the full-reference image quality assessment (FR-IQA) metrics as intermediate training targets of the CNN. In addition, we incorporate the pooling stage in the training stage, so that the whole parameters of the model can be optimized in an end-to-end framework. The proposed model, named as a blind image evaluator based on a convolutional neural network (BIECON), achieves state-of-the-art prediction accuracy that is comparable with that of FR-IQA methods.", "authors": ["Jongyoo Kim", "Sanghoon Lee"], "year": 2017, "venue": "", "cited_by_count": 21, "type": "article", "concepts": ["Computer science", "Convolutional neural network", "Pooling", "Artificial intelligence", "Deep learning"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W4292559719", "doi": "https://doi.org/10.1016/j.imed.2022.08.001", "title": "Automated assessment of transthoracic echocardiogram image quality using deep neural networks", "abstract": "", "authors": ["Robert B. Labs", "Apostolos Vrettos", "Jonathan Loo", "Massoud Zolgharni"], "year": 2022, "venue": "Intelligent Medicine", "cited_by_count": 20, "type": "article", "concepts": ["Image quality", "Computer science", "Artificial intelligence", "Transthoracic echocardiogram", "Deep learning"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W3149775056", "doi": "https://doi.org/10.1109/tpami.2021.3071759", "title": "Active Fine-Tuning from gMAD Examples Improves Blind Image Quality Assessment", "abstract": "The research in image quality assessment (IQA) has a long history, and significant progress has been made by leveraging recent advances in deep neural networks (DNNs). Despite high correlation numbers on existing IQA datasets, DNN-based models may be easily falsified in the group maximum differentiation (gMAD) competition. Here we show that gMAD examples can be used to improve blind IQA (BIQA) methods. Specifically, we first pre-train a DNN-based BIQA model using multiple noisy annotators, and fine-tune it on multiple synthetically distorted images, resulting in a \"top-performing\" baseline model. We then seek pairs of images by comparing the baseline model with a set of full-reference IQA methods in gMAD. The spotted gMAD examples are most likely to reveal the weaknesses of the baseline, and suggest potential ways for refinement. We query human quality annotations for the selected images in a well-controlled laboratory environment, and further fine-tune the baseline on the combination of human-rated images from gMAD and existing databases. This process may be iterated, enabling active fine-tuning from gMAD examples for BIQA. We demonstrate the feasibility of our active learning scheme on a large-scale unlabeled image set, and show that the fine-tuned quality model achieves improved generalizability in gMAD, without destroying performance on previously seen databases.", "authors": ["Zhihua Wang", "Kede Ma"], "year": 2021, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "cited_by_count": 30, "type": "article", "concepts": ["Computer science", "Generalizability theory", "Artificial intelligence", "Set (abstract data type)", "Image quality"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W3212793811", "doi": "https://doi.org/10.1109/tcsvt.2021.3128014", "title": "Multi-Angle Projection Based Blind Omnidirectional Image Quality Assessment", "abstract": "Most of the existing blind omnidirectional image quality assessment (BOIQA) methods are based on data-driven approach where the end-to-end neural network or deep learning tools are mainly used for feature extraction. However, it usually lacks interpretability and is difficult to discover the perceptual mechanism behind. In this paper, from the perspective of perception modeling, we propose a novel multi-angle projection based BOIQA (MP-BOIQA) method. Considering the omnibearing and near eye display characteristics with head mounted display, multiple color cubemap projection images with respect to different viewpoints are grouped as the color omnidirectional distortion (COD) units so as to simulate the user’s viewing behavior in subjective quality assessment. In the designed multi-angle projection based feature extractor, tensor decomposition is implemented on each COD unit for dimensionality reduction, and piecewise exponential fitting is used to get the distribution of mean subtracted contrast normalized coefficients of the unit’s feature matrices in tensor domain. Finally, the extracted features are pooled with random forest. The experimental results on three omnidirectional image quality datasets show that the MP-BOIQA method can deliver highly competitive performance compared with some representative full-reference quality assessment methods, as well as some state-of-the-art BOIQA methods.", "authors": ["Hao Jiang", "Gangyi Jiang", "Mei Yu", "Ting Luo", "Haiyong Xu"], "year": 2021, "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "cited_by_count": 21, "type": "article", "concepts": ["Artificial intelligence", "Computer science", "Computer vision", "Projection (relational algebra)", "Feature extraction"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W4210559218", "doi": "https://doi.org/10.1007/s00500-021-06662-9", "title": "Deep ensembling for perceptual image quality assessment", "abstract": "", "authors": ["Nisar Ahmed", "Hafiz Muhammad Shahzad Asif", "Abdul Rauf Bhatti", "Atif Khan"], "year": 2022, "venue": "Soft Computing", "cited_by_count": 20, "type": "article", "concepts": ["Computer science", "Benchmark (surveying)", "Artificial intelligence", "Distortion (music)", "Generalization"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W4206481149", "doi": "https://doi.org/10.1016/s0140-6736(21)00218-x", "title": "Parkinson's disease", "abstract": "", "authors": ["Bastiaan R. Bloem", "Michael S. Okun", "Christine Klein"], "year": 2021, "venue": "The Lancet", "cited_by_count": 3263, "type": "review", "concepts": ["Disease", "Parkinson's disease", "Medicine", "Parkinsonism", "Levodopa"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W2922226472", "doi": "https://doi.org/10.1109/istel.2018.8661024", "title": "No-Reference Image Quality Assessment using Transfer Learning", "abstract": "With the recent advancements in deep learning, high performance neural networks have been introduced. These neural networks also can be used to solve similar problems in a transfer learning approach. Recently, several state-of-the-art Convolutional Neural Networks (CNNs) are proposed for computer vision tasks. On the other hand, in-the-wild No-Reference (Blind) Image Quality Assessment (NR-IQA) problem is known as a challenging human perceptual problem. In this paper, a transfer learning approach is used to solve the problem of in-the-wild NR-IQA. With a few training times, the proposed neural network exceeds all the previous methods which are not using deep neural networks. Further, the proposed method predicts the opinion score distribution in its output which has more valuable information than single Mean Opinion Score (MOS). Moreover, the proposed method can accept arbitrary image size in its input which is often not applicable in the most CNNs which have a certain output size.", "authors": ["Hatef Otroshi Shahreza", "Arash Amini", "Hamid Behroozi"], "year": 2018, "venue": "", "cited_by_count": 17, "type": "article", "concepts": ["Transfer of learning", "Convolutional neural network", "Computer science", "Artificial intelligence", "Artificial neural network"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W4385574853", "doi": "https://doi.org/10.1109/tmm.2023.3301276", "title": "Going the Extra Mile in Face Image Quality Assessment: A Novel Database and Model", "abstract": "An accurate computational model for image quality assessment (IQA) benefits many vision applications, such as image filtering, image processing, and image generation. Although the study of face images is an important subfield in computer vision research, the lack of face IQA data and models limits the precision of current IQA metrics on face image processing tasks such as face superresolution, face enhancement, and face editing. To narrow this gap, in this article, we first introduce the largest annotated IQA database developed to date, which contains 20,000 human faces – an order of magnitude larger than all existing rated datasets of faces – of diverse individuals in highly varied circumstances. Based on the database, we further propose a novel deep learning model to accurately predict face image quality, which, for the first time, explores the use of generative priors for IQA. By taking advantage of rich statistics encoded in well pretrained off-the-shelf generative models, we obtain generative prior information and use it as latent references to facilitate blind IQA. The experimental results demonstrate both the value of the proposed dataset for face IQA and the superior performance of the proposed model.", "authors": ["Shaolin Su", "Hanhe Lin", "Vlad Hosu", "Oliver Wiedemann", "Jinqiu Sun", "Yu Zhu", "Hantao Liu", "Yanning Zhang", "Dietmar Saupe"], "year": 2023, "venue": "IEEE Transactions on Multimedia", "cited_by_count": 19, "type": "article", "concepts": ["Computer science", "Face (sociological concept)", "Artificial intelligence", "Image quality", "Generative model"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W3217519524", "doi": "https://doi.org/10.1016/j.image.2021.116576", "title": "QL-IQA: Learning distance distribution from quality levels for blind image quality assessment", "abstract": "", "authors": ["Rui Gao", "Ziqing Huang", "Shiguang Liu"], "year": 2021, "venue": "Signal Processing Image Communication", "cited_by_count": 14, "type": "article", "concepts": ["Quality Score", "Image quality", "Artificial intelligence", "Computer science", "Distortion (music)"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W4327946446", "doi": "https://doi.org/10.3390/healthcare11060887", "title": "ChatGPT Utility in Healthcare Education, Research, and Practice: Systematic Review on the Promising Perspectives and Valid Concerns", "abstract": "ChatGPT is an artificial intelligence (AI)-based conversational large language model (LLM). The potential applications of LLMs in health care education, research, and practice could be promising if the associated valid concerns are proactively examined and addressed. The current systematic review aimed to investigate the utility of ChatGPT in health care education, research, and practice and to highlight its potential limitations. Using the PRIMSA guidelines, a systematic search was conducted to retrieve English records in PubMed/MEDLINE and Google Scholar (published research or preprints) that examined ChatGPT in the context of health care education, research, or practice. A total of 60 records were eligible for inclusion. Benefits of ChatGPT were cited in 51/60 (85.0%) records and included: (1) improved scientific writing and enhancing research equity and versatility; (2) utility in health care research (efficient analysis of datasets, code generation, literature reviews, saving time to focus on experimental design, and drug discovery and development); (3) benefits in health care practice (streamlining the workflow, cost saving, documentation, personalized medicine, and improved health literacy); and (4) benefits in health care education including improved personalized learning and the focus on critical thinking and problem-based learning. Concerns regarding ChatGPT use were stated in 58/60 (96.7%) records including ethical, copyright, transparency, and legal issues, the risk of bias, plagiarism, lack of originality, inaccurate content with risk of hallucination, limited knowledge, incorrect citations, cybersecurity issues, and risk of infodemics. The promising applications of ChatGPT can induce paradigm shifts in health care education, research, and practice. However, the embrace of this AI chatbot should be conducted with extreme caution considering its potential limitations. As it currently stands, ChatGPT does not qualify to be listed as an author in scientific articles unless the ICMJE/COPE guidelines are revised or amended. An initiative involving all stakeholders in health care education, research, and practice is urgently needed. This will help to set a code of ethics to guide the responsible use of ChatGPT among other LLMs in health care and academia.", "authors": ["Malik Sallam"], "year": 2023, "venue": "Healthcare", "cited_by_count": 2517, "type": "review", "concepts": ["Health care", "Documentation", "MEDLINE", "Context (archaeology)", "Medical education"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W3147474004", "doi": "https://doi.org/10.1109/tmm.2021.3068561", "title": "Motion Blur Removal With Quality Assessment Guidance", "abstract": "Non-uniform blind motion deblurring is a challenging yet fundamental task in the computer vision field, which aims to restore the latent sharp image from the blurry input. Recently, deep-learning-based methods have made significant improvement and progress, on the metric of PSNR. They achieve good results mainly because they adopt Mean Squared Error (MSE) as the optimization objective, in addition to their good model design. However, simple adoption of the PSNR metric and the MSE loss, has non-ignorable disadvantages. PSNR cannot always succeed in assessing the deblurred quality in accordance with the human visual system (HVS), and MSE guides the network to generate over-smoothed images. To address these problems, we are the first to propose the deep-learning-based multi-scale non-reference quality assessment network (Deep DEBLUR-IQA) for assessing the quality of deblurred results. Moreover, a deblurring network of high efficiency is presented. It is more than 50 times faster than other SOTA multi-scale Convolution Neural Network (CNN) methods, with the newly propose Residual Dilated Block (RDB) and Light ResBlock (LRB). The deblurring network's performance can be further boosted with Multiple Dilation Block (MDB), with an acceptable speed decrease. Finally, and most importantly, we are the first to let Deep DEBLUR-IQA guide the deblurring network's optimization. This IQA-guided enhancement paradigm can significantly improve the deblurring results’ subjective quality while achieving excellent PSNR. Experimental results demonstrate that the proposed method performs favorably against state-of-the-art methods quantitatively and qualitatively.", "authors": ["Jichun Li", "Bo Yan", "Qing Lin", "Ang Li", "Chenxi Ma"], "year": 2021, "venue": "IEEE Transactions on Multimedia", "cited_by_count": 17, "type": "article", "concepts": ["Deblurring", "Computer science", "Artificial intelligence", "Residual", "Metric (unit)"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W2974196484", "doi": "https://doi.org/10.1016/j.ophtha.2019.09.014", "title": "An Ophthalmologist's Guide to Deciphering Studies in Artificial Intelligence", "abstract": "", "authors": ["Daniel Shu Wei Ting", "Aaron Lee", "Tien Yin Wong"], "year": 2019, "venue": "Ophthalmology", "cited_by_count": 54, "type": "editorial", "concepts": ["Artificial intelligence", "Scopus", "Deep learning", "Medicine", "Scheimpflug principle"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W2751069891", "doi": "https://doi.org/10.1038/sdata.2017.117", "title": "Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features", "abstract": "", "authors": ["Spyridon Bakas", "Hamed Akbari", "Aristeidis Sotiras", "Michel Bilello", "Martin Rozycki", "Justin Kirby", "John Freymann", "Keyvan Farahani", "Christos Davatzikos"], "year": 2017, "venue": "Scientific Data", "cited_by_count": 2781, "type": "article", "concepts": ["Glioma", "Segmentation", "Neuroradiologist", "Glioblastoma", "Magnetic resonance imaging"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W2952218014", "doi": "https://doi.org/10.1109/taslp.2019.2915167", "title": "Conv-TasNet: Surpassing Ideal Time–Frequency Magnitude Masking for Speech Separation", "abstract": "Single-channel, speaker-independent speech separation methods have recently seen great progress. However, the accuracy, latency, and computational cost of such methods remain insufficient. The majority of the previous methods have formulated the separation problem through the time-frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of time-frequency representation for speech separation, and the long latency of the entire system. To address these shortcomings, we propose a fully-convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. Speaker separation is achieved by applying a set of weighting functions (masks) to the encoder output. The modified encoder representations are then inverted back to the waveforms using a linear decoder. The masks are found using a temporal convolutional network (TCN) consisting of stacked 1-D dilated convolutional blocks, which allows the network to model the long-term dependencies of the speech signal while maintaining a small model size. The proposed Conv-TasNet system significantly outperforms previous time-frequency masking methods in separating two- and three-speaker mixtures. Additionally, Conv-TasNet surpasses several ideal time-frequency magnitude masks in two-speaker speech separation as evaluated by both objective distortion measures and subjective quality assessment by human listeners. Finally, Conv-TasNet has a significantly smaller model size and a much shorter minimum latency, making it a suitable solution for both offline and real-time speech separation applications. This study therefore represents a major step toward the realization of speech separation systems for real-world speech processing technologies.", "authors": ["Yi Luo", "Nima Mesgarani"], "year": 2019, "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing", "cited_by_count": 1952, "type": "article", "concepts": ["Magnitude (astronomy)", "Ideal (ethics)", "Masking (illustration)", "Separation (statistics)", "Speech recognition"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W4384071683", "doi": "https://doi.org/10.1038/s41586-023-06291-2", "title": "Large language models encode clinical knowledge", "abstract": "", "authors": ["Karan Singhal", "Shekoofeh Azizi", "Tao Tu", "S. Sara Mahdavi", "Jason Lee", "Hyung Won Chung", "Nathan Scales", "Ajay Kumar Tanwani", "Heather Cole-Lewis", "Stephen Pfohl", "Perry W. Payne", "Martin Seneviratne", "Paul Gamble", "Christopher Kelly", "Abubakr Babiker", "Nathanael Schärli", "Aakanksha Chowdhery", "P. Mansfield", "Dina Demner‐Fushman", "Blaise Agüera y Arcas", "Dale R. Webster", "Greg S. Corrado", "Yossi Matias", "Katherine Chou", "Juraj Gottweis", "Nenad Tomašev", "Yun Liu", "Alvin Rajkomar", "Joëlle Barral", "Christopher Semturs", "Alan Karthikesalingam", "Vivek Natarajan"], "year": 2023, "venue": "Nature", "cited_by_count": 2617, "type": "article", "concepts": ["Computer science", "Benchmark (surveying)", "Language model", "Comprehension", "Artificial intelligence"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W4324046518", "doi": "https://doi.org/10.1080/14703297.2023.2190148", "title": "Chatting and cheating: Ensuring academic integrity in the era of ChatGPT", "abstract": "The use of artificial intelligence in academia is a hot topic in the education field. ChatGPT is an AI tool that offers a range of benefits, including increased student engagement, collaboration, and accessibility. However, is also raises concerns regarding academic honesty and plagiarism. This paper examines the opportunities and challenges of using ChatGPT in higher education, and discusses the potential risks and rewards of these tools. The paper also considers the difficulties of detecting and preventing academic dishonesty, and suggests strategies that universities can adopt to ensure ethical and responsible use of these tools. These strategies include developing policies and procedures, providing training and support, and using various methods to detect and prevent cheating. The paper concludes that while the use of AI in higher education presents both opportunities and challenges, universities can effectively address these concerns by taking a proactive and ethical approach to the use of these tools.", "authors": ["Debby Cotton", "Peter A. Cotton", "J. Reuben Shipway"], "year": 2023, "venue": "Innovations in Education and Teaching International", "cited_by_count": 1714, "type": "article", "concepts": ["Cheating", "Academic dishonesty", "Academic integrity", "Honesty", "Engineering ethics"], "search_query": "image quality assessment no-reference blind deep learning"}
{"openalex_id": "https://openalex.org/W2077663753", "doi": "https://doi.org/10.1371/journal.pmed.1001744", "title": "Critical Appraisal and Data Extraction for Systematic Reviews of Prediction Modelling Studies: The CHARMS Checklist", "abstract": "Carl Moons and colleagues provide a checklist and background explanation for critically appraising and extracting data from systematic reviews of prognostic and diagnostic prediction modelling studies. Please see later in the article for the Editors' Summary.", "authors": ["Karel G. M. Moons", "Joris A. H. de Groot", "Walter Bouwmeester", "Yvonne Vergouwe", "Susan Mallett", "Douglas G. Altman", "Johannes B. Reitsma", "Gary S. Collins"], "year": 2014, "venue": "PLoS Medicine", "cited_by_count": 1804, "type": "article", "concepts": ["Checklist", "Critical appraisal", "Data extraction", "Systematic review", "MEDLINE"], "search_query": "image quality assessment no-reference blind deep learning"}
